Design of Internet of Things Information Interactive Perception System Based on 5G Mobile Communication Technology
Suzhou Vocational Institute of Industrial Technology, Suzhou, Jiangsu, 215104, China
Keywords: 5G Mobile Communication, Internet of Things, Development of the Times
Abstract: In recent years, with the continuous reform and progress of Internet technology and computer technology, mobile communication technology also has a cross era development.
Now, human society is stepping into the 5g mobile information age, and the Internet of things is also born.
The Internet of things mainly deals with the communication and connection between things, mainly using 5g mobile communication technology, which is conducive to solving the problem of things The barrier between communication and connection is also the direction often discussed in many fields of modern society.
This paper mainly introduces the network characteristics of 5g technology, and then analyzes the role of 5g technology in the development of the Internet of things, hoping to help people from the Internet of things industry.
1. Introduction
5g mobile network is also called the 5th generation mobile Internet.
It is mainly built on the basis of the original 4G.
The data transmission speed of 5g network is about 100 times higher than that of 4G network, which can show that users can download a file with a large amount of information in a few seconds.
So when the concept of 5g mobile communication technology was born, it was widely concerned by the society.
It has 1g file receiving speed per second, which greatly facilitates the work and life of users[1].
According to the data reflected by relevant operators, 5g mobile communication technology will be fully promoted and serve the public in 2020.
In addition, compared with the previous mobile network communication technology, 5g mobile communication technology has a higher spectrum efficiency, a wider range of use, fewer areas will be limited, and can bring users a new and high-definition video viewing experience.
5g mobile communication technology has a fast network speed, which can receive instructions in time and transmit them to self driving vehicles.
Such a fast response speed and transmission speed will effectively alleviate traffic congestion, promote communication and exchange between different vehicles, and keep a good distance within a reasonable range at all times[2].
The use of 5g mobile communication technology in medicine can help doctors to operate, and so on.
So 5g mobile communication technology has the characteristics of wide field and high level, which can better promote the development of information age and Internet of things era.
2. Advantages of 5g Mobile Communication Technology
2.1. Good Energy Saving Effect 
5g mobile communication technology has a strong compatibility and flexibility[3].
In general, 5g communication network does not need to consume too much energy, so it has a strong energysaving effect, and this feature is consistent with the concept of sustainable development in China, so the technology has a better development prospect.
2.2. High Efficiency
5G mobile communication technology takes user experience as the main foothold, its acceptance frequency is much higher than 4G network, and 5g network has strong penetration ability, it can deal with external environment factors freely[4].
Therefore, 5g mobile communication technology has a wide coverage, and presents an interactive mode, which greatly enhances the efficiency of the company's use of the network, so as to better promote the development of the animal network, also expand the company's business, and change the way of operation and so on.
2.3. Improved Scalability
5g mobile communication technology effectively combines the previous mobile communication technology, bringing users a new experience and use function[5].
According to relevant data, 5g mobile communication technology has more than ten times the coverage of 4G network technology, and has further strengthened the reception and release of signals, thus facilitating more users, so 5g mobile communication technology has greatly improved the scalability.
With the support of 5g network, social network will make human civilization in the future the development is more rapid.
Figure 1 Information conversation recognition rate 
2.4. Enhanced Reliability
The reliability of 5g mobile communication technology has been further enhanced to better meet the needs of different users[6].
On this basis, 5g mobile communication technology can alleviate the user's network delay phenomenon and improve the user's use experience[7].
Compared with the previous network technology, 5g mobile communication technology has larger capacity and realtime network transmission.
3. Progress of Internet of Things Under 5g Mobile Communication Technology
3.1. Increased Network Scope
5g mobile communication technology can better use antenna and high frequency band to complete data transportation, or adopt direct communication technology[8].
Before strengthening cellular data mode, the communication mode between each terminal can be established, which can further improve the frequency band and network coverage, so as to promote the development of the Internet of things era.
3.2. Cultivate New Thinking
Under the application of 5g mobile communication technology, the continuous development and improvement of artificial intelligence has been derived, and the intelligent navigation driving has also been promoted.
In addition, 5g mobile technology also promotes the emergence of Internet of things technology innovation thinking, which plays an important role in the development and innovation of Internet of things[9].
We should continue to use new technologies, and effectively play its internal role to better serve the Internet of things era.
3.3. Transformation and Implementation of Traditional Network
After 5g mobile network mode, the data transmission speed of 5g mobile communication is about 100 times of the previous network technology, which also promotes the quality and speed of information transmission of mobile terminals such as mobile phones and tablets[10].
At this time, the control signals under 5g mobile communication technology have become more diversified, and also accelerate the development of information technology such as cloud computing and big data, so as to create favorable conditions for the Internet of things era.
4. Future Automation
Although the labor and capital were replaced by CIM, the attempt to implement "unmanned factory" ended in failure. 
However, the specific elements of CIM philosophy are solidified in practical activities. 
In the context of big data expanding, the cognition of Internet and data is not only professional ability, but also social ability and interdisciplinary ability. 
In short, for the common development of production technology, automation technology and software, more work will be completed in the context of technology, organization, socialization and wide flexibility. 
Industry 4.0 means new logic and quality for smart factory production. 
The Self-discipline Function of intelligent system product production is the communication of machinery, labor and other system components and communication control technology, the non-human factors and tasks of independent production, and the control of production process.
5. Big Data Research
5.1. What is Big Data
With the development of society and science and technology, data is no longer regarded as a "byproduct" of social production, but a raw material that can be processed twice or many times. 
From now on, we can explore more value. Moreover, its essential value is often taken off and becomes an important asset. 
With the development of the third industrial revolution, the ability of human beings to process and accumulate data has made a certain leap, but the speed of data generation can not bear it. 
For example, the nature of big data, from the perspective of industry 4.0, big data is no longer "hardware generated" data, but can be used as "live and flexible" data. 
By analyzing these data, people can see the nature of things and provide a new way for people to understand and transform the world.
5.2. Big Data Research
Big data integration (big data equipment) is a software and hardware product specially used for large-scale data analysis and design.
It consists of server, storage device, operating system, database management system and some special data.
This is for query, processing, and analysis purposes.
This is to solve the problem of big data infrastructure data processing and integration, and integrate product data analysis.
Big data processing uses different data processing architectures to integrate distributed system architecture, hardware and software into one system to support different industry applications.
Through all distributed data processing systems and optimized hardware and software structures, the customer data business platform continues to grow.
The vertical hardware can be expanded, and the horizontal linear expansion of nodes can also be increased.
6. Internet of Things and Information
Internet is an information interactive industry, which covers data security, data acquisition, remote data connection and other knowledge, and uses advanced industrial technology to integrate with each other.
Through the Internet of things, data remote service, diagnosis system monitoring, fault data collection and system user rights management can be carried out.
Through the application program and rich technical knowledge, the staff analyzes the equipment information fed back to customers to achieve rapid information feedback and troubleshooting.
6.1. Remote Operation
Figure 3 shows the Internet remote service network topology of object information interaction.
It can be seen from the topological diagram that the industrial production line equipment sends the equipment information status data to the local data analysis node through the industrial Internet of wireless communication technology.
Local analysis node analyzes information status data and forwards it to cloud storage.
Machine maintenance personnel can use computers to read information data and analyze the status of the machine.
6.2. Monitoring Function
When the system state changes, the big data of the network system is transmitted to the diagnosis system.
The diagnosis system displays the change information in the GUI interface and stores it in the database.
At the same time, send email to the designated person, process diagnostic information and change information.
When the system changes information, the diagnostic system diagnoses the change information of the system.
System IOT and remote diagnosis system can monitor production line information of multiple plant systems.
The system diagnosis system monitors all systems in the system production line with multiple complete sets of equipment.
Realization goal: users can select system production line of specific factory through GUI interface.
Users can select the system on the production line through GUI interface.
6.3. Collection of Fault Data
In case of system failure, collect the bus data after failure and monitor the system status in real time.
Stop collecting data when the alarm status is reported to the system.
Store the data collected in real time in the database.
The database can hold data for up to 15 minutes.
In the engineering workshop, the fault collection interface of the robot is carried out.
The interface provides the function of querying and checking out bus data according to conditions.
Support the operation of various states &amp; functions.
For example, search for both time conditions and error code conditions.
The field personnel connect the analyzer to the fault system, turn on the system switch, and data collection begins.
When the system switch is off, data collection is interrupted.
After the system diagnosis system receives the error information reported by the virtual physical system, it displays the error information in the GUI interface and stores it in the database.
When a system error occurs, the system diagnostic system can describe the content of the error and send an email to the designated person to solve the problem in time.
7. Conclusion
At present, 5g mobile communication technology has become mature and perfect, but with the continuous progress of society, the development level of Internet is higher.
The Internet of things will appear in various and flexible forms in the future.
And make people's life and work more convenient.
However, the development of the Internet of things is inseparable from the development of 5g mobile network communication technology.
References
[1] Godfrey, Akpakwu., Bruno, Silva., Gerhard, P. Hancke. A Survey on 5G Networks for the Internet of Things: Communication Technologies and Challenges. IEEE Access, vol. 5, no. 12, pp. 3619-3647, 2017.
[2] Parisa, Ramezani., Abbas, Jamalipour. Toward the Evolution of Wireless Powered Communication Networks for the Future Internet of Things. IEEE Network, no. 99, pp. 12-19, 2017.
[3] Khoueiry, B.W., Soleymani, M.R. A Novel Machine-to-Machine Communication Strategy Using Rateless Coding for the Internet of Things, vol. 3, no. 6, pp. 937-950, 2017.
[4] Sotirios, K. Goudos., Panagiotis, I., Dallas, Stella, Chatziefthymiou. A Survey of IoT Key Enabling and Future Technologies: 5G, Mobile IoT, Sematic Web and Applications. Wireless Personal Communications, no. 3, pp. 1-31, 2017. 
[5] Song, S., Chang, K, H., Yoon, C., et al. Special Issue on 5G Communications and Experimental Trials with Heterogeneous and Agile Mobile networks, vol. 40, no. 1, pp. 7-9, 2018.
[6] W. Wang., J.-J. Zhao., L. Peng, Research on the Energy Saving Strategy for Long Distance Communication of Mobile Internet of Things Based on UAVs. Tien Tzu Hsueh Pao/Acta Electronica Sinica, vol. 46, no. 12, pp. 2914-2922, 2018.
[7] Yan, R.Y., Xiang-Yang, L.I., Gao, B.Q. Research on Information and Communication Supporting Architecture and Operation Mode of "Internet Plus" in SGCC, 2018.
[8] Tham, Jason Chew Kit. Interactivity in an Age of Immersive Media: Seven Dimensions for Wearable Technology, Internet of Things, and Technical Communication. Technical Communication, 2018.
[9] Tham, Jason, Chew, Kit. Interactivity in an Age of Immersive Media: Seven Dimensions for Wearable Technology, Internet of Things, and Technical Communication. Technical Communication, 2018.
[10] M. Majid, Butt., Petar, Popovski., Muhammad, Zeeshan, Shakir. IEEE Access Special Section Editorial: Physical and Medium Access Control Layer Advances in 5G Wireless Networks. IEEE Access, vol. 5, pp. 27845-27849, 2017. 
Engaging Government-Industry-University Partnerships to Further Gender Equity in STEM Workforce Education Through Technology and Information System Learning Tools
Recommended Citation: Knestis, K., Cheng, J., Fontaine, C. M., & Feng, R. (2022). Engaging Government-Industry-University Partnerships to Further Gender Equity in STEM Workforce Education Through Technology and Information System Learning Tools. Journal of Information Systems Education, 33(1), 23-31.
ABSTRACT
This paper has two goals: First, to detail processes through which a project funded under a National Science Foundation workforce development program (Innovative Technology Experiences for Students and Teachers, ITEST) leveraged active partnerships among government agencies, industry firms, and universities to develop and study an innovative, out-of-school information system and technology workforce education program.
The aim of the program was to improve equity of opportunity for high school girls.
The program engaged young women from underrepresented subgroups in data science, analytics, information communication technology, and programming learning activities in an experiential, law enforcement computer forensics context.
This description of the research team's process is intended as inspiration and guidance to others considering developing similar programs targeting workforce development in science and technical fields through an equity lens.
Second, this paper shares reflections from senior project personnel on lessons learned while working with cross-sector collaborations, including challenges encountered while implementing components of the program facilitated by the partnership model.
The authors adopt a reflective practice orientation, considering implications regarding the most useful-and evolving-roles that cross-sector partnerships might play in developing programs to help students traditionally underrepresented in technical fields be more aware of, interested in, and prepared for careers in science, technology, engineering, and mathematics (STEM) disciplines.
In so doing, the authors offer insights about how university partners might address potential tensions involved in such collaborations.
Keywords: Gender equity, Industry partnerships, Analytics, STEM, Workforce development, Reflective practice
1. ABOUT THE PROJECT
Women in the United States are underrepresented in technology-focused professions (Keune et al., 2019).
Barriers to equity include gender-related social influences (e.g., stereotypes, values, or prevailing role beliefs) and a lack of opportunities and support for girls navigating education pathways to such careers (Glass et al., 2013; Keune et al., 2019).
Focusing on the latter to address inequities and diversify the technology workforce, a team of University of Central Oklahoma (UCO) researchers developed an out-of-school time (OST) program for high school girls, combining team-based learning activities, career explorations, mentoring, internships, and research fellowships.
The resulting Computer Forensics (CF) Program is an authentic, experiential opportunity for young women to prepare for science, technology, engineering, and mathematics (STEM) and emerging STEM-related careers as diverse as CF Examiner, Data Scientist, Cybersecurity Specialist, and Artificial Intelligence Engineer (Billionniere & Rahman, 2020; Fitzgerald et al., 2014; Mew, 2020).
1.1 The NSF ITEST Program
To build an equitable, diverse workforce in STEM careers, the National Science Foundation (NSF) has identified ten priorities for investment in research and development (R&D) of innovations to grow human capital and sustain U.S. long-term competitiveness and global leadership (NSF, 2019).
UCO's efforts focus on: (1) preparing learners for work at the humantechnology frontier via innovation; (2) converging interdisciplinary research via partnerships; and (3) harnessing the data revolution via analytical tools (NSF, 2019).
NSF awarded funding for R&D of UCO's learning innovation under their Innovative Technology Experiences for Students and Teachers (ITEST) program (award #1758975).
NSF grants ITEST awards to study and improve innovations that enhance PreK-12 student outcomes crucial to emerging STEM-related career pathways.
ITEST learning innovations are expected to apply "technology-rich experiences" to "increase interest in and awareness of STEM-related emerging careers" (NSF, 2017, p. 4).
Importantly, NSF expects grantee investigators to leverage "business and industry partners" in their efforts to motivate students to pursue education pathways into STEM occupations.
(NSF, 2017, pp. 4-5).
This principle is grounded in the idea that partners from sectors likely to employ graduates in STEM careers are uniquely positioned to inform decisions about the technologies ITEST-designed innovations should include, as well as how they contribute to authentic, experiential learning for groups underrepresented in STEM.
In this article, we adopt a reflective practice lens to focus on and share retrospective lessons learned about this central tenet of the ITEST program.
We also discuss translating assumptions about the value of partnerships into decisions about program design and implementation, with the aim of informing how senior personnel of similar education R&D projects might effectively engage diverse partners in their work.
1.2 The STEM CareerBuilder Project
The UCO team's project (2018-2022, including a COVIDrelated extension) pursued this aim in partnership with other entities, to design, implement, test, and improve the STEM CareerBuilder OST workforce and information systems (IS) education pathway for high school girls.
The project team initially tested a one-week pilot version of the model in grant  Year 1 (summer of 2018), which was based on UCO's previous Crime Scene Investigation (CSI) program.
The Oklahoma State Regents for Higher Education funded CSI (2013-2018) to broaden STEM participation among underserved high school students, male and female.
The new CF Program model included several substantive changes over CSI, the most fundamentally being its conception as an all-girl experience.
This change was based on the finding from the CSI project that female students outperformed males in targeted STEM workforce-related outcomes in the Summer Academy model central to the program (Cheng & Feng, 2018).
This decision proposed that increasing the number of girls given opportunities in STEM education programs most directly addresses the problem of gender underrepresentation, in turn increasing equity to grow a diverse, skilled, and innovative STEM workforce (National Research Council, 2002).
As implemented in what was intended to be its final form (grant Year 2, 2019), the program included a hybrid, two-week CF Summer Academy experience for 50 young women (an increase from 15 in the mixed-gender CSI program).
The program combined in-person and online learning supported by selected K-12 teachers and student participants from previous years returning to serve as near-peer leaders.
The model's core Academy activities were supplemented with additional opportunities-mentoring, internships, and research fellowship placements-available through competitive applications to five girls.
The project team designed these activities to allow participants to interact with STEM professionals during Academy activities, then work with them one-on-one as a supervisor or mentor.
Such independent activities were designed to complement experiential, team-focused learning facilitated by female role models for young women from industry and government (National Science Board, 2018).
1.2.1 Targeted Recruitment of Underserved Girls.
Given the equity priority, the CF Program emphasizes targeted recruiting through venues chosen to reach girls generally, but more importantly to engage subgroups of females particularly underrepresented in STEM workforce pipelines.
Recruiting outreach actively shared the CF Program opportunity with girls where they were already congregated, targeting those who would be first-generation college attendees, are from Native American communities for whom English is a second language, and/or who have a disability (Borgman et al., 2008; Dasgupta & Stout, 2014; National Science Board, 2018).
For example, 2019 Summer Academy recruiting provided program information to thousands of K-12 attendees of the Oklahoma Women in Science Conference, an activity of the NSF-funded Established Program to Stimulate Competitive Research (EPSCoR) program.
Messaging was developed to reach girls supported by Oklahoma Promise, a state program to help children in low-income families with college scholarships, participating in federal programs for learners from economically-disadvantaged backgrounds (e.g., Female Upward Bound), attending the Francis Tuttle Technology Center (a vocational cooperative among four Oklahoma City school districts), and enrolled in the Metro Technology Centers' career and technology education system.
The project team recruited participants from indigenous populations through Native American-serving institutions associated with the Choctaw Nation, the 11,000 square-mile tribal area in southeastern Oklahoma.
Lastly, the primary investigator (PI) distributed program information through counselors positioned to reach underserved female students in more than 400 Oklahoma high schools.
1.2.2 Computer Forensics as an Emerging Interdisciplinary Field.
CF is an emerging, interdisciplinary subfield of forensics that applies technology tools and analytic methods to gather and preserve evidence from compromised networks or computers (U.S. Department of Defense United States Cyber Command, 2015).
With the cost of cybercrime anticipated to double between 2015 and 2021 to an estimated $6 trillion, CF has become increasingly crucial in fighting these contemporary crimes including cyber security breaches, terrorism, and various white-collar offenses (Morgan, 2019).
The CF Program is a response to a growing need for CF professionals, resulting from increased reliance on technology and recognition of the costs and difficulty of dealing with cybercrimes (Choo, 2008; U.S.Department of Defense United States Cyber Command, 2015).
While these labor needs are important, however, the CF Program uses forensics primarily as a lens through which girls examine STEM discipline content more broadly, and as thematic context for the outcomes that really matter- generalizable STEM, IS, as well as information and communication technology (ICT) skills and understandings.
1.2.3 Integration of IS Skills.
The skills and knowledge required to effectively use IS tools, first within and then beyond the context of forensic investigations, make up many of the CF Program's outcomes.
Participants learn them in a way that integrates IS fully with the other STEM- and workforce-related outcomes anticipated for participants-applying their growing IS skillset to learn other content, even as learning the content (e.g., analytic thinking or communication) serves as the vehicle to increase IS proficiencies.
Program activities further this approach through collaborative career and skill-building explorations utilizing common workplace technologies across three IS realms-databases, programming, and analytics-in fully integrated ways.
1.2.4 Leveraging Multiple ICT Modalities for Content Delivery.
From its inception as CSI, the program design has utilized web-enabled facilitation of learning activities, furthering ITEST R&D priorities by iterating the CF Program design each year in terms of delivery formats.
The team's 2018 pilot of the central, weeklong portion of the Academy had girls participating onsite at the UCO campus.
In 2019, the Academy was expanded to two weeks utilizing the hybrid model, with the first reprising the resident camp model at UCO and the second facilitated through a web-based learning management system.
The model evolved further for 2020, transitioning the Academy to 100% online delivery in response to the COVID-19 pandemic.
In hindsight, it was fortuitous that the intentional evolution of ICT modalities for the program laid the foundation for a fully online evolution of the model, the project team retaining this COVID-safe version of the Academy for the summer of 2021.
2. ENGAGING PARTNERSHIPS
Central to the STEM CareerBuilder project is the GovernmentIndustry-University (GIU) partnership model-a purposeful, cross-sector collaboration of entities engaged in the R&D and learning aims of the project.
The GIU partnership contributes to the delivery of the programming, but more importantly undergirds project R&D efforts, providing a collaborative framework within which K-12 educators, interdisciplinary college faculty members, STEM researchers, technologists, and industry professionals inform testing and improvement of the CF Program's career-exploration and skill-building curriculum.
As the name implies, the GIU approach brings together partner organizations from federal, state, and local governments (G), technology industries (I), and entities in the university system in Oklahoma (U).
The government piece is largely law enforcement agencies given the forensics focus.
Given existing organizational connections to transition high-school learners to post-secondary experiences, the final category also includes K12 education partners and institutions serving Native American learners.
The three-pronged partnership structure of the model itself was central to STEM CareerBuilder implementation, so it is only natural to extend it to broader thinking about partnering for education R&D efforts.
It may be of further use, however, to guide consideration of the existing body of research on partnerships to support STEM education and workforce pipelines, the bulk of which focuses more narrowly on partnerships between universities and school districts (e.g., Bowen & Shume, 2020; Burrows, 2015; Cress et al., 2020; Hunter & Botchwey, 2016; Icel & Davis, 2018; Ufnar & Shepherd, 2019).
This leaves gaps about how to best engage government and technology industry actors.
Synergies among these partner organizations helped create the precursor CSI program, with new representatives from all three sectors joining in the transition to the CF program.
Under ITEST funding, contributions of GIU partners take several forms, including: (1) as key personnel managing the project, including development of curriculum and digital resources; (2) by serving on a cross-partnership advisory board, overseeing and guiding the work; (3) as Academy presenters, sharing their expertise and serving as role models; (4) as Internship Sponsors who select, supervise, and evaluate girls for positions as interns; and (5) as Fellowship Mentors, assisting selected girls with proposals and research projects of their own design.
The STEM CareerBuilder project's partnering approach bears certain similarities to-and notable differences from- other programs in the STEM workforce development space.
Like programs in Kentucky (Strode et al., 2021) and western Michigan (Thelenwood et al., 2020), the STEM CareerBuilder project and CF Academy include partnerships with university, industry, and K-12 entities.
However, the project and the CF Program are distinct in their specific focus on redressing inequities around gender, race, and class, objectives not thoroughly addressed in the academic and trade literature.
A formal, a priori process of collecting and analyzing evidence was adopted to answer the project's research questions, focusing on how partnerships with industry professionals and government entities encourage young women's awareness of and interest in STEM careers.
In contrast, the inquiry central to this article, examining how better to engage those partners, applies a reflective practice orientation.
The motivation for adopting this approach lies largely in response to challenges perceived by senior project personnel during the latter years of the project, regarding unexpectedly low rates of participants pursuing internships and fellowships.
This effort aims to formalize a backward look at what worked-and what might have worked better-tomaximize the value of partners' contributions over the life of the STEM CareerBuilder project, and to leverage partner relationships into increased internship and fellowship opportunities for CF Academy participants.
Reflective practice is the process of intentionally reflecting on professional experiences and decisions to grow one's expertise and develop one's professional practice.
It is a practice, meaning it takes shape in action and in context, assuming significance and gravity as it moves the individual to deeper understanding and facilitates participation in professional communities of practice, where the ability and willingness to account for decisions are marks of membership.
Educational philosopher John Dewey is widely credited as an early proponent of reflective thinking, emphasizing the generative possibilities of "perplexity, hesitation, [and] doubt" (1910, p. 9) as states of mind that help circumvent rote thinking and action.
Dewey's ideas were taken up by Donald Schon (1983), who distinguishes between reflection-in-action, or thinking while doing, and reflection-on-action, or thinking after the event, to inform future practice.
In this instance, the practice is retrospective reflection after the event, activated by the authors' knowing-in-action (or tacit knowledge), gained through expertise from years of collective experience.
Atkins and Murphy (1993) posit three stages of reflection: (1) uncomfortable thoughts or an awareness that things are not all well, what Schon (1983) refers to as the "experience of surprise" or what Boyd and Fales (1983) call a sense of "unfinished business;" (2) a critical analysis of one's own feelings and what is known about the situation; and (3) reflection with the intent of developing new perspectives on the question at hand.
This article aims to serve that last purpose, ideally contributing something new to collective knowledge (intellectual merit, to the NSF, 2017) of how best to leverage GIU partnerships to develop, study, and implement education programs to address STEM workforce learning and participation outcomes.
2.1 Government Partners
Law enforcement agencies play a substantial role in the CF Program model, as forensic sciences serve as the thematic framework for young women's workforce learning activities.
The Federal Bureau of Investigation (FBI) and Oklahoma State Bureau of Investigation (OSBI) have been key partners since UCO's inception of this approach.
A Senior Criminalist from the Combined DNA Index System (CODIS) lab at OSBI was a crucial contributor to the program design, grounded in her professional expertise and six years of CSI program experience prior to creation of the CF Program.
She is the key connection between the R&D team and OSBI, serving in a senior management role and supporting program operations (e.g., providing OSBI tours and coordinating lab materials for handson activities during the onsite Academy).
Another senior OSBI staffer, an expert in digital evidence and cyber security, served on the project advisory board.
OSBI also supports internships for girls interested in law enforcement or forensic sciences.
The Edmond (Oklahoma) Police Department (PD) provided substantial input on program activities, as well as (preCOVID-19) site visits for participants.
Their face-to-face contributions transitioned in the summer of 2020 to video explanations of digital forensics by a female presenter from the Edmond PD.
The Oklahoma City Office of Sustainability-responsible for technical assistance, planning, and outreach services to city departments and the public-partners by supporting internships, while representatives from the Oklahoma Center for the Advancement of Science and Technology (OCAST) have contributed in advisory roles and provided matching funds for internships with Oklahoma-based companies.
2.2 Industry Partners
As key IS industry players, Apple and IBM are the two most recognizable industry-sector CF Program partners.
An Apple executive shared resources and advice with the project team to inform recruiting efforts and contributed as an Academy presenter.
A Chief Technology Officer in IBM's Security Systems Division informed program development and facilitated girls' forensic analytics and cyber security learning activities.
Woman owners of Oklahoma small businesses also partnered with the project team in design and implementation of the CF Program.
The President of Becks Intelligence Group, a longtime contributor (eight years, including with the CSI program), served in a senior personnel role and as a board member.
Another female-identified technology-sector leader, with the nonprofit Oklahoma Women in Technology, presented at the Academy and managed internships.
2.3 University Partners
Educational institutions played crucial roles in project activities, as UCO led the NSF ITEST proposal.
The PI and CoPI (the latter Chair of the Computer Sciences department) managed development and implementation of the CF model, with support from technologists at the UCO Center for eLearning and Connected Environment (CeCE).
Accustomed to helping university faculty design online coursework, the CeCE team developed human-computer simulations for the program based on real-world crime scenarios.
The CeCE Director also provided expertise in STEM curriculum design as a board member.
The UCO Center for Transformative Learning was a partner as well, helping develop active learning resources for the CF Program, and their director was also a board member.
Non-UCO postsecondary partners contributed as well, including the National Forensic Science Technology Center (NFSTC), an extension of the American Society of Crime Laboratory Directors situated at Florida International University.
NFSTC provided the online introductory course in crime scene investigation that makes up the second week of the Academy.
Given the emphasis of CF Program fellowships on research, university partner faculty played key roles as mentors to support girls' self-designed studies (e.g., a chemistry professor at Cameron University in Lawton, Oklahoma, mentoring a participant's nanotechnology project).
And again, partners in this sector include education agencies in the K-12 pipeline feeding university programs, including the districts employing classroom teachers who facilitated the Academy and one who served in an advisory board role.
3. GIU PARTNER INFLUENCES ON THE CF MODEL
Partner institutions contributed to the CF Program in two ways: (1) by informing development of the model, and (2) by providing staffing support during its implementation to test and improve the program's curriculum, strategies, and interventions.
Again, developers grounded the model's design on evidence that IS tools can support multiple aims in realizing those outcomes.
Experiences that apply IS tools to authentic tasks aim to grow skills with these technologies while advancing outcomes in STEM content understandings and "soft skills" valued by employers (e.g., problem-solving).
This premise reflects a key proposition of ITEST's theory of action, that innovative uses of ICTs encourage participation, engagement, satisfaction, and persistence in STEM career learning.
Immediate outcomes should in turn advance skills and dispositions that ultimately lead to desired behaviors like increased STEM course enrollment (NSF, 2017).
The STEM CareerBuilder project research agenda explores relationships among education program features-particularly IS components-and this range of outcomes (McCreedy & Dierking, 2013; National Research Council, 2012; Reider et al., 2016).
This theory-based approach influenced decisions about which technologies the CF model should employ, and how to integrate them into Academy learning experiences.
Active participation of GIU partner staff in CF Program delivery helped assure that girls' learning authentically reproduces the world of work, and that they benefit from the examples and expertise of a diverse group of professionals.
Other theories further argue that targeted competencies are transferable across disciplinary domains (e.g., Dede et al., 2005).
The UCO team's schema categorized technologies by the purposes they support for the program's criminal investigation-driven learning activities.
The contributions of various GIU partners can be defined within the same three domain areas: (1) Databases, (2) Programming and Analytics, and (3) ICT.
3.1 Databases
In the context of the CF Program, "database" outcomes include IS understandings relating to systematic thinking about data, variables and variable types, relationships among data elements, and concepts regarding storage and retrieval of data.
Law enforcement agencies regularly use collections of data in their investigative work, notably CODIS, the FBI's overarching program that supports the use of DNA databases in criminal justice, the National DNA Index System database of DNA profiles, and the Automated Fingerprint Identification System (AFIS), which serves functions and purposes similar to CODIS but for digital captures of hybrid, biometric (fingerprint) data.
Since high school girls in an OST STEM learning program cannot be granted access to compare crime scene samples to profiles in live, law-enforcement databases, OSBI helped the project team develop online simulations that mimic them.
OSBI forensic scientists introduced Academy participants to CODIS and AFIS, and university IS professors helped them understand how relational databases work, conceptually, and how to comb different types of data for analysis.
Some girls also applied their newfound understandings of data and data analysis to learner-designed optional experiences.
For example, one high school student undertook an internship with the Oklahoma City Department of Sustainability to examine city-level greenhouse gas emission inventories of peer cities like Indianapolis and Salt Lake City, to make policy recommendations about the inventories and standards they could use moving forward.
This example illustrates how the CF Academy cultivated conceptual understandings about data structures and computational reasoning that participants later had the opportunity to apply within new content areas.
3.2 Programming and Analytics
This grouping of IS uses in the CF model addresses programming, that is, the process of giving computers instructions allowing communications between humans and digital hardware, and analytics, the process of discerning patterns and making sense of raw data.
Programming and data analytics are complementary skill sets; gains in one area advance learning in the other (Mortenson et al., 2015).
The CF Academy uses simulations to advance understandings in both areas.
The Fingerprint Scanner, created by CeCE with input from OSBI, is a gamified learning tool that facilitates comparison of fingerprints from virtual crime scenes with examples from a simulated known perpetrator database.
Another simulation teaches girls about Blockchain-a decentralized public record system that ensures integrity in data transactions-as they play the role of crime scene investigator, securely sharing evidentiary data from field investigations with a detective and forensic artist, documenting the chain of custody to ensure admissibility of evidence.
In addition to simulations, the CF Academy offers explicit instruction in programming and data analytics.
A female Apple executive supported the development of an instructional module that teaches coding mobile apps using the Swift compiled programming language for iOS devices.
Leveraging Apple's Everyone Can Code initiative, this activity promotes creative and systematic thinking to "bring ideas to life" that addresses real-world problems (Apple, Inc., 2021).
Former FBI and OSBI agents spoke about analytics in investigative uses of DNA and cyber security, and a representative from IBM shared how the IT industry detects cybercriminal intrusions and network security breaches through behavior pattern analysis.
IS experts demonstrated programming languages including Java, C#, and Python, and helped girls explore how Structured Query Language (SQL) can be used to create, read, update, and delete information in a database.
Girls added these programming skills to their growing understanding about databases to work a fictional murder case, searching databases for fingerprint and DNA matches from their simulated crime scene.
Analytics and programming feature in internships and mentoring opportunities as well, such as in a data mining project with OSBI examining correlations between juvenile crimes and drug use.
This work of a 2019 research fellow culminated in a presentation at a conference of the American Academy of Forensic Sciences.
Another CF Program participant collaborated with a Fulbright Scholar in Iran, offering high school girls in her community free classes to learn Python for data analytics.
3.3 Information and Communication Technology
Finally, the CF Program cultivates familiarity and comfort with ICT systems of telecommunications and computing hardware and software for the creation, storage, and sharing of information.
GIU partners made crucial contributions to designing how girls would use ICT tools to access resources and content, communicate with teammates, manage their forensic examinations, share information, and develop products of their investigations.
Early iterations of Academy programming relied on live presentations to girls with participation in onsite presentations and hands-on processing of a staged crime scene.
For the 2019 Academy, a returning teacher helped develop online materials for these crime scene investigations, beginning the evolution from in-person to ICT-enabled experiences, which ultimately became necessary for program COVID-safe delivery in 2020.
These enhanced resources were integrated into Academy activities alongside additional CeCE simulations designed to develop workplace skills like communication, collaboration, and critical thinking.
Participants role-play 911 calls to a security specialist in the simulated 911 Triage and Information Center, applying protocols developed by the Edmond Police Department.
A Crime Scene Investigation simulation was introduced at the 2019 Academy, enabling OSBI agents and facilitators to guide girls to process a crime scene and collect digital evidence following authentic protocols in an immersive, virtual reality experience.
Finally, the Virtual Detective Office provides interview rooms in which girls apply communication strategies designed by state and local law enforcement partners, with near-peer or professional Academy participants playing witnesses and suspects as avatars in a webbased environment.
The GIU design team created these simulated forensic environments through which learners develop critical thinking by solving problems in authentic contexts using professional practices (Manlow et al., 2010; National Research Council, 2012).
The CF Program also leveraged a full slate of ICT tools to manage course delivery and communication outside of these virtual environments.
Participants used other cloud-based ICT solutions to manage their activities-online meeting applications, a mobile texting app piloted in the 2018 iteration of the model, and networking tools like LinkedIn and Twitter.
All of these were in environments resembling the virtual teams increasingly prevalent in networked, global workplaces, and technology-rich societies (Borgman et al., 2008; Reider et al., 2016).
4. LEVERAGING PARTNERSHIPS FOR LEARNING INNOVATION R&D
In considering the implications of engaging partners in efforts like the STEM CareerBuilder R&D project, the UCO team acknowledges that there is still much to be learned.
Lessons come slowly when learning innovation development happens on 12-month implementation-and-improvement cycles.
Further, successful partnerships are extremely difficult to achieve in practice, and research-informed understandings of how to establish productive working relationships across different types of organizations lag behind the substantial enthusiasm for these efforts (Noam & Rosenbaum Tillinger, 2004).
It is also important to remember-and probably selfevident in the midst of the COVID-19 pandemic-that external factors impact program design and implementation decisions, separate from purposeful changes that research findings might recommend.
While understandings about the role and influences of the GIU model continue to evolve, the project team has clarified several potentially useful ideas about leveraging cross-sector partnerships to better deliver technology-facilitated workforce education innovations.
4.1 Partnerships Are Powered by Individuals
Regardless of the GIU sector, connections between R&D project managers and partner agencies exist only through individuals affiliated with both the partner and the developmental research project.
Individuals get involved in such projects for a variety of reasons, and motivations may change, as may contributors' professional positions and obligations.
Teams delivering, studying, and improving education innovations therefore often experience turnover for reasons outside of project managers' control, leading to a loss of valuable connections.
Motivations frequently differ by partner type, presenting orientation-related barriers (Bruneel et al., 2010) to productive partnership.
A university faculty member may have a different sense of urgency than an industry partner representative, depending upon the relationship of a project to the former's research agenda.
Institutions may have policies or incentive systems that constrain or compromise partnering, for instance around intellectual property (IP) rights, an example of so-called transaction-related barriers (Bruneel et al., 2010).
Individuals representing partners may also be more or less engaged, effective, or persistent depending on factors internal to a project like STEM CareerBuilder.
It contributes to the effort, the specific roles to which they are assigned, the effectiveness of management processes (e.g., communication), and the extent to which individuals' contributions are valued or perceived as useful.
Project managers must attend to such factors bearing on individual contributors' involvement, fixing those they can influence (e.g., task-tracking and teamwork practices) and accommodating or mitigating those they cannot.
Barriers can be alleviated by prior collaborative research experiences and greater levels of trust (Bruneel et al., 2010), both of which develop at a person-to-person level as well as at an institutional or organizational level.
The development of trust takes time, and requires structures that facilitate individuals from partnering organizations to interact in ways that develop senses of mutual understanding and support; this requires in turn the dedication of personnel time to facilitate these structures and interactions (Noam & Rosenbaum Tillinger, 2004) in addition to the time and effort required up front to build and maintain relationships with area employers, schools, and community organizations (Thelenwood et al., 2020).
Indeed, relationships are foundational to the development of deep and effective partnerships, and ideally should be cultivated even prior to the identification of funding opportunities (Allen et al., 2020; Ivey, 2019).
This allows partnering organizations to articulate common goals and develop processes for working together on their own terms, exclusive of the influence of funders' requirements, an approach that is associated with highly sustainable partnership models.
4.2 Different Partners Make Different Contributions
During planning for the NSF ITEST proposal, the PI actively engaged partners from all three GIU sectors, assuring that design of the CF Program model represented their varying perspectives, and that all sectors would contribute to implementation of grant-funded R&D activities.
This approach acknowledged that each partner entity brought to the project (through their representatives) specific expertise, technical capacities, connections, and other organizational capital important to the effort.
Initially, individuals from all three sectors contributed in all the ways discussed previously-as senior personnel; in advisory positions; presenting or facilitating activities for participants; and sponsoring or supervising internships or fellowships.
This calls to mind the triple helix model of innovation, in which R&D efforts among the three types of entities benefit from collaboration through interactive and nonlinear processes, versus a model in which universities "invent" through basic research and industry partners "apply" what is learned (Etzkowitz & Leydesdorff, 1995).
The sectors contributed in different ways and amounts across those functions, based on their interests in the CF Program and the grant-funded project studying and improving it.
Government partners, largely law enforcement agencies, were instrumental in defining the substance of the forensics content, a very different role for government than that is typical in triple helix approaches where government entities are more likely to fund or facilitate R&D (Etzkowitz & Leydesdorff, 1995) or establish rules for IP ownership.
Industry partners' contributions were arguably different from those in traditional technology-transfer R&D partnerships.
They made crucial substantive contributions to decisions about what workforce outcomes really matter, how high school students might learn them, and which technologies girls should learn to prepare for STEM education and workforce pipelines.
Individuals from this sector also served as indispensable role models of women reaching leadership positions in technology and IS careers.
Given the aims of ITEST and the UCO project, and the absence of any immediate return-on-investment, their institutional incentives and personal motivations for such contributions seem to be driven by social mission, or perhaps by abstract future benefits anticipated to accrue from productive STEM workforce pipelines.
University partners were central to developing the curriculum, materials, and technologies to facilitate both inperson and remote learning activities and played an important role in securing research-focused internship and fellowship opportunities for participants; while affiliated K-12 sector partners designed and executed efforts to recruit young women from underrepresented populations.
As in Thelenwood and colleagues' (2020) discussion of a similar university-industrycommunity partnership model in the engineering space, the university partners focused on providing STEM content-heavy programming as opposed to sharing information about the university.
While formal partnerships involving K-12 schools and districts and outside partners are typically fragile and challenging to sustain (National Research Council, 2012), the up-front recruitment role in which K-12 partners functioned in this instance made the partnership less vulnerable to changes in policy, personnel, or direction at the school or district level.
4.3 Needs for Partners Change Over Time
Given the relative areas of strength across the GIU sectors, ongoing evolution of the CF Program-as the product of an R&D effort-requires different contributions from different partners at various moments in its development.
This happened for the STEM CareerBuilder team as their model matured and will likely be so for similar developmental research projects.
Awareness of this dynamic, however, should be balanced against a simultaneous understanding of the importance of engaging continually with the same partners over long periods of time (Thelenwood et al., 2020).
Encouraging contributors to define the substance of learning activities was crucial in the early phases of program development but became less so as content foundations for learning activities were laid.
Conversely, industry partners became increasingly important as program participants transitioned out of facilitated group activities to individual, self-guided experiences requiring oneon-one mentors or supervisors.
University partners may become less important if and when an innovation grows beyond being the subject of research and becomes commercialized, sustained, or scaled up by other means.
Different aspects of an educational innovation mature at different rates, or are best developed in a particular sequence, as was the case for Academy activities translated to ICTenhanced (eventually fully online) modalities.
Content must be defined before curricula are created, which must be prototyped before technologies can be fine-tuned for use with learners.
Researchers must collect and analyze data about an innovation's utilization and outcomes before developers can make design improvement decisions.
Given that each partner brings unique expertise and other forms of capital to developmental research projects, their contributions might be greater or lesser depending on where the innovation is in the R&D cycle, from a new idea to an established model with demonstrated efficacy.
The STEM CareerBuilder team experienced shifts in needs-and so in partnerships-as it engaged educators and technology workforce employers to develop IS-intensive programming to train future STEM professionals (Hoanca & Craig, 2019, p. 238).
It accommodated these changes rather than actively planning for them, where (with hindsight) it should be possible to anticipate such situations.
This is critical because workforce skills like innovation are crucial in global, digital markets, and STEM fields are at the core of our country's ability to sustain long-term competitiveness (Fifolt & Searby, 2010).
Research and development of new models to effectively address these needs call for inclusion of a diverse set of partners as well as attention to nurturing individual relationships to optimize contributions.
Attending to partnership structures as well as relationships is necessary to best evolve innovations and meet ever-changing project needs.
5. ACKNOWEDGEMENTS
This article is based upon work supported by the National
Science Foundation under Grant No. 1758975.
Features of technology and its linkages with turnover intention and work exhaustion among IT professionals: A multi-study investigation
Keywords:
Features of technology
IT professionals
Turnover intention
Work exhaustion
Mixed-method
Expectation Disconfirmation Theory (EDT)
ABSTRACT
Technology is central to the work-life of professionals in the information technology (IT) domain.
However, inadequate attention has been paid to technology as an antecedent of IT professionals' work outcomes, such as turnover intention and work exhaustion.
In the present work, we conducted a mixed-method study.
In Study 1, we used an inductive qualitative approach to explore features of technology from the career perspective of IT professionals.
We explain how perception about the features of technology influences IT professionals' work outcomes.
Based on semi-structured interviews of 35 IT professionals, we found three features (long-term consequence, perceived challenge, and technology-related uncertainty) on which IT professionals assess and evaluate a given technology.
Subsequently, in Study 2, we used the Expectation Disconfirmation Theory (EDT) as the framework to develop a model to measure the perceived misfit between 'expected' and 'experienced' features of technology and test its impact on IT professionals' turnover intention and work exhaustion.
The study provides valuable insights for the management of IT professionals.
1. Introduction 
IT professionals' contributions to organizations for sustained competitive advantage are well documented (Devaraj & Kohli, 2003; Iyengar, Sweeney, & Montealegre, 2015; Sykes, 2015).
We define IT professionals as individuals trained in various technologies to perform software/applications development, i.e., software developers (Johnson & Senges, 2010).
Scholars have focussed on the usage and adoption of technology and its subsequent outcomes (Bala & Bhagwatwar, 2018).
However, studies examining the impact of technology on IT professionals' work outcomes are relatively scarce.
This is a critical omission as technology1 is central to IT professionals' work.
Technology is a multidimensional construct characterized by its features (Griffith, 1999).
Technology features are crucial as they impact individual sensemaking of the technology (Griffith, 1999), leading to work-related outcomes.
Previous studies have examined the usefulness and pace of change as technology features to explain person-technology misfit and related outcomes (Ayyagari, Grover, & Purvis, 2011).
Hence, features of technology may impact individuals' preference for working with a given technology.
For instance, the threat of technological obsolescence might reduce preference for working with the technology due to perceived decline in job market opportunities (Fu, 2011; Joseph, Ang, & Slaughter, 2015).
On the other hand, an emerging or dominant technology might provide better career prospects to IT professionals, thus enhancing their preference to work in such technologies (Ang & Slaughter, 2004; Tomer & Mishra, 2019).
However, studies exploring features of technology from the career perspective of IT professionals are limited.
Investigating the features of technology to explain IT professionals' work outcomes is important for several reasons.
One, IT professionals' perception of the suitability of a given technology leads to their work fulfillment, job satisfaction, and career growth (Hsu, Jiang, Klein, & Tang, 2003; Speier & Venkatesh, 2002).
Two, unmet expectations from a job setting (such as technology) mainly contribute to adverse work outcomes among such professionals (Chilton, Hardgrave, & Armstrong, 2005; Moquin, Riemenschneider, & Wakefield, 2019).
Though a few studies have explored the impact of features of technology, we found a gap in explaining the process and mechanism of how career-related features of technology impact IT professionals' work outcomes.
Scholars argue that differences in career orientations (Fu & Chen, 2015; Igbaria, Greenhaus, & Parasuraman, 1991) might differentially impact IT professionals' expectations from a given technology.
It would be interesting to examine how the discrepancy between expectations and experiences associated with a given technology impacts these professionals' work outcomes (Brown, Venkatesh, & Goyal, 2014; Tomer & Mishra, 2019).
Our research explores features of technology that IT professionals value.
It further investigates the fit/misfit between expected and experienced features of technology and IT professionals' work outcomes, such as turnover intention and work exhaustion.
We focus on turnover intention and work exhaustion for the following reasons.
IT professionals exhibit higher turnover intention and experience higher exhaustion and stress levels (Ahuja, Chudoba, Kacmar, McKnight, & George, 2007; Armstrong, Brooks, & Riemenschneider, 2015).
A recent labor market survey reported that the attrition rate among IT professionals is about 30% compared to about 16% among other professionals.
2 As a result, successfully managing IT professionals is one of the biggest challenges in this industry.
Scholars have highlighted the need to re - evaluate traditional modes of thinking(focus on compensation and performance management) and seek more innovative approaches to manage these professionals (Agarwal & Ferratt, 2002, p. 74, see also Lo, 2015).
Considering how IT professionals' work and careers are closely connected to the features of technology (Shih, Jiang, Klein, & Wang, 2013; Wang, Wang, Zhang, & Ma, 2020), our findings will have important implications for managing IT professionals.
We approach the present research through two studies following a mixed-method approach.
We believe that a mixed-method approach would help improve contextual understanding of important constructs, their relationships and help test proposed relationships.
2. Literature review
Information Systems (IS) research has significantly contributed to understanding technology through multiple lenses across different contexts.
These studies have provided the foundation for our present work.
The constant change in technological features can put IT professionals on edge (Kaplan & Lerouge, 2007; Sharma & Stol, 2020).
Changes in technology, which may be gradual or disruptive, lead to career-related challenges for IT professionals.
They are expected to continuously upgrade their knowledge and technology-related skills to retain competencies.
This constant pressure of acquiring knowledge often leads to an increased workload.
An increase in workload has the potential to increase work exhaustion among these professionals (Gaudioso, Turel, & Galimberti, 2017; Moore, 2000; Venkatesh, Thong, Chan, Hoehle, & Spohrer, 2020).
Perceived misfit with features of technology can also act as a stressor (Venkatesh & Goyal, 2010).
For instance, high levels of perceived challenge encountered while working with a technology may increase workload.
The professionals need to invest additional time and effort to update learning and continue working in a given technology.
Individual factors also contribute to perceived misfit.
For example, IT professionals' self-efficacy towards acquiring newer technology-related skills might impact their evaluation of a technology (Compeau & Higgins, 1995; Rong & Grover, 2009).
Similarly, unmatched career expectations (Igbaria et al., 1991), such as lack of challenge in work (Edwards & Rothbard, 1999), acts as a stressor, leading to enhanced turnover intention.
In the present study, we build on the Expectancy Disconfirmation Theory (EDT) to investigate the dynamics between IT professionals' expectations and experiences with a technology.
The EDT concept posits that in a given context, when expectations and experiences of individuals are misaligned, the resulting disconfirmation will influence the related outcomes.
The EDT relates individual-level outcomes to preexisting expectations (Fan et al., 2014; Nam, Baker, Ahmad, & Goo, 2020).
These professionals compare their prior expectations with current experiences.
Disconfirmation leads to positive or negative evaluation.
Disconfirmation is a misfit, and we examined how positive and negative disconfirmation impacts outcomes.
We have employed the Polynomial Regression method and Response Surface Methodology to explore the effect of directionality of both expected and experienced measures, i.e., both positive and negative disconfirmation.
Scholars have extensively used EDT in individual technology interfaces, including operationalizing expectations and experience while exploring new technologies (Fan & Suh, 2014).
Thus, EDT is an appropriate lens for the present study.
Similarly, we believe that EDT can contribute towards a better understanding of the fit/misfit between expectation and experience with features of technology.
The above approach aligns with studies conducted in IS literature where technology evaluation and perceptions are believed to be influenced by individual-level factors (Brown et al., 2014; Speier & Venkatesh, 2002).
The misfit between expectations from a given technology and experience with it3 can explain work outcomes of IT professionals.
3. Research design
We have adopted a mixed method approach in our present study.
Research exploring the impact of technology on IT professionals' career and work outcomes is scant and fragmented.
Mixed-method approach provides a holistic understanding of the phenomenon, particularly when there is a gap in extant research (Venkatesh, Brown, & Bala, 2013; Edwards, 2022).
However, in IS research, studies using a mixed-methods approach are relatively scant (Prakash & Das, 2022).
We use a sequential mixed-method approach where a qualitative study forms the exploratory stage, and the insights derived from the qualitative study are used in a confirmatory quantitative study (Venkatesh et al., 2013).
Qualitative study rooted in the inductive approach helps gain deeper insights from the context and identify relevant constructs (Johns, 2006; Teddlie & Tashakkori, 2009), which then is tested through quantitative analysis (Tashakkori & Creswell, 2007).
In the qualitative study (study 1), we explored the processes and mechanisms by which IT professionals evaluate technology on its relevant features.
In a robust quantitative study (study 2), we explained the impact of the misfit/fit between 'expected' and 'experienced' features of technology on IT professionals' work outcomes.
The current study design is consistent with past empirical studies (Hossain & Dwivedi, 2014).
4. Study 1
4.1. Overview
Studies have indicated the possible impact of technology on IT professionals' work lives.
However, there is lack of comprehensive studies explaining how IT professionals' perception of a given technology influences their work outcomes, such as turnover intention and work exhaustion.
This existing gap in literature motivated us to adopt an exploratory approach to start the study.
We wanted to understand the features of technology from IT professionals' perspectives and its relevance in their work.
We further wanted to explore the influence of technology on IT professionals' work outcomes.
With these broad themes as our research objective, we conducted semi-structured interviews of 35 IT professionals in four months.
These respondents were software developers and coders, having work experience ranging from two to six years.
They worked in large servicebased firms in India and were trained in different technologies involved in the software development process.
We shared a broad and brief overview of the study and sought consent from these IT professionals through digital platforms.
Respondents were then recruited for the qualitative study based on their consent, interest, and suitability to participate in the study.
Appendix 1 contains respondents' profiles.
4.2. Method 
We followed the constant comparative method on an iterative basis to analyze the data (Kotlarsky, Scarbrough, & Oshri, 2014; Ladge, Clair, & Greenberg, 2012).
Initially, data collection and data analysis were conducted sequentially and then concurrently.
We started analyzing data after the first five interviews.
We iteratively moved back and forth between data and theoretical themes, refining the emerging framework.
We employed a three-stage data analysis process (Locke, 2001; Pratt, Rockmann, & Kaufmann, 2006).
In Stage 1, data was organized into first-order codes based on the similarity of opinions and expressions.
These categories were revised as we analyzed and grouped narrations.
In Stage Two, we attempted to create convergence across the first-order thematic codes by comparing within and between themes.
After Stage 2, we developed abstract and broad theoretical dimensions fitting the overall data.
In Stage 3, we continued converging the themes into more generalizable theoretical categories.
We also attempted to explore the relationships among these theoretical categories to develop an overall understanding of the phenomenon.
For instance, how features of technology explain behavioral themes such as work exhaustion.
The research protocols and the process followed for data analysis are provided in Appendix 2.
We analyzed the statements/narrations to identify the underlying theoretical themes.
These theoretical themes were aggregated based on emerging similarities in theoretical dimensions.
While categorizing the data, we tried to reach a consensus in coding by resolving any conflict through discussions, reviewing relevant literature, and active support from independent raters.
We collected and examined expert reviews for all the categories for two reasons: (a) to minimize researcher biases and (b) to ensure that the theoretical dimensions have emerged from the data.
4.3. Results 
Based on the qualitative study, we have identified three features of technology that IT professionals value.
The three features are: long-term career consequences, perceived challenge, and the uncertainty encountered while working with a technology.
Details of the features are provided below.
These features helped us understand IT professionals' career expectations from the technology they were working with and subsequent work outcomes.
4.3.1. The long-term career consequence of technology
Prevalent use of certain technologies in the IT industry influences IT professionals’ interest to work in that technology. 
Some respondents mentioned that a given technology enhances chances of getting good projects inside the organization and increases the possibility of getting better job offers outside the organization. 
"The primary concern for me is ’on bench’ time…You do not add value to your profile, and it induces stress and a sense of job insecurity. 
I want to work in technologies where lots of projects are available in my organization.
" (R 22) "Technologies that are widely used across the market provide advantages in terms of career growth. 
It is a cycle, you acquire expertize and experience in these technologies, and with that experience, you get better work opportunities, and your profile becomes more promising…" (R 19) 
This feature of technology denotes prevalence of the technology and the importance of technology-related skillsets, both inside and outside an organization. 
Based on our analysis, we arrived at two inferences. 
One, IT professionals believe that technologies prevalent in the market provide better career opportunities within their organizations. 
For example, respondents reported that working with these technologies provided better opportunities for securing in-house projects. 
Two, the experience gained by working with these technologies provided better opportunities outside the organization. 
We found associated self/peer esteem among respondents working with these technologies. 
Based on the long-term impact of a given technology on both internal and external career outcomes of IT professionals, we named it as "long-term career consequences of technology."
4.3.2. Perceived challenge
The work of IT professionals is woven with a set of technologies.
However, while some respondents derive motivation from the challenging nature of the technology they are working with, others prefer working with relatively easy technologies.
Thus, one set of respondents complained about the extent of complexity and the challenge of working with a given technology.
Another set of respondents was disappointed with the repetitive and mundane nature of their work, associated with the technology.
Every day, I work on similar problems.
It is like a cut - paste job; usually, whatever I do is a repetition of the previous day's job.
We extensively use search engines like Google for our major work: where is the challenge ?.
(R 31) A few respondents shared that they were satisfied and motivated by the technology they were working with.
I am a part of the team which controls retail solution for a leading pharmaceutical company in Europe:over the past years, we have worked very hard for updation of the software and reportedly, the company has seen a rise in profit figures after implementing our system.
Solving challenging problems, fixing bugs:my job last year has given me a lot of intellectual satisfaction.
(R 1) From the responses of IT professionals, we inferred the following.
Individuals seek 'challenge' in the technology; however, magnitude of the challenge is perceived differently by different individuals.
Thus, perception about technology as less challenging or more challenging depends on differences in its evaluation at the individual level.
A perceived misfit with the challenge associated with current technology may lead to unfavorable work outcomes.
4.3.3. Technology uncertainty
From the career perspective, IT professionals consider uncertainty associated with technology as a crucial feature. 
Frequent changes or updates in technology are common in their profession. 
Our analysis revealed that IT professionals assess technology uncertainty based on their career-related preferences. 
One set of respondents was averse to working with technologies that undergo continual updates, owing to the additional efforts needed to acquire newer technological skills. 
Other respondents stressed the importance of a healthy balance between work and life while working with a given technology. 
"Technologies keep changing…some survive and evolve with new updates, and some vanish… 
Moreover, fate of your career is closely related to the technology you are working with. 
It is possible to learn new technology, people keep doing that, but it is not easy…" (R9) 
"My current technology is distributed system based, so I have to work according to US timing as I constantly communicate with the client team. 
Though that gives me good technological exposure, I get very little time to spend with friends and family members. Most of the time, I sleep at home. 
This work situation is causing much stress to me." (R23)
On the other hand, another set of respondents derived satisfaction from learning new technologies; they perceive uncertainty as an opportunity to progress in their careers. 
Moreover, some respondents valued career growth more than any other work outcome. 
"After [SAP] acquisition, Business Object Data Integration (BODI) tools have some changes in its interface. 
Due to the new updates, we must ensure our team keeps acquiring the expertize. 
Being at a junior level, I am one of the few employees who are experts in the usage of all these modules. 
I often take knowledge transfer sessions, even for seniors and managers. 
That gives me immense satisfaction." (R 33) 
"My career is very important to me; that is why I always opt for more challenging work. 
I can rate myself as an expert in my very complex technology framework and keep updating frequently. 
I understand I have to compromise on the personal front, but for steady career growth, that is just a price to pay…" (R11) 
Thus, updates and obsolescence are critical features of technology that affect the work outcomes of IT professionals. 
During our analysis of data transcriptions, we observed differences among IT professionals in expectations and willingness to learn the additional skill sets related to newer technologies. 
While one set of respondents marked continuous skill update as an opportunity for professional development, another set perceived it as a stress-inducing task due to additional workload. 
Such differences in opinions and preferences lay the foundation for individual-specific evaluation of a given technology. 
The findings indicate that technologies are perceived and evaluated differently by IT professionals based on their preconceptions and preferences. 
As already discussed, we found three significant features of technology. 
IT professionals use these three features as references to assess the fit/ misfit between technology-related expectations and the technology they are working with. 
The above findings motivated us to explore the fit/ misfit between expected and perceived features of technology on IT professionals’ work outcomes.
4.3.4. Influence on work outcomes
During our analysis of data narratives, we found that IT professionals’ evaluation of fit or misfit between expected and experienced technology has adverse consequences on their work outcomes. 
For example, one respondent (Respondent 35) perceived that if he continued with the technology he was working with, there would be limited job opportunities in the future. 
Hence, he was looking for another job opportunity. 
"I am working with a database technology which is now being replaced by better technology. 
I don’t see a good career in this technology. 
Where will I go if I want to change (organization), fewer people are using this backend. 
I always feel insecure and stressed about my job prospects. 
I am trying to find a better job where I can get exposure to a different technology." (R 35) 
Respondents also reported the complexity of technology and its consequences. 
According to some respondents, the complexities of a given technology force them to struggle to fulfill the task at hand and hence experience stress. 
"This technology is very complex. every day, new challenges. 
Eventually, I struggle to even finish my work on time.often, I feel exhausted. 
My problem is not that I have to spend more time in the office, that is ok, the problem is whatever time I spent writing code (in the given technology), it puts me under so much pressure that I get tired. 
End of the day, the thought that I have to continue the same struggle the next day is distressing." (R29)
4.4. Study 1 conclusion
The findings indicate that different IT professionals evaluate a given technology differently based on their preconceptions and preferences.
We found three significant features of technology: long-term consequences of working with a technology, perceived challenge, and technology uncertainty.
IT professionals assess the fit between their technology-related expectations and the technology they are working with on these three features.
They evaluate different features of technology based on the technology they are currently working with.
Our findings suggest that the misfit between expected and perceived features of technology leads to adverse work outcomes such as turnover intention and work exhaustion.
5. Study 2
5.1. Overview
The qualitative study helped us understand how IT professionals evaluate a given technology and its features, and how this evaluation impacts their work outcomes.
Despite being one of India's thriving industries, the IT industry has many issues that call for rigorous research.
For example, compared to non-IT professionals, IT professionals are more prone to turnover and work exhaustion (Ahuja et al., 2007; Armstrong et al., 2015).
Though scholars have added valuable insights to understand the turnover intention and work/emotional exhaustion, IT professionals' attrition is still a challenge for the industry (Lin, 2020).
Scholars have suggested that unmet expectations in the workplace lead to adverse employee outcomes (Kristof-Brown, Zimmerman & Johnson, 2005) however, extant research has not paid adequate attention to explore how technology, through its inherent features, can lead to unmet expectations.
We believe that understanding the expectations from technology would explain IT professionals' turnover intention and work exhaustion.
5.2. Theoretical background and hypotheses development 
A set of pre-exposure beliefs about an object or an activity forms an employee’s expectation (Susarla, Barua, & Whinston, 2003). 
Such expectation guides the employee’s frames of reference for subsequent evaluations. 
The discrepancy between expectations and actual experiences leads to disconfirmation (Venkatesh & Goyal, 2010). 
An experience that is better than the expected one leads to positive disconfirmation. 
On the other hand, an experience that is worse than expected leads to negative disconfirmation (Kopalle & Lehmann, 2001). 
Thus, the EDT’s basic argument is that individuals cognitively evaluate experiences with expectations. 
Discrepancy leads to subjective calculations of disconfirmation. 
Thus, EDT suggests "reactions toward an object or an event as a function of prior expectation and disconfirmation" (Bala & Bhagwatwar, 2018, p. 652). 
Though EDT has been used in IS research to explain information technology usage (Bhattacherjee, 2001), it has been used primarily to explain technology users’ behaviors (Bhattacherjee & Premkumar, 2004). 
For example, EDT has been used to explain users’ intentions towards e-service usage (Chea & Luo, 2008), e-learning services (Lee, 2010), online rating behavior (Ho, Wu, & Tan, 2017), and disposition towards a job or organization (Bala & Bhagwatwar, 2018). 
The present study extends the expectancy disconfirmation theory by studying work outcomes of IT professionals who are not passive users of technology; they are active developers who work in different technologies. 
This section hypothesizes expectation disconfirmation about features of technology and its impact on employee turnover intention and work exhaustion.
5.2.1. Long-term career consequence
IT professionals have varied career expectations, and they actively engage in managing their careers (Ahuja et al., 2007; Igbaria et al., 1991; Lim & Teo, 1999). 
Technology plays a significant role in influencing IT professionals’ career prospects (Hsu et al., 2003; Moreno, Sanchez-Segura, Medina-Dominguez, & Carvajal, 2012; Woszczynski, Dembla, & Zafar, 2016). 
Long-term career consequence is defined as an outcome of working with a technology that pays off in the future (Chau, 1996). 
Respondents from Study 1 narrated that the technology they work with influences opportunities and future career growth potential. 
Study 1 also suggests that a given technology can influence career opportunities. 
Prior studies have also cited technology-related career consequences such as professional advancement, flexibility to change work, peer esteem, and meaningful work opportunities (Fu & Chen, 2015). 
However, individual preferences such as career aspirations and expectations are crucial factors impacting work outcomes. 
Employees’ perception of growth opportunities available in the work assignment is a significant factor explaining their decision to quit or continue in the organization (Hsu et al., 2003; MacCrory, Choudhary, & Pinsonneault, 2016). 
Scholars further argued that when the given technology does not fulfill their career goals, individuals are more likely to seek other opportunities to seek career advancement (Armstrong et al., 2015; MacCrory et al., 2016). 
Studies have found that unfulfilled individual career expectations induce turnover intention among IT professionals (Igbaria et al., 1991; Jiang & Klein, 1999). 
Thus, we hypothesize that a lack of fit between expected and experienced features of a given technology (i.e., the technology that the IT professional is currently working with) is related to increased turnover intention.
H1(a). The misfit between expected and experienced long-term career consequences of working with a given technology is positively related to the turnover intention of IT professionals
Unfulfilled career expectations and their impact on turnover intention are consistent with the expectancy disconfirmation theory (Brown et al., 2014; Irving & Meyer, 1994). 
Career outcomes of working with a technology enhance IT professionals’ feeling of competence and professional efficacy (Wang et al., 2020). 
While the fulfillment of psychological needs such as, perceived competence, enhances mental well-being, lack of such fulfillment diminishes motivation and well-being (Ryan & Deci, 2000). 
In fact, it may lead to one’s belief that they may not take control of their career. 
Scholars argue that "control over one’s career provides individuals with opportunities to adjust their work according to their needs, abilities, and circumstances" (Armstrong et al., 2015, p. 718). 
On the other hand, lack of control may lead to deleterious consequences such as work exhaustion (Armstrong et al., 2015). 
Moreover, the expectancy disconfirmation theory argues that a mismatch between expected and experienced work induces stress among individuals (Lankton, McKnight, Wright, & Thatcher, 2016). 
Thus, we propose that unmet career expectations about the long-term consequences of working with a given technology will induce work exhaustion.
H1(b). The misfit between expected and experienced long-term career consequences of working with a given technology is positively related to the work exhaustion of IT professionals.
5.2.2. Perceived challenge 
Individuals differ in capabilities and self-efficacy (Compeau & Higgins, 1995). 
As a result, different individuals may perceive the challenges associated with a given technology differently. 
The above argument can be supplemented with ability-demand (what a person can achieve against the work demand) misfit. 
This mismatch might occur in both situations, i.e., when the experienced challenge exceeds the expected challenge and vice versa. 
When the experienced challenge is more than expected, an individual will experience an increased workload. 
The individual has to meet excess workload beyond their abilities and expectations. 
These mismatches often lead to unmet expectations. 
Drawing from EDT, scholars (Brown et al., 2014; Kristof-Brown et al., 2005; Verquer, Beehr, & Wagner, 2003) posit that individuals would be more satisfied when expectations are more closely aligned with job experiences their work, thus reducing their turnover intention. 
On the other hand, when individuals feel that continuing on the job will not alleviate unmet expectations, it may lead to adverse outcomes such as increased turnover intention (Armstrong et al., 2015) and stress (Brown, Venkatesh, & Goyal, 2012; Edwards & Rothbard, 1999). 
IT professionals invest huge personal resources in learning the technology. 
As a result, the exit barrier from one technology to another is very high for these professionals. 
As technology is salient for these professionals, the expectation disconfirmation theory seems to be a suitable framework to explore IT professionals’ work outcomes related to technology. 
IT professionals can take a highly challenging technology as an opportunity for professional and career growth. 
However, the additional workload of working with a challenging technology can induce exhaustion and intent to find alternate work options. 
For instance, in Study 1, multiple narrations explained how perceived challenges associated with the technology could create positive and negative disconfirmations. 
In another study, scholars noticed the misfit in expectation and experience reduces the career satisfaction of these professionals (Tomer & Mishra, 2019). 
Based on the above discussion, we argue that a misfit between expected and experienced challenges in a given technology will induce turnover intention among IT professionals.
H2(a). The misfit between the expected and experienced challenge of working with a given technology is positively related to the turnover intention of IT professionals.
Challenging work is a subjective construct, influenced by selfefficacy, individual abilities, and intent to work (Fu, 2011). 
Study 1 explores the difference in the perceived challenge and highlights how IT professionals evaluate a given technology differently based on their orientation. 
For instance, IT professionals may derive meaning from challenging work as it is a medium to attain professional competence and satisfaction. 
When the perceived challenge is higher than their expectations, another set of professionals views it as a source of additional workload. 
Perceived challenge can lead to increased workload and exhaustion. 
This assertion is consistent with the ’met expectations hypothesis,’ which suggests that "any difference between expectations and experiences is bad" (Brown et al., 2014, p. 734). 
Hence, technology can be attractive or unattractive to individuals based on their preferences and expectations (Leonardi, 2011; Speier & Venkatesh, 2002). 
For instance, misfit of expectations and experience in a given technology can be caused by either lack of challenge in work (Edwards & Rothbard, 1999) or increased workload. 
In both cases, perceived misfit acts as a stressor and leads to work exhaustion (Rigas, 2009; Vogel, Rodell, & Sabey, 2020). Thus, we hypothesize,
H2(b). The misfit between expected and experienced challenge of working with a given technology is positively related to work exhaustion of IT professionals.
5.2.3. Technological uncertainty
Technological uncertainty reflects the "perceived speed of technological change in a given industry" (Atuahene-Gima & Li, 2004, p. 583). 
Constantly evolving technologies might increase technology-related uncertainty among IT professionals (Korunka, Weiss, Huemer, & Karetta, 1995; Moreno et al., 2012). 
Thus, technology advancements put pressure on IT professionals to invest time and effort to acquire newer skillsets (Armstrong et al., 2015; Shih et al., 2013). 
For IT professionals, it represents a risk that can lead to professional obsolescence and subsequent job insecurity and turnover (Joseph & Ang, 2001). 
However, it also be perceived as an opportunity to reskill, thus offering professional advancement (Speier & Venkatesh, 2002). 
Thus, the consequence of technological uncertainty depends on the individual’s expectations, and experiences of uncertainty related to the technology they are working with. 
Study 1 presents the impact of uncertainty through multiple and often contradictory perspectives. 
Individuals may prefer a continuously changing technology over an established technology, allowing them to build technology-related proficiency and acquire newer skill sets. 
On the other hand, individuals can also perceive changing technology as a threat as it imposes additional efforts to acquire new skill sets, failing which they may face the threat of professional obsolescence. 
Professional obsolescence is also connected to career commitment and intent to leave (Fu, 2011). 
Thus, the experienced technology-related uncertainty that does not fit expected uncertainty levels will likely impact IT professionals’ turnover intention. H3(a). 
The misfit between expected and experienced uncertainty of working with a given technology is positively related to the turnover intention of IT professionals. 
Individuals perceive that learning or upgrading technology-related skills are an additional workload requiring additional time and effort. 
Working with a stable technology means consistent task routines, which reduces the demand for continuous decision-making and coordination (Limayem, Hirt, & Cheung, 2007). 
However, studies have argued that demand per se need not lead to adverse outcomes. 
For example, when the demand is self-imposed, it can minimize resulting stress and exhaustion (Benamati & Lederer, 2001). 
Hence, individual motivation for acquiring newer technology-related skills might contribute to positive/negative evaluation of specific technologies (Chang, 2010; Rong & Grover, 2009; Speier & Venkatesh, 2002). 
Thus, coping with continually changing technologies is individual-specific and depends on personal competence and willingness to manage demands. 
Study 1 also explores differences in perceived disconfirmation with technology uncertainty. 
While a set of respondents considered uncertainty as a way to improve professional competence and stay updated, another set of respondents considered uncertainty as a source of stress. 
To explain IT professionals’ work outcomes, we borrowed from the EDT that argues that the mismatch between expected and experienced jobs induces adverse outcomes (see Brown et al., 2014). 
We predict that the mismatch between expected technology-related uncertainty and the experienced technology-related uncertainty may lead to enhanced perception of deskilling and fear of professional obsolescence (Braverman, 1974), which are identified as predictors of work exhaustion (Ayyagari et al., 2011; Moore, 2000).
H3(b). The misfit between expected and experienced uncertainty of working with a given technology is positively related to the work exhaustion of IT professionals.
5.3. Method
5.3.1. Sample characteristics
We employed a web-based survey to collect responses and shared the link to the survey along with the cover letter.
Our sample comprises IT professionals involved in the software development process as coders and developers.
They were employed in leading service-based IT organizations in India.
We contacted 630 IT professionals in the top six service-based firms (based on revenue) over email and digital platforms.
We received 410 responses (response rate of 65%) from IT professionals; 386 were usable.
Of these, 58% were between 24 and 28 years, 35% were 29-32 years, and the rest were above 32 years of age.
Respondents have an average experience level of 38 months (median 36 months) in the current organization.
Also, they have an average experience of 31 months (median 24 months) in the current technology they are working with.
About 27% of the respondents were female professionals.
About 34% of the respondents were married, and 10% were married and had children.
All the respondents were graduates, out of which about 11% had completed their post-graduation degree.
5.3.2. Measurement
We used valid scales to capture the studied variables to address the issue of measurement errors (Zwanenburg & Qureshi, 2019).
We adapted the scale developed by Chau (1996) to measure the long-term career consequence of working in a particular technology domain.
The scale items were framed to assess whether the technology provides better work opportunities, increases peer esteem, and provides job security and flexibility.
We adapted the Job diagnostic survey developed by Hackman and Oldham (1976) to measure perceived challenge.
The three-item scale captures the respondents' linking for simple, repetitive tasks or preferences for using judgment in their task.
We used the 5-item scale adapted from Jaworski and Kohli (1993) to measure technology uncertainty.
It captures the perception regarding how difficult it is to assess the future of technology in the next 2-3 years, whether technological changes provide opportunities and whether there were significant developments in technology.
We measured turnover intention using a three-item scale developed by Cammann, Fichman, Jenkins, and Klesh (1979).
We borrowed the work exhaustion scale from Moore's (2000) work on exhaustion among IT professionals.
We captured the items on a 7-point scale.
We followed a validation process, as suggested in a prescriptive work by Aguinis and Edwards (2014), on conducting robust research.
In our scale design and validation exercise, we requested five of our colleagues to review the appropriateness of the scale items selected.
We prepared a document that thoroughly discussed the definitions of the constructs and listed down the items of the respective scales.
They were asked to read and understand the construct and, based on their judgments, evaluate the scale's ability to assess the given construct.
After sharing the document, we discussed it with them and collected their feedback.
All of them consented that scale items are measuring the defined constructs appropriately.
We made minor changes in the language/selection of words based on their suggestions.
We conducted a pilot study among a sample of 68 respondents who have worked as IT professionals.
All the constructs in the pilot study displayed acceptable reliability scores.
In our final dataset, we found the factor loadings were statistically significant; reliability scores were above.70.
As self-report might lead to common method variance, we followed Podsakoff, MacKenzie, Lee, and Podsakoff (2003) 's suggestions to minimize CMV.
We employed procedural measures to maintain psychological separation (using different anchor points) while capturing the data.
Also, we conducted a posthoc analysis (Herman's single factor test) to check the possibility of CMV in our study.
The first factor explained 24% (principal component analysis) and 22% (principal axis rotation), indicating CMV may not be a concern in the present study.
The internal consistency reliability score (see Table 1) for all the scales was greater than 0.70 (Mackenzie et al., 2011), suggesting good convergent reliability.
Results of our discriminant validity analysis are presented in appendix 4.
All the items loaded well onto their own constructs (loadings > 0.70) and poorly loaded onto other constructs (cross-loadings < 0.50) (Fang et al., 2014; Mackenzie et al., 2011; Qureshi et al., 2009).
We also conducted Discriminant validity test as per the Fornell-Larcker criterion (Fornell et al., 1981).
We found that the square root of average variance extracted (AVE) for each construct exceeds the correlation of that construct with other constructs (Appendix 5).
The test results show good discriminant validity.
We captured age (Moore, 2000; Sarker, Sarker, & Jana, 2010); gender (Sarker et al., 2010); marital status (Igbaria et al., 1991); length of experience in the current technology (Ayyagari et al., 2011); promotability (Ahuja et al., 2007); and negative affectivity (Moore, 2000) of the IT professionals as control variables.
These variables are related to turnover intention and work exhaustion.
5.3.3. Analysis
We checked the normality (Shapiro Wilks test), skewness, kurtosis, heteroscedasticity (Cook-Weisberg test), and multicollinearity (Variance inflation factor valued between 1 and 2) issues in the data.
We did not find any data anomaly at this stage.
Since the analysis involves higher-order polynomials in the equations, we mean-centered our variables (Edwards, 2002) to minimize multicollinearity in the results.
Since higher-order terms are more vulnerable to the outlier effect, we employ the indicative test such as Cook's D and standardized residuals test for each regression equation to eliminate the possibility of influential outliers.
None of the constructs display any abnormalities, such as lack of variation or problematic deviations, as displayed in descriptive statistics (Table 1).
However, since higher-order terms were derived from lower-order terms, Table 1 shows the higher correlation among lower-order terms and other higher-order terms in many instances.
Studies have recommended the use of commensurate measures, which means that both component variables refer to the same content dimension (Edwards, Cable, Williamson, Lambert, & Shipp, 2006; Edwards & Rothbard, 1999).
We employed commensurate measures to capture the fit between the individual and the features of technology.
Recent studies (Brown et al., 2012) have also favored the usage of commensurate measures as these measures ensure that we approach the measurement of fit on the same dimensions.
We have summarized the details of the scales used for both the components in Appendix 3.
To check for the non-response errors, we conducted t-tests.
We found no significant difference among early and late responses (Armstrong & Overton, 1977) on any of the variables collected for this study.
Thus, non-response error is not a concern in the present study.
In step one, we did a comparative analysis of unconstrained and constrained equations to verify that the value of squared R of unconstrained equations is significantly higher than constrained equations (Edward & Parry, 1993).
This comparison also confirms the support for the theoretical model of higher-order equations.
For example, a linear difference score equation can be expressed as:.
Where WO is the work outcome and b0, b1 represents the regression coefficients.
P and T are the expected and experienced features of technology, respectively.
We found that R2 of unconstrained equations was higher than the constrained equation in each case, confirming the suitability of higherorder terms.
We conducted a Polynomial Regression Analysis (PRA) and the Response Surface Method (RSM) to test our hypothesis.
Instead of traditional methods like difference score, PRA-RSM has been extensively used in IS literature (Benlian, 2013; Brown et al., 2012; Lankton et al., 2016; Sedera & Atapattu, 2019; Yang, Kang, Oh, & Kim, 2013).
Research models based on PRA-RSM avoid the restrictive assumptions of linear regression models, offer a meaningful variety of interpretation and theoretical inferences, and yield reliable results (Edwards & Parry, 1993; Klein, Jiang, & Cheney, 2009).
It allows us to understand and explore the magnitude of the misfit (component variables) and the directionality of the misfit in a three-dimensional representation.
Further statistical tests (reported in Table 2) helped us explore contours and details of the plotted surfaces.
The PRA-RSM projects a more accurate picture of component and outcome variables.
Research models based on PRA offer a meaningful variety of interpretation and theoretical inferences and yield reliable results for fit/ misfit research (Edwards & Parry, 1993; Klein et al., 2009).
In their methodological research work, Venkatesh and Goyal (2010) recommends PRA-RSM as the right approach to test fit/misfit in the context of expectation-disconfirmation theory.
5.4. Results 
The first set of hypotheses [1(a) and 1(b)] proposed that the misfit between the expected features of technology (P) and the experienced features of technology (T) increases the turnover intention.
We tested these hypotheses for each technology-related feature, i.e., long-term career consequence, perceived challenge, and technology uncertainty.
We entered control variables simultaneously in the regression equation, as directed by recent studies (Ahuja et al., 2007).
The work outcomes of IT professionals (precisely their turnover intention and work exhaustion) were regressed on control variables, expected feature of technology (P), experienced feature of technology (T), the squared value of expected feature of technology (P2 ), expected feature of technology times the experienced feature of technology (P ? T), experienced feature of squared (T2 ).
The regression equation used to test these relationships is shown below (control variables are omitted for simplification).
After performing polynomial regression analysis (PRA), we conducted additional tests to determine the slopes and curvatures along two critical lines of interest, i.e., congruence line (T = P) and incongruence line (T = -P).
The first test of hypotheses in PRA was concluded by nonzero R squared terms.
All hypotheses generated significant R squared values through PRA (R square value range from 0.16 to 0.38).
However, regression coefficients' interpretation through their significance is difficult in PRA compared to other regression methods.
Therefore, we generated and analyzed response surface graphs for each hypothesized relationship (Figs. 1 and 2).
Our understanding of the relationship between different variables is augmented with surface graphs and four surface test values: a1, a2, a3, and a4 (Edwards, 2002).
These surface values are calculated through unstandardized beta coefficients for the centered predictor variables, as expressed in Eq. (1).
The slope of the congruence line (P = T) as related to work outcomes (turnover intention and work exhaustion) is given by a1 = (b1 + b2), and curvature along the line of congruence as related to work outcomes is assessed by calculating a2 = (b3 +b4 +b5).
Similarly, the slope of the line of incongruence/discrepancy as related to work outcomes is assessed by calculating a3 = (b1 - b2), and curvature is assessed by calculating a4 = (b3-b4 +b5).
Results of the polynomial regression analysis are presented in Table 2.
According to Edwards and Parry (1993), a significant congruence effect exists when the higher-order terms (i.e., P2 , T2 , and PxT) are jointly significant.
We found F-test for three higher-order terms significant, as shown in Table 2.
Thus, our hypotheses were supported: misfit between expected and the experienced features of technology exhibited a significant relationship with turnover intention and work exhaustion.
We further tested the hypotheses based on the significance and direction of surface values.
The surface values (a1, a2, a3, and a4) in our analysis provided interesting insights on incongruence or discrepancy among the predictor variables.
While we have theorized a linear relationship, studying slope and curvature along the line of incongruence allows us to look at the phenomenon at a deeper level.
For instance, we can explore the outcome when discrepancy (1) is positive/negative; (2) increases sharply; (3) decreases sharply.
From the linear coefficients (slope a3), we found a significant slope (supporting Hypotheses 1(a) and 3(b)) along the incongruence line.
The findings signify that incongruence leads to higher work outcomes.
Hypotheses 1(b), 2 (a, b), and 3(a) did not find support, as revealed by insignificant values of coefficient of slope along the incongruence line.
However, based on significant values of curvature (a4) along the line of incongruence (X = -Y), we found that work outcomes increased sharply with increasing discrepancy between both the predictor variables, thus providing support to the hypothesized relationship [1(b), 2 (a, b) and 3(a)].
The results also indicated that outcomes vary with the magnitude of the predictor variables, which means higher outcomes when values at the X-axis are higher than those at the Y-axis.
For instance, Hypothesis 1(a) gets supported as turnover intention spikes to a higher value when the expected value of long-term consequences (X-axis) is higher than the perceived long-term consequences of working with the technology (Y-axis).
5.5. Study 2: conclusion
Using EDT as a theoretical framework, we tested the impact of perceived misfit between expected and experienced features of technology on turnover intention and work exhaustion.
Employing polynomial regression analysis and response surface method, we explained how misfit (both in magnitude and direction) impacts turnover intention and work exhaustion.
It is interesting to see how IT professionals correlate career expectations with a given technology, and their attribution of career and work outcomes to technology is a unique phenomenon.
6. Discussion
We expanded the scope of existing research on IT professionals' work outcomes (Armstrong et al., 2015; Joseph et al., 2015; Moquin et al., 2019; Shih et al., 2013) by explaining how a given technology (through its inherent features) can influence turnover intention and work exhaustion among IT professionals.
Employing a mixed-method approach, we inductively explored features of technology (long-term consequences, perceived challenge, and technology uncertainty) that hold the potential to explain IT professionals' behavior.
Recent work highlights that technology changes (Kaplan & Lerouge, 2007; Sharma & Stol, 2020) and the constant need for professional updates (Gaudioso et al., 2017; Venkatesh et al., 2020) could induce exhaustion from work among IT professionals.
Study 1 and Study 2 explain how the features of technology are linked with work and career expectations and impact the work outcomes among IT professionals.
Thus, this study contributes to enhance the present understanding of technology as a construct and its impact on IT professionals (Korunka et al., 1995; Rong & Grover, 2009; Speier & Venkatesh, 2002; Yi, Jackson, Park, & Probst, 2006).
Study 1 directed us to the perceived fit/misfit phenomenon and improved our understanding of individual-technology interface in the context of IT professionals.
Using EDT as a theoretical framework to examine fit/misfit, this study links EDT to explain IT professionals' work outcomes.
Turnover intention and work exhaustion increase when expectations with features of technology are disconfirmed.
Individuals have a priori expectations with features of technology.
For instance, our study suggests that IT professionals' career advancement is tied up with the technology they are working with.
Their work outcomes are shaped by comparing their initial set of expectations with the experiences of working with a given technology.
Extant work has studied the impact of features of technology (such as complexity and technology uncertainty) on individuals (Joseph et al., 2015; Rong & Grover, 2009; Venkatesh et al., 2020).
Our study explains the nature of the relationship between individuals and features of technology as dyadic and explores it using the overall framework of expectations-disconfirmation.
Hence, the present study unifies EDT and individual-technology interactions in the IT professional's context.
In the present work, we have focused on turnover intention and work exhaustion as work outcomes related to the perceived misfit with technology.
Scholars observed that compared to non-IT professionals, IT professionals experience an increased level of stress, exhaustion (Ragu-Nathan, Tarafdar, Ragu-Nathan, & Tu, 2008; Joia & Mangia, 2017; Tarafdar, Tu, Ragu-Nathan, & Ragu-Nathan, 2007) and exhibit higher attrition levels (Joseph et al., 2015; Shih et al., 2013).
Niederman, Sumner, and Maertz (2007) explained that IT professionals could take multiple paths while contemplating leaving.
Understanding these paths may enable early interventions to ensure that their present work is relatively superior to other available opportunities.
We believe that exploring features of technology and its impact on turnover intention can explain multiple paths and interventions to minimize turnover intention.
6.1. Theoretical contribution
The present study contributes to extant research in the following ways. 
Using an inductive approach, we have explored three unique technology-related features relevant to IT professionals’ work and careers. 
These three features of technology are long-term career consequences, perceived challenge, and related uncertainty. 
Findings of our study hold potential contributions to the literature dealing with the career of IT professionals. 
For instance, we can attribute IT professionals’ career mobility (Joseph et al., 2015) to individual’s orientation towards a given technology. 
Drawing from the overarching EDT (Bhattacherjee, 2001), we conceptualized a relatively novel phenomenon, i.e., how a given technology fulfills IT professionals’ career expectations. 
Our results confirm the impact of the perceived misfit between expected and experienced features of technology on turnover intention and work exhaustion. 
Our results explored how both magnitude and directionality impact the work outcomes of IT professionals. For example, work exhaustion and turnover intention become high when the expectation is more than the experienced feature of technology compared to when the experienced feature of technology is more than the expected feature of technology. 
This indicates that individuals with higher career-related expectations are vulnerable to adverse work outcomes (Oh & Ma, 2018). 
Thus, managing IT professionals’ career expectations become crucial to ensure positive outcomes. 
Studies examining the mismatch between expected and experienced features of a technology on IT professionals’ work outcomes are scarce. 
In the present study, we have extended the existing theory by investigating the misfit with features of technology as significant predictors of turnover intention and work exhaustion of IT professionals. 
Scholars in IS literature have called for novelty in research methods and approaches to generate robust theories (Venkatesh et al., 2013; Zachariadis, Scott, & Barrett, 2013). 
The use of both qualitative and quantitative methods is argued to be vital to "identify both internal (necessary) and external (contingent) relations" (Zachariadis et al., 2013, p. 875). 
Although there have been several calls for combining qualitative and quantitative research (Gable, 1994; Kaplan & Duchon, 1988), with maturing of the field of Information Systems in the last decade, there is increasing recognition that multi-method approaches hold the potential for providing deeper insights (Kim, Kankanhalli, & Lee, 2016; Venkatesh et al., 2013). 
Our paper demonstrates this potential by integrating qualitative and quantitative approaches.
6.2. Implications for practice
IT professionals experience higher stress levels and exhibit higher attritions than other professionals.
Our study explores and establishes features of technology as a critical predictor of work behavior among IT professionals.
We propose that devising human resource strategies that incorporate IT professionals' technology-related expectations in project/ technology allocation can be an effective way to ensure a better fit/ congruence between IT professionals and their work settings.
Our findings further suggest that organizations can take actions to bring positive orientation of IT professionals towards a given technology.
IT professionals work in an uncertain and dynamic technological environment.
Due to changing market requirements or technological changes, IT professionals must acquire newer technology-related skills and gain experience in a portfolio of technologies rather than retaining experience in a single technology.
These skill acquisition requirements often contribute to their perceived intent to leave and often cause stress.
We propose that organizations consider proactive measures to prepare the IT professionals to learn and adapt to the continually changing technology.
Preparedness to learn technologies would help reduce stress caused by technology-related uncertainty.
Moreover, organizations may devise orientation programs to help these professionals revisit their expectations from technology and acquire better-coping strategies.
Our findings suggest that organizations need to consider IT professionals' expectations from the technology they work with to minimize work exhaustion and turnover intention.
Moreover, organizations may explore mechanisms to set the expectations of IT professionals to minimize disconfirmations.
To minimize the occurrence of misfits, organizations should invest in understanding individual dispositions such as intent to avoid uncertainty or long-term career orientation and ensure appropriate career and professional development opportunities.
Organizations should also enable IT professionals to manage their career paths while working with an ever-changing technological ecosystem and, wherever possible, deploy easy-to-use, simple, and 'good enough' technology rather than always resorting to cutting edge ahead of curve technology.
Our findings among Indian IT professionals contribute to the scant research on the work dynamics of IT professionals working at Global Software Development (GSD) sites.
Such GSD sites, often located in emerging economies, face challenges in developing, acquiring, and retaining skilled professionals.
Our findings would be applicable in GSD environments of similar economies where many IT professionals are employed.
6.3. Limitations and directions for future research
The present study has certain limitations that must be acknowledged.
The findings assert that the misfit between individuals' expectations and experiences with a given technology leads to adverse outcomes.
One critical assumption in the present study is that the preferences of individuals are relatively constant over time.
IT professionals at various stages of their careers gain experience and exposure to several technologies.
Thus, their expectations and preferences are likely to change over time.
Future studies may explore whether perceptions about technology change with time.
Longitudinal research design, such as latent growth modeling and growth mixture modeling (Chan, 1998; Qureshi & Fang, 2011), to observe and analyze changes in perception over a given period will help understand the long-term impact of a given technology.
Also, studies can explore the fit/misfit with technology at different career stages of these professionals to understand their work behaviors.
Studies may investigate how employees learn to manage the incongruence between their expectations and experiences with a given technology.
We found significant coefficients along the incongruence line, which suggest that misfit has a more pronounced negative impact than the positive impact of perceived fit on both turnover intention and work exhaustion.
Though present research focuses on two outcomes (turnover intention, work exhaustion), future studies may explore the impact of perceived misfit on other work outcomes of IT professionals.
Even though we have taken a priori measures and post-hoc test indicates CMV is not an issue in the present study, our design has inherent limitations.
Future studies may consider robust procedural measures such as introducing marker variables (Lindell & Whitney, 2001) and maintaining temporal separation in the design to minimize the chances of CMV.
Another limitation of the study is its research context.
The study was conducted on IT professionals, for whom technology is a central construct.
Due to its specificity, extension to other occupations will call for major adaptations.
However, as technology has increasingly become central to several occupations, rebuilding the fundamental premise of technology on career consequences can be employed as the foundation for theory development in different work contexts.
Scholars should also study the role of technology diversity.
For instance, an individual might be involved in more than one technology while working on a project.
It could be possible that more than one technology leads to outcomes such as increased stress.
Future studies can address this concern by developing a typology for technology characteristics, where we can assign technology characteristics to specific technologies based on generalizable standards.
The study can also be extended to different socio-cultural contexts.
Considering that invasion of technology and information overload (Yin, Ou, Davison, & Wu, 2018) are very high in IT professionals' lives, future studies can also explore the dark side of being an IT professional (Turel et al., 2019).
Our findings open an avenue to understand the adverse influence of technology on IT professionals' work life, such as continuous fear of obsolescence, coping with learning new technologies, and experiencing stress while working with complex technologies.
7. Conclusion 
Considering the high attrition and stress among IT professionals, our findings provide novel predictors of these outcomes.
We have examined how IT professionals evaluate technology through perceptive measures of career-related outcomes.
Specifically, our results indicate that professionals evaluate technology on three important features: long-term consequences, perceived challenge, and technology uncertainty.
The findings establish a significant relationship of fit/misfit between individual expectations from a given technology and the technology they are working with on work outcomes such as turnover intention and work exhaustion.
By highlighting the importance of fit/ misfit in a technologydominant profession, our study draws attention to the most crucial resource, i.e., the IT professionals.
CRediT authorship contribution statement
Gunjan Tomer: Conceptualization, Methodology, Formal analysis, Investigation, Resources, Visualization, Writing - original draft.
Sushanta Kumar Mishra: Conceptualization, Methodology, Formal analysis, Resources, Visualization, Writing - original draft.
Israr Qureshi: Formal analysis, Validation, Resources, Visualization, Writing - original draft.
Appendix 1. Detailed description of respondents of the qualitative study
Appendix 2. Protocols and method employed in conducting the qualitative study
One of the three authors have conducted each interview.
The interview duration, on average, was around 45-60 min.
Our interviews tried to capture respondents' understanding and perceptions regarding the technology they were working with.
We started the interviews with objective questions regarding their job, such as years of experience, technology exposure, and training.
Further, we built over the initial foundation to understand more complex questions such as their experience and opinions while working with a technology (e.g., what is your comfort level with the technology you are working with? How do you see yourself in the next 2-3 years while working with your present technology? are you able to balance your personal and professional lives?).
We obtained prior appointments on emails and phones and communicated the confidentiality and anonymity of their responses.
We also ensured that they consider the interviews as a voluntary exercise and refuse to respond to any question.
All the responses were transcribed within 24 h of the interview.
We concluded our data recording after we achieved theoretical saturation as advised in the grounded theory methodology.
Appendix 3. Measurement scales used for Study 2
Responses are to be scored on a 7-point scale from (1) strongly disagree to (7) strongly agree. 
Appendix 4. Internal consistency reliability (ICR), item loading and cross loadings
Appendix 5. Result of Fornell-Larker criterion for discriminant validity
The Future of Cybersecurity: A System Dynamics Approach
Abstract
The Fourth Industrial Revolution is disrupting conventional practices in the digital space.
Cybersecurity strategies are considered increasingly central and crucial aspects of the transformations.
Cybersecurity relates to processes and networks designed intelligently to digitally protect unauthorized access.
In this work, the authors provide support into how businesses can align operations in the realm of cybersecurity driven by the technologies of industry 4.0 towards ensuring sustainability.
To this end, a System Dynamics Modelling (SDM) approach that considers simulating the impact of cybersecurity initiatives on a network and security solution business is developed.
As an initial step, the perceptions of network and security solution organisations are gathered.
Thereafter the cybersecurity initiatives gathered are used to explore the system dynamics model.
The results provide insights for strategic and tactical business decision support suitable to mitigate probable cybersecurity breaches and threats.
Keywords: cybersecurity initiatives; industry 4.0; sustainability; system dynamics model
1. Introduction
In this dynamic digital age, businesses are taking advantage of the confluence of technologies and trends provided by the Fourth Industrial Revolutions (4IR) to remain competitive.
Industry 4.0 relates to the current inclination for digitization, data exchange, communication, and automation [1].
Technologies of industry 4.0 such as robots, simulation, cybersecurity, additive manufacturing, cloud computing, big data, and analytics position organisations for transformative and radical change [2].
This paper focuses on simulation and cybersecurity two underlying aspects of 4IR, capable to provide advancements for businesses to track and resist varying security trepidations such as unauthorized access [3].
Cybersecurity is constantly challenged by the increasing activities of hackers, privacy, data loss, and risk management.
A large number of peer-reviewed publications discuss the importance and practices of cybersecurity with none currently suggesting that cyber threats and attacks will decrease.
This paper attempts to gather insight from network and security solution organisations and articulate the crucial cybersecurity initiatives with significant impacts on business sustainability.
An initial investigation with selected cybersecurity parameters is set to constitute the baseline of this paper through developing a system dynamics model (simulation) for strategic and tactical business decision support.
A simulation model is used to develop insights into crucial combinations of parameters aimed at achieving sustainable cybersecurity goals.
This is focused on overcoming the limitations of conventional models and tools that have been developed for understanding the sustainability of cybersecurity initiatives.
A value adds of this paper is to complement available literature on cybersecurity providing continuous research that provides information on understanding the crucial strategies necessary to counteract cyber risks.
This paper reviews network and security solutions organisations in Nigeria, a western African country.
Simulation is a key concept of 4IR suitable to design a business and predictive model for decision-making.
Recent literature highlights the role of simulation in the digitalisation of operations in a business.
To complement current research related to cybersecurity, this paper focuses on simulation for developing a decision-making model suitable to provide insights on the perceptions of network and security solution organisations.
The information gathered is used to explore the protocols of the system dynamics model for strategic and tactical business decision support.
[4] describe cybersecurity as the convergence of technologies, people, systems, and processes functioning together to protect businesses, networks, and individuals from digital theft, attacks, unauthorized access, damage, or disruptions in services.
Some notable cybersecurity threats collected from the literature [5] include: "Malware and ransomware infiltration such as viruses, worms, rootkits, trojans, and spyware", "Cyber fraud such as vishing, phishing, and whaling", "Remote attacks through password decryption, keylogging, and hacking", and "Social engineering".
To protect businesses from these threats, employees need to be aware of the conventions of the cybersecurity system.
An effectively designed cybersecurity system has multiple layers of safety or protection connected in horizontal or vertical integration across networks.
These multiple layers extracted from the literature [6] include "Physical security, Wireless security, Passwords, Two-factor authentication, Social engineering, Email security, and Anti-virus".
A comprehensive review of the literature [7] details five notable types of cybersecurity to include "Critical infrastructure security, Application security, Network security, Cloud security, and Internet of Things security".
Cybersecurity is important to govern the conduct of operations and interactions of businesses from internal and external threats.
Most business physical devices inclusive of software and hardware are connected to the internet with exposure to security breaches.
The benefits of implementing a sustainable cybersecurity protocol or initiative are extensively detailed in the literature [8] and include but are not limited to:.
Improvement of confidence in organisations and recovery time in case of a breach.
Protection for network architecture, data theft, and end user’s identifiable information.
Preventing unauthorized users from accessing digital assets.
Defending against ransomware, hacking, malware, viruses, social engineering, keylogging, unwanted programs, and phishing”.
Cybersecurity requires business considerations and prioritisation relative to budget, tools, people, governance, processes, and a leader capable to drive and implement sustainable changes. 
Important milestones in cybersecurity devolvement adapted from the literature [5] are detailed in Table 1.
Table 1. Important milestones in cybersecurity development.
The first commonly recognised computer virus know as creeper was found.
Massachusetts Institute of Technology (MIT) was granted a patent (Cybersecurity patent) for cryptographic communications methods and systems.
The advent of computer viruses resulted in the infection of a number of personal computers. 
This popularizes the context of cybersecurity as a household concern that influenced the development of more antivirus software.
The first defense conference that focuses on cybersecurity was held.
The first well-known hacker group referred to as anonymous was formed
The target breach occurred in which an estimated 40million debit and credit card records are stolen and accessed.
Yahoo reported two cybersecurity breaches involving hackers that gained access to data from over 500 million user accounts.
Equifax security breach occurred that exposed the personal information of an estimated 147 million users.
The general data protection regulation that focused on the protection of end-user data in the European Union was implemented. 
(2018) also witnessed the implementation of the California consumer privacy act that supports individuals’ right to control distinct personal information.
2. Methodology
2.1. Structured interview
A structured interview is a quantitative research approach most often used in survey research to gather information on predetermined questions. 
The structured interview is conducted in two phases. 
The first phase occurs immediately after the questionnaires are distributed to provide clarity on each set of questions defined. 
The second phase ensues after gathering the filled questionnaires to gather further insights on the perceptions of the target audience based on the predetermined questions.
2.2. Questionnaires
Questionnaires invented by the statistical society of London in (1838) are an inexpensive survey research tool suitable for gathering and interpreting multiple sources of data based on predefined sets of questions that are either open-ended or closed-ended.
The evidence comprising the questions together with responses in the questionnaire construct is guided on the inferences provided by [9] and designed in a standardized manner aligning with best-practice cybersecurity benchmarks detailed in Table 2.
The particular sets of questions and answer choices are selected as suitable to provide solid data about cybersecurity needs and drives as informed by relevant cybersecurity practitioners.
The answer choices are articulated such that it is easily configured in a simulation model suitable to assess the impact of change of each of the cybersecurity benchmark in the enterprise under consideration.
The questions associated with each cybersecurity benchmark are also captured.
Table 2. Cybersecurity benchmarks, descriptions, and associated questions
The authors review the database of registered network and security solution organisations available with the Nigeria cooperate affairs commission (cac.gov.ng). 
The information collected indicates a total of 197 registered network and security solution organisations. 
The authors design and circulate a total of 197 questionnaires among either of the “Chief information security officer, Security Engineer, or Security architect/analyst at the 197 registered network and security organisations. 
The questionnaires are distributed with an estimated one-month duration to gather feedbacks and conduct the structured interview to support data collected from the questionnaires.
2.3. Simulation
Simulation provides a sustainable method of analysis that can be easily verified, communicated, and understood.
Emerging literature affirms the importance of simulation across disciplines and industries.
Enabled, tested, and validated for experimentation on a valid digital representation of a complex system to provide valuable solutions by providing clear insights into the system.
With the information on the cybersecurity benchmarks discussed above from the literature [4]; [8]; [10], [11]; [12] and validated based on sunset questions designed in a questionnaire.
These benchmarks are selected as parameters to form the basis of this paper.
The effect through simulation of each parameter on cybersecurity initiatives is estimated.
The simulation facilitated by the system dynamics model is used to understand the "cause and effect" relationship between the parameters.
This is achieved through the stock and flow diagrams designed to manage the simulation scenarios defined for the system.
The simulation concept introduced in this paper supports planning and decision-making providing a framework to estimate an alternate scenario and assess the impact of change on the comprehensive system.
Change in this sense relates to what happens or could happen over time when the complex interaction between parameters is set under a variety of conditions.
3. Results
An effectively develop cybersecurity architecture results in a more flexible, nimble, and collaborative effort (employees, clients, partners) sufficient to secure and implement cybersecurity initiatives.
A total of 150 questionnaires are returned at the end of the one-month duration timeline set for analysis of data collected.
Slovin's formula is used to compute the margin of error based on the number of completed forms received and proposed for analysis.
The literature [19] affirms the suitability of Slovin's formula to estimate the sample size in a study given the total population.
The margin of error gives an approximate 2.2% detailed in the literature [20] to be an acceptable and fair representation of a total population to conduct a research investigation.
The authors also infer the 2.2% as acceptable for the current investigation with the sample size, population size, and percentage placed into considerations.
To further ascertain the suitability of the response rate considered for analysis, the mean (0.24); sample variance (0.182); sample standard deviation (0.43), and population standard deviation (0.018) are estimated.
The authors estimated the mean, sample variance, and standard deviation to allow an objective measure of opinion or subjective data that provides a best practice basis for comparison.
The 150 responses from the survey are analysed as an intermediate output to provide data that measures the parameters influencing the sustainability of cybersecurity strategies.
The analysis consisting of both closed and openended questions is analysed using the Statistical Package for Social Science (SPSS) tool suitable for quantitative analysis.
The outputs based on each cybersecurity benchmark are captured in (Fig. 1) and elaborated in Table 3.
Table 3. Questionnaire outputs
The (%) of responses relating to these questions are captured in Figure 1.
The research team deduces from the responses (questionnaires & structured interviews) that though organisations are no longer taking a wait-and-see approach to tackling cybersecurity breaches.
The context of cybersecurity is still fairly understood and unclear to employees.
Employees are willing to embrace the opportunities offered by the cybersecurity context.
The research team recommends more conscious sensitizations and education on the definition, types, layers, threat landscape, and opportunities offered by cybersecurity initiatives.
Cybersecurity strategies that promote greater awareness drive and unifies comprehensive cybersecurity protocols to improve productivity and sustain digital transformations are important.
A discussion of the cybersecurity definitions, types, multiple layers, threat landscape, and benefits is detailed in the contextual background of this research paper.
The (%) of responses relating to these questions are captured in Figure 1.
The research team infers from the responses (questionnaires & structured interviews) that a relative number of employees agree that digital transformations are the most challenging aspects of cybersecurity management.
Also, as a result of the increasing risks in cyber breaches, significant efforts are still required in aligning cybersecurity initiatives to corporate digital transformation initiatives or priorities.
In addition, lots of businesses still require awareness education on suitable tools, and strategies to drive innovation relative to developing a sustainable cybersecurity system.
Some of the awareness tools detailed during the desktop literature review are inclusive of ThreatCop, PhishingBox, Wombat, and PhishMe.
The (%) distribution from Q1 relating to what percentage of the cybersecurity budget is allocated to digital transformation efforts is captured in Figure 1.
The (%) distribution of responses relating to Q2 under resource allocation indicates from the responses (questionnaires & structured interview) that all respondents (100%) agree productivity is linked with cybersecurity.
Employees comprehensively understand the importance of effectively linking cybersecurity services with business productivity.
[14] details the value additions of effective cybersecurity initiatives in ensuring total productive maintenance.
The (%) of responses relating to Q3 indicates "Outsource (67%), Do not outsource (28%), and Neutral (5%).
The research team deduces from the responses that as a result of increasing cyber defense or security efforts, employees support proactive measures towards addressing cybersecurity breaches, therefore a need to outsource cybersecurity initiatives.
The (%) of responses relating to what timeline is best to review cybersecurity strategies (Q1) indicates "Quarterly (42%), BiAnnual (36%), Annual (15%), Monthly (6%), and Bi-monthly (1%)".
The authors infer respondents propose the quarterly timeline to review cybersecurity strategies.
The (%) of responses relating to who is responsible for effectively implementing cybersecurity strategies (Q2) indicate "Executives (54%), Managers (31%), and Employees (15%)".
The authors infer respondents believe that the executives are responsible for effectively implementing cybersecurity strategies.
However, the research team facilitates that cybersecurity implementations are a collective effort, and the responsibility lies with the entire workforce to ensure ease in enablement.
The (%) of responses relating to what are the current top-ranked cybersecurity initiatives (Q3) indicate "Cloud computing (35%), Data analytics (29%), Internet of things (20%), Artificial Intelligence (11%), and others (5%).
The research team infers from the responses (questionnaires & structured interviews) that the current highest ranked cybersecurity transformative initiatives include cloud computing, data analytics, Internet of Things (IoT), Artificial Intelligence (AI), and others.
The others are cumulative of industry 4.0 technologies such as robotics, additive manufacturing, simulation, and Augmented reality.
The literature [15]; [16]; [17]; [18] indicates new advances relating to the current top-ranked cybersecurity initiatives are experiencing developments suitable to assist security stakeholders record, correlate, and manage large volumes of data together with identifying or predict possible threats.
The concluding objective of the result section introduces the concept of system dynamics modelling essential to further elaborate and validate through a use case as to how cybersecurity strategies can be sustainable relative to standards. 
This paper designs a system dynamics model based on the crucial parameters of cybersecurity initiatives providing insights at developing a sustainable cybersecurity strategy. 
Cybersecurity strategy and sustainability are therefore defined as the overall output of the proposed system dynamics modelling development. 
This allows the relevant stakeholders in the cybersecurity sector to perform forecasting investigations through “what-if-analysis” that facilitates ease at planning and decision-making. 
The stock and flow diagram captured in Fig. 2. constitutes the parameters of cybersecurity initiatives discussed in the literature section of this paper. 
Fig. 2. illustrates the relationships of the crucial parameters of cybersecurity strategy towards achieving sustainability. 
Fig. 2. Stock and flow diagram for the sustainability of cybersecurity initiatives.
To constitute the dynamic behaviour of a business cybersecurity strategy that carries resource allocation, budget, and turnaround time as crucial initiatives.
This paper prepositions associated indicators and enablers suitable to test and validate the model.
Cybersecurity strategy is defined as a stock, increase (sustainability) and decrease (sustainability) are defined as flows, while the change, awareness, and time dynamics include other crucial cybersecurity initiatives captured in Fig. 2.
The impact of the other crucial cybersecurity initiatives on sustainability is manipulated.
The time frame for the model is set at 12 months and defined constant equation inputs for each model parameter are captured in Appendix A1.
This paper explores the direct structure test adopted from the literature [21] for the reliability and validity testing of the extreme conditions of the systems dynamics model.
The test assesses the validity of the system dynamics model by direct comparison of the parameter and structure confirmation.
This includes all forms of logical relationships, constants, equations, and available data against information provided on the real system being modelled.
The Vensim tool supports the direct structure tests with the 'model check' icon applied to assess the validity of the constituted model.
This is carried out and the result indicates 'model is ok'.
Prior to capturing the real-time crucial behavioural patterns of the factors under consideration, intermediate results based on various input combinations are traced and compared with observed outcomes.
This additionally validates the system dynamics model that provides a satisfactory confidence level for the model more than using the built-in functions provided by the VenSim tool.
Once the model structure and parameters conform to a satisfactory confidence level, the next step investigates the output that captures the crucial behavioural pattern of the business implementation on adjusting the model parameters in real-time (Fig. 3).
This presents an illustrative example of the value add of the system dynamics model based on the cybersecurity benchmarks discussed and validated extensively in this paper.
Resource allocation, change, awareness, and time are discussed and validated in this study as crucial benchmarks of cybersecurity initiatives.
Figure 3 captures a combined graph of the behavioural pattern of cybersecurity strategy triggered against a change (increase & decrease) of resource allocation.
The graphical illustration indicates a continual increase of resource allocation over a period of time from the baseline (initial state) will result in a decline in the cybersecurity strategy.
This consequently positively increases the budget and negatively impacts the sustainability of the business.
Continuous decrease of resource allocation over a period of time from the baseline will result in an increase in cybersecurity strategy, decreasing budget, and enhancing the sustainability of the business.
The outputs provide awareness to relevant stakeholders on the impact (linear or indiscriminate) of change (budget & sustainability) relative to an increase or decrease in resource allocation.
4. Discussion
This paper gathers insights on the perceptions of network and security solution organisations used to explore the protocols of the system dynamics model for strategic and tactical business decision support.
The current state of cybersecurity initiatives within the Nigerian network and security solution organisations is tested.
A desktop literature review provides information on the crucial parameters for a sustainable cybersecurity strategy.
The impacts of these parameters are addressed by structured interviews and questionnaires selected as the methodological approach to gathering data while the SPSS statistical tool and simulation are used for analysis.
The methodology and analysis illustrate the "cause and effect" relationship between each parameter.
This paper assumes the demonstrated scenarios serve as an illustrative example of the benefit of simulation towards achieving a sustainable cybersecurity strategy.
The outputs detail crucial decision variables and prospects provided by industry 4.0 relative to the digital cybersecurity space.
The results present insights for businesses to embrace digital opportunities that the cyber norm offers for network and security solutions.
This is combined with complementing and expanding on the discussion of cybersecurity and promoting its acceptance of how businesses align cybersecurity initiatives for the future and beyond.
While the SDM approach may need to be refined to cover the comprehensive cybersecurity strategies, the outputs demonstrate a practicable and illustrative pathway for future investigations reinforcing the need for discussions on cybersecurity to align strategies and operations with technologies of industry 4.0 for business delivery.
The overall outputs assist investors, management, and corporate establishments to explore distinct standpoints relative to developing a sustainable cybersecurity system for enhancing sustainability.
Future investigations will aim to incorporate other crucial inputs which significantly influence the sustainability of cybersecurity strategies and implementations.
Acknowledgements
The research team thanks the University of Johannesburg for support of this work.
Evaluating Price-Quality Properties of Information Technology Public Procurement using Decision Analysis
ABSTRACT
In public procurement, byers typically try to resolve the problem of not knowing the tenders before stating their preferences through the use of price relative models or comparison prices and different procurements use slightly different spreadsheet models for tender evaluation.
This renders it difficult to assess the quality attributes in a way enabling for evaluating and comparing price-quality properties of real-life tender evaluations.
However, tender evaluation is an instance of multi-attribute decision-making, and for each tender evaluation instance there is a corresponding multi-attribute decision model enabling for a more systematic assessment of tender evaluations.
In this paper, we investigate how robust the selection of tenders in IT procurement are towards changes in the assessment of quality and to the presence of quality attributes in the tender evaluation model.
Conclusions include that "decisive" attributes cannot be expected to exist, and that score based tender evaluation model with precise estimates of quality levels put strong demands on the capabilities of the buyer.
CCS CONCEPTS
• Applied computing; 
• Operations research; 
• Decision analysis; 
• -Multi-criterion optimization and decision-making;
KEYWORDS
Procurement, decision analysis, information technology, quality evaluation, imprecision
ACM Reference Format:
Aron Larsson, Sam E. S. Dahlin, and Mathias Åström. 2022. 
Evaluating PriceQuality Properties of Information Technology Public Procurement using Decision Analysis. 
In 2022 The 9th International Conference on Industrial Engineering and Applications (Europe) (ICIEA-2022- Europe), January 12–14, 2022, Barcelona, Spain. 
ACM, New York, NY, USA, 4 pages. https://doi.org/ 10.1145/3523132.3523133
1 INTRODUCTION
Public procurement of information technology services and solutions is a common activity of public institutions such as agencies and municipalities.
The overall aim is ".
the promotion of efficiency, i.e. the selection of the supplier with the lowest price or, more generally, the achievement of the best value for money" and ".to maximize the resulting benefits for society and to protect competitive markets" [1].
The ambition is to, as a buyer, be able to "select the tender with the financially most advantageous offer" [2], which boils down to assessing the cost of a service or product together with the quality properties of the service/product.
Typically, for IT services or products, the quality attributes are often of a qualitative nature related to foreseen needs which are hard to express, see, e.g., Ref. [3, 4].
Still, quality attributes' performance levels of tenders must be evaluated ex ante by the byer with no room for expressing uncertainty or regard that putting a numerical value on a level of quality is often subject to inherent uncertainty and imprecision, cf. [5].
This situation motivates the underlying question of this paper, being that of how robust the selection of tenders in IT procurement are towards changes in the assessment of quality and to the presence of quality attributes in the tender evaluation model? Consequently, an approach for such a robustness analysis has to be defined capable of a systematic and common approach to analyze a set of tender evaluations.
In EU public procurement, the appraisal of the tenders must be done in a, seemingly, transparent manner and the supplier needs to be able to view the "weights" put on price and quality before submitting a tender to inform the supplier on what priorities the buyer has.
This entails that the buyer has to specify weights prior to knowing price levels and quality attributes (to be translated into quality score levels) of the tenders subject to be compared.
This requirement leads to a deficiency in the evaluation process of the tenders, which byers typically try to resolve with price relative models or comparison prices.
Further, this renders it more difficult to assess the quality attributes in a way enabling for evaluating and comparing the value for money between different tenders, calling for the need of a common representation for tender evaluation model.
Due to the use of different appraising methods, comparing tender evaluations against each other is not straightforward.
However, from a decision science viewpoint, tender evaluation is an instance of multi-attribute decision-making, and for each tender evaluation instance there is a corresponding multi-attribute decision model preserving the relative differences between the evaluated tenders.
1.1 Multi Attribute Value Theory (MAVT)
The MAVT model is used for aggregating performance levels over a set of attributes and then through encompassing the preferences of a decision maker in terms of value functions and attribute weights providing a preference order on a set of objects characterized by multiple attributes.
MAVT operates with measurable value functions as a preference model, such that if an alternative a is preferred to and alternative b written as a ? b then v(a) > v(b) and in the case when v(a) = v(b) a decision maker is indifferent between a and b.
Further, if the decision maker would rather exchange b with a compared to exchanging d with c then v(a) - v(b) > v(c) - v(d) which implies that the value function is a cardinal (or measurable) value function.
This property then enables for aggregation of attributes as a weighted average in a multi-attribute value function where each attribute x is associated with a value function vx(x) such that where l and u are finite lower and upper range bounds, typically l = 0 and u = 1.
Given that all attributes have value functions sharing range v(·) 7→ [l, u] and that some independence assumptions are accepted [6], an additive aggregation over multiple attributes X = (x1, x2, . . ., xn) can be done so that the value of an alternative is given by.
1.2 Public procurement models and their definitions
In practical use, the most common models for tender appraisal are the price-to-quality scoring model and the quality-to-price rate of substitution model [7].
In a MAVT interpretation of the former, all quality attributed is associated with "score points" together with a scored comparison price and all scores are given within a common range [l, u].
Quality scoring is cognitively demanding.
A quality score of tender Ti for quality attribute Qj is given by a "scoring function" that essentially operates as a value function, where l and u are range defining constants and Qj is generalized to the (finite) set of quality distinguishing features for quality attribute Qj .
P(Qj ) is then the power set of Qj entailing that, in theory, a buyer should be able to provide a score for each subset of Qj .
For instance, if the Qj quality attribute is "Functionality", Qj would consist of a set of assessed functionalities where each tender fulfils these functionalities to various degrees.
In theory, if all functionalities are fully fulfilled, then vj (qij) = u and if they are only fulfilled to their minimum accepted degree, then vj (qij) = l.
Important to note here is that tenders not complying with the minimum accepted degree for a quality attribute should have been discarded prior to scoring.
The comparison price pi for a tender Ti is the offered price plus estimated costs for maintenance and support during the economic life span of the service or product.
Then, the price score for a tender Ti is the value where p is the comparison price and vp(p) represents a negative preference direction, i.e. it is strictly decreasing (the higher the p the lower the value), and l and u are range defining constants where typically l = 0 and u is commonly selected to be easy to interpret, typically an integer between 5 and 10.
A so-called "price relative" price scoring function will adjust the range [pmin, pmax] to become the difference between the lowest and the highest price.
Typically, and for all analyzed tenders in this paper, this is done by adopting a strictly decreasing linear scoring function such that In contrast, a price relative but difference-bounded price scoring function has pre-defined levels of pmin and pmax so that the scoring function is defined according This entails that the greatest value difference between the tenders' comparison prices are pre-defined and that the scale [pmin, pmax] will not be sensitive to tenders having unreasonably low prices.
As the weights of all attributes must be provided to the potential supplier in advance, a price-to-quality model can be judged as an improper use of weights for aggregation of scores in decision analysis as the weights should reflect the value difference between the best and worst available tender which is not doable when providing weights upfront, cf., e.g., [8].
Besides the scoring model with explicitly stated weights, the other main approach is to adopt the usage of monetary marginal rates of substitution for the quality attributes in the quality-toprice model.
Essentially, for each quality attribute Qj , a quality score difference |vj (q+c) - vj (q)|, c > 0, is said to equal a given price difference thus implying that in the quality-to-price model vj (·) is expressed in monetary terms.
Assuming the linear price scoring function in Eq. 5) the stipulated rate of substitution together with the score ranges then provide weights through the ratios 7) and the constraint wp + I wj = 1.
For both price-to-quality and quality-to-price tender evaluations, a MAVT model replicating the preferences stated and the appraisals done can then be created.
The value of a tender Ti is then given by the additive function (8) where pi is the comparison price of Ti and qij is the quality level of quality attribute Qj of tender Ti .
Further, all weights are positive and wp + I wj = 1 and explicitly stated by the buyer in this model.
The tender T* such that V(T*) is greater than all other tender values V(Ti ) is then selected by the buyer.
yer.
2 MATERIAL AND METHOD
Eight procurement reports was translated into a corresponding MAVT model with two high level attributes, price and quality. 
The quality attribute was partitioned into a set of quality attributes which was the one subject for appraisal by the public buyer. 
This translation then enabled for a controlled sensitivity and robustness analysis in the following three different ways.
2.1 Presence of quality attribute
The presence of a quality attribute and its impact on which tender T* that has the highest value is investigated by in sequence letting each quality attribute weight wj assume a value of zero while distributing its original value over the remaining attributes, maintaining the remaining attributes’ relative ratios wk /wl where k, l , j. 
This enables for assessing whether the tender selection is systematically sensitive or not to discarding a particular quality attribute. Further, if the tender selection is not sensitive to the presence of one quality attribute, the resulting selection cannot easily be attributed to a “deciding factor” but on the aggregation of performance of all attributes motivating more elaborated techniques for assessing quality levels and their evaluation to reach a selected tender. 
2.2 Price-quality weight variation
By varying the price weight wp between 0.1 and 0.9 and distributing (1–wp) over the quality attributes, maintaining their relative ratios, the presence of how sensitive the tender selection are to price sensitivity is assessed. 
If cheapest tender is not selected, the higher the value of wp for a selected tender to remain the tender T* having the highest value, the less price sensitivity. 
Conversely, if the cheapest tender is selected, the lower the value of wp for a selected tender to remain the tender T* having the highest value, the less price sensitivity. 
We herein use a heuristic rule for the analysis, meaning that if the weight can be increased with a factor of 1.5 or less (or conversely decreased by a factor of 2/3 or more) while a selected tender remains to be selected, the tender is not sensitive to price-quality weight variation.
2.3 Quality score imprecision
As previously mentioned, for a given tender Ti each quality attribute Qj is provided with a score vj (qij) taking values in a closed interval [l, u]. 
Given that the capability of a buyer to provide such a precise estimate of quality, especially when the quality attributes are of a qualitative nature, it is reasonable to study the sensitivity of an evaluation towards introducing imprecise quality scores. 
This is done by replacing each quality score vj (qij) with an interval value [vj (qij)min, vj (qij)max] where the bounds are given by 25% of the value scale [l, u]. 
Hence, if l = 0 and u = 10, then a quality score of 7 would be represented by an interval valued score of [4.5, 9.5].
2.4 Modeling tool
For analytical purposes, the tender evaluation for each procurement case was modeled using the DecideIT 3 decision tool [9], which allows for robustness analysis with interval values. 
As allowing for interval value statements will lead to interval valued tender values V(Ti ), in the case when min(V(Ti )) > max(V(Tj )) does not hold, the robustness of a particular preference Ti ≻ Tj is obtained by means of calculating the proportion δij of value points where V(Ti ) > V(Tj ) and vice versa. 
If then, min(V(Ti )) > max(V(Tj )) holds, then δij = 100% and the opposite δji = 0%, and if δij < 90% then a preference statement Ti ≻ Tj is not deemed as confident (see Ref. [9]).
3 RESULTS AND ANALYSIS
The results for each of the three approaches to sensitivity and robustness and for the eight evaluated tenders are provided in Table 1 below. 
From Table 1, it can be seen that all analyzed tenders showed “No change” of the top ranked tender when investigating the presence of quality attribute sensitivity analysis. 
This was rather surprising, especially for the tenders with fewer quality attributes entailing that each attribute is likely to have more impact on the evaluation result and there does not exist a “decisive” quality attribute that can be communicated. 
One explanation could be that the selected tenders were superior in many attributes, and that high performance in one quality attribute correlates with high performance in the other quality attributes, however that requires further work and falls beyond the scope of this paper. 
As for the price-quality weight variation, the cheapest tender was selected in three cases, indicating that there is a delicate price-toquality assessment and the performance of the quality attribute to a substantial degree matter for IT procurement. 
Three procurements were subject to price sensitivity according to the rule for analysis and the weight distribution between quality and price is likely to become of major importance. 
For Procurement 3, the tender with the highest evaluated quality was also the cheapest meaning that it dominated the other tenders. 
Introducing imprecision on the quality scores provides further interesting result, as only one of the procurements managed to provide a confident selection of winning tender, and that one was the procurement when the selected tender was dominating its competitors. 
This strongly indicate that whenever assessing a quality attribute and associating it with a score reflecting its degree of quality level, the process of scoring the quality attributes leaves rather small room for “assessment uncertainty” which is not reflected nor communicated in the procurement reports.
4 CONCLUSION
We have presented a decision analytic approach to analyzing the price-to-quality tender evaluations in public procurement of information technology solutions.
In practice, instances of tender evaluations is typically carried out using different spreadsheets conforming to either a price-to-quality or a quality-to-price evaluation approach.
The alternate approaches in combination with different spreadsheets for each tender evaluation renders a systematic comparison between them infeasible without representing them in a common generic model.
The MAVT model utilized in this paper is capable of representing the preferences and trade-offs between price and quality attributes stated in the procurements and enables for a systematic analysis of price-quality properties and to what extent the selection of a winning tender is subject to be sensitive to changes in preferential statements such as the weight put on price and the requirement to specify precise scores on quality attributes.
The results when analyzing the presence of quality attributes indicate that even for less complex tender evaluations with fewer quality attributes (less than ten), it seems like the assessment of and aggregation of performances on the quality attributes is a delicate issue and that "decisive" attributes cannot be expected to exist.
Further, the weight distribution between price and quality stipulated by the buyer is of significant importance for which tender that becomes selected, given that price-sensitivity was present as well as given the distribution between selecting the cheapest tender and more expensive ones in the sample.
Finally, when introducing imprecision on the quality scores the confidence in the ranking of tenders dropped below the default confidence threshold in all cases but one.
This indicate that using a score based tender evaluation model with precise estimates of quality levels put strong demands on the capabilities of the buyer, and that approaches relying less on scores and more on relative rankings and comparative statements can have a role to play when assessing and comparing performance levels of quality attributes in IT procurement.
ACKNOWLEDGMENTS
The authors wish to thank Henrik Melander at the AdviceU consultancy firm for enabling access to the empirical material analyzed.
REFERENCES
[1] Competition and Procurement: Key findings, OECD 2011, available atwww.oecd.org
[2] P. S. Stilger, J. Siderius, E. M. Van Raaij, “A comparative study of formulas forchoosing the economically most advantageous tender,” J. Publ. Procurement, vol. 17, no. 1, pp. 89–125, 2017.
[3] F. Zandi and M. Tavana, “A multi-attribute group decision support system for information technology project selection,” Int. J. Bus. Inf. Sys., vol. 6, no. 2, pp. 179–199, 2010.
[4] C. E. Moe, A. C. Risvand, and M. K. Sein, “Limits of public procurement: information systems acquisition,” in M. A. Wimmer et al. (eds.), Electronic Government. EGOV 2006. Lecture Notes in Computer Science, vol 4084. Springer, Berlin, 2006.
[5] M. Danielson, L. Ekenberg, M. Göthe, and A. Larsson, “A decision analytical perspective on public procurement processes,” in J. Papathanasiou et al. (eds.), Electronic Government. EGOV 2006. Real-World Decision Support Systems. Integrated Series in Information Systems, vol 37. Springer, 2016.
[6] J. S. Dyer and R. K. Sarin, “Measurable multiattribute value functions,” Operations Res., vol. 27, no. 4, pp. 810-822, 1979-
[7] M. A. Bergman and S. Lundberg, “Tender evaluation and supplier selection methods in public procurement,” J. Purch. Supply Mgmt, vol. 19, no. 2, pp. 73-83, 2013.
[8] M. Danielson, L. Ekenberg, A. Larsson, and M. Riabacke, “Weighting under ambiguous preferences and imprecise differences in a cardinal rank ordering process,” Int. J. Comp. Intell. Sys., vol. 7, no. 1, pp. 105-112. 2014.
[9] M. Danielson, L. Ekenberg, and A. Larsson, “A second-order-based decision tool for evaluating decision
VIRTUAL REALITY FOR LOST ARCHITECTURAL HERITAGE VISUALIZATION UTILIZING LIMITED DATA
KEY WORDS: VR, Lost Heritage, Data Visualization
ABSTRACT:
The process of building digital models of architectural heritage has become increasingly complex and accordingly this enables the potential of utilizing digital techniques as a tool in the context of research.
Depending on the objective of the research, there are various tools and outcomes.
Ranging from information management projects by using Building Information Modelling (BIM) and Geographical Information Systems (GIS) technologies, to providing Virtual Reality (VR) and Augmented Reality (AR) by using smart technologies for visualization of architectural heritage, there is an increasing demand because of their fast developing technological abilities.
Additionally, the digitization processes also becoming less dependent to the information coming from the building and as a result the subject of such research includes buildings that have disappeared without various archival data or other types of historical information.
This paper investigates the different visualization techniques and tools for lost architectural heritage examples in postconflict societies with limited available data, focusing on the VR mobile applications and their implementations.
1. INTRODUCTION
1.1 Lost Architectural Heritage
A comprehensive study for an architectural heritage consists of several steps and with each and every step, different kind of data is collected.
On site data collection includes the collection of data from the building itself and by interacting with the different stakeholders of the studied architectural heritage, such as current residences, former residences, site managers, etc.
On site investigation also includes archival and documentary research from historic resources which also is identified as a valuable data source.
Consequently, studies dealing with architectural heritage involves collection of data in different formats in different stages of the operation.
Dealing with lost architectural heritage however, the collection of data could be limited due to the fact that the building might have disappeared without a trace and with minimal historical records and documents.
Data has a principal role in defining the strategies concerning architectural heritage visualization especially if the building is lost without adequate historical document or data.
It is also a valuable attribute that provide essential discussion points concerning the hidden values of historical buildings and sites.
While dealing with this complex and multi-faceted data, it is also critical to provide convenient data management to support the architectural heritage conservation process especially for the heritage visualization.
As mentioned above, architectural heritage itself is an important data provider for the conservation of architectural heritage studies.
The researcher could capture various exposed or hidden valuable data by just studying the different historic periods of the architectural heritage that has traces on the building in various formats.
The conventional methods of data collection on the site is still valuable even with the rapid technological advancements for the survey phase of architectural heritage studies.
However, with the integration of digital technologies, it is now possible to consider the digitization process from data collection to data visualization.
Hence, the use of digital technologies allows alternative ways of data digitization as part of the architectural heritage visualization process.
The integration of digital technologies in lost architectural heritage visualization on the other hand could define interesting research areas (Gunay, 2021).
With recently developing technologies in mobile digital devices and tools in architectural heritage visualization, 3D virtual reconstructions can be developed with focuses on outcomes considering the use of these 3D visuals for research purposes.
Virtual reconstructions of architectural heritage, 3DGIS by using web based platforms (Richards-Rissetto et al., 2013), virtual museum applications (Huang & Han, 2014), virtual reproductions of certain components of architectural heritage (Cruz, 2017; Ramsey, 2017) could be named as some of the advances in using digital technologies in virtual heritage today.
The MayaArch3D - University of Nebraska, Temple 22 (Harvard University), Via Appia Antica Project (Virtual Heritage Lab) Safranbolu 3DGIS (Istanbul Technical University) ARCHES Heritage Inventory and Management System (Getty) HBIM for the Basilica di Collemaggio (ENI), SAHRIS (NHRA South Africa) JHBIM (King Abdul Aziz University), HeritageTogether (Bangor University), Byzantium1200 (3D model of Istanbul through Allan Sorrel painting), are some of the several recent projects where the use of information management and integration of 3D visualization are implemented.
Limitations in the collection of historical data require innovative solutions for the visualization process of lost architectural heritage (Bevilacqua et al., 2019).
Archival images could be a valuable source of information in addition to cartographic resources (Balletti et al., 2020).
Hence, the necessity of gathering visual historical data could enable the virtual reconstruction of lost buildings even though there are limitations in data collection (Gunay, 2019).
Furthermore, the virtual reconstruction might require assumptions and interpretations since the photographic or cartographic resources could be limited.
At this point, it is essential to assess the data source for accuracy and reliability (Knowles, 2018).
Discussions around accuracy are related to data resources.
A reliable resource such as archival documents or previously done research could enhance the reliability of the outcome (Pietroni & Ferdani, 2021).
This way, the historical evidence could be verified by multiple references, and virtual reconstruction of lost architectural heritage could be more authentic.
1.2 Architectural Heritage Visualization
Integrating digital technologies in architectural heritage visualization is a developing area of study that finds its earliest examples from early 1980s (Webb & Brown, 2016) and architectural heritage visualization continue to develop since then.
Technologies dealing with 3D visualization is widely used in several disciplines in various formats.
In architecture and particularly in heritage visualization the use of 3D computer graphics can be found in different formats ranging from collection of data with digital tools or other mobile devices and technologies to 3D printing of an architectural detail.
With this research the focus was on using computer generated 3D visualizations and their implementations by using image based modelling and AR/VR technologies in 'Virtual World'.
The term Virtual World from architectural visualization point of view is relevant to the term 'simulation' which is described by the Oxford Dictionary as the production of a computer model of something especially for the purpose of study.
Bartle (2004) describes the term as an environment implemented by the computer, which is partially controlled by the individuals (Bartle, 2004).
At this point the digitization of data is important and should be done comprehensively.
1.3 Digitizing the data
Following the data collection and digitization of data phases, visualization of data in various formats could be achieved by using different digital tools.
Digital maps, 2D and 3D drawings and recently popular mobile technologies provide the viewers an understanding of architectural heritage throughout its timeline or with a proposal that sheds light on the possible future interventions.
Digital reconstructions of architectural heritage in 3D platforms offers users visualization options by using mobile devices and tools.
Although 3D virtual reconstructions could be considered as popular, due to the fact that they provide a better understanding of architectural heritage with rendered digital images by using computer technologies, they also bring up challenges of visualizing data interactively in 3D format.
Whether the visualized data is related to the material usage or finishes or it is related to a historic detail, interactive visualization could achieve the data visualization by seeing analytics presented visually with real time changes.
1.4 The use of VR / AR
Nowadays VR/AR technologies can be used wider public for different purposes and recently the use of VR/AR technologies for architectural heritage visualization purposes are gaining more and more popularity.
Although these technologies and their applications is perceived as something new, the studies and inventions dates back to 1950s.
The invention of Morton Heilig (Gladstone et al., 2000) 'Sensorama' is considered the first virtual environment that simulate different experiences including flying a helicopter and riding a bicycle.
And later on these technologies commercialized and particularly they are being used for gaming purposes.
The use of VR/AR technologies in heritage visualization also linked with their use in digital museums.
They are useful platforms that the public can access via the internet in order to view different objects and display items in virtual environment.
Alternatively, digital tools also are being used in museums to digitally perceive the objects or spaces that could require innovative methodologies during the visualization processes.
2. THE DEVELOPED METHODOLOGY
2.1 The Case Studies
The lost architectural heritage buildings in Izmir (Turkey) and Thessaloniki (Greece) is the subject of this research and 3D models of the selected buildings have been built with reference to the old photos and historic maps for well-known monumental buildings of the city as well as the domestic and non-monumental architectural examples.
(Figure 1 - 2).
Historic photographic documents from digital and city archives provided information to virtually create the 3D models of these lost architectural heritages.
The detail of the model was relevant to the detail of the photographic evidence and accordingly various types of digital models were constructed.
Information brought from the photographic documents were mainly related to the proportions, dimensions and identification of certain finish materials of the lost architectural heritage examples of Izmir and Thessaloniki.
Due to the fact that majority of these photos were black and white, the relevant information regarding the materials and finishes was also limited.
The virtual reconstructions allowed utilization of these visuals in various platforms for different interpretations of lost heritage examples.
As more advanced graphic representations in 3D modelling were developed, advanced 3D interactions became possible to integrate with the created models.
Several simulations and mixed media design by superimposing the historical photos with digital model renderings, other types of experimentations were possible.
VR experience, as a rapidly developing platform, provided the integration of computer graphics where users can be a part of a computer-generated reality.
Additionally, this provided more direct interaction between users and the environment by using mobile devices.
2.2 The Method
The technological developments and the dynamic information sharing possibilities allow various ways to construct knowledge for architectural heritage visualization.
The technologies also have an impact on the creation of knowledge related to the past particularly for the lost heritage examples.
Several digital technology solutions were explored while investigating the ways to reintroduce the lost architectural heritage examples for this research.
As key data resources, photographic and cartographic material enabled the creation of the digital architectural models for selected case study lost heritage examples.
Virtual reconstructions were possible with the implementation of the Image-Based Modelling (IBM) technique using historical photos.
Several archival research allowed the identification of suitable historical images to create the virtual reconstructions of the buildings with their surrounding contexts and relevant building groups.
Google Sketch Up enabled the construction of digital models of the selected lost architectural heritage buildings from Izmir and Thessaloniki by matching the historical photos into the 3D (Figure 3 - 5).
It was possible to utilize more than one photo within this process.
Additionally, historical maps provided information on a plan format which enabled the measurement of the buildings on these maps.
The identification of buildings on the studied historical maps was only possible for some of the digital models.
The possibility to include more than one photo allowed verification of data collected from these historical photos.
For instance, the heights, dimensions, proportions, and other details were checked by comparing different photos.
After completing adjustments, taking into consideration the role of depicting the essence of the finishes, materials, colours, street patterns, etc.
digital models have been improved to achieve a more realistic and dynamic depiction of these lost architectural heritage examples.
IBM is widely used in architectural heritage representations for documentation or visualization purposes for existing and nonexisting buildings (Kouimtzoglou et al., 2017).
The outcome of such works is considered to be better due to the fact that the documentation of an existing structure could be done with high resolution images by following scientific approaches.
Dealing with the lost architectural heritage however, IBM could be the only option to visualize a lost heritage in order to generate 3D digitized architectural model if there are not sufficient architectural documentation or other types of historical documents about the building.
3D architectural models that were generated by IBM were then rendered with further details and for the purpose of creating more realistic virtual experience for the participants to experience the past with more realistic way.
After evaluating different software options and various alternatives, renderings have been generated by using Twin Motion on the digital architectural models created with the help of Sketch Up.
The panoramic renderings generated from the digital models enabled the creation of VR simulations for a more realistic virtual experience.
VR experience was also supported with relevant audio support depicting historic settings and neighbourhoods and also to improve the virtual experience and generate a more realistic audio experience (Figure 6 - 7).
Generated 3D models have been visualized with the help of VR/AR mobile applications and through VR goggles and tablet computers as well as mobile devices.
These material were a part of fieldwork discussions and during participant observations the above mentioned digital media was shared with the participants and their reaction were explored as part of this research.
3. THE VIRTUAL ENVIRONMENT
3.1 3D Visualization
Producing digitally reconstructed models of lost settlements or buildings provides a virtual platform where digital models of these lost examples could be visualized.
The challenging issue regarding the 3D visualization of lost architectural heritage is the reliability of the historic data.
Since the buildings do not exist and in some cases the amount of information gathered from different resources is limited, it is essential to utilize verified data, in other words the data that could be find in more than one historic resources.
The information displayed with the 3D reconstructed models should be verified and cross referenced from different sources.
Consequently, the outcome is also linked to the building's past or the reason behind its loss.
For instance, ongoing or post-conflict situations could affect access to adequate or trustworthy historical data.
Furthermore, different than research for a historic building that has partial or complete remains, investigating a lost heritage might end up with limited data sources such as a single photo or even in some cases, just a written description that can give clues about the building's past.
Three areas from both cities were the main focus of the VR simulations.
Further detailing and adjustments enabled a more realistic experience for the visitors.
Although many other models have been built with SketchUp, three areas were selected for VR experience based on their significance (Figure 8 - 10).
4. CONCLUSION
With this research the selected lost architectural heritage buildings in Izmir and Thessaloniki have been virtually reconstructed.
The digital models allowed the current residents of these cities virtually experience the disappeared buildings of their cities that belong to societies that have been displaced.
The specific cases for lost heritage data collection and visualization allowed the understanding of the previous and current critical issues and challenges.
Studies related to lost architectural heritage visualization vary depending on the purpose and depending on the technologies that are implemented.
The developing visualization technologies and mobile/wearable digital tools also bring up a flourishing area of visualization techniques for lost architectural heritage examples.
The possibility of experiencing the virtually reconstructed architectural or urban heritage examples closer to what it was perceived by its original users is a developing area of research.
The review established the link between the current political powers and their role in the selection of lost heritage for heritage interpretations.
The impact of ongoing or former conflict on heritage ownership and its link to the perception of heritage by today's residence is a valuable research discussion.
Therefore, the need for researching the lost historical buildings with limited data is an area of research that requires attention to overcome the limitations of conflict cases and managing the data for future research.
With the development of devices to be integrated into interaction technologies, VR gives the users the feeling of being a part of the digitally generated virtual environment.
The users could interact in the virtual environment through 3D interaction input devices and technologies.
The concept behind experiencing the past focused on integrating different digital media as well as various outcomes from selected software options.
One of the key contribution of this research is the production of user friendly 3D reconstructions of the lost architectural heritage examples of Izmir and Thessaloniki by using VR and AR technologies.
The use of digital technologies allowed the users to experience the previous lives of these cities in digital environment by using different mobile visualization tools.
The reason behind the loss of architectural heritage could affect the virtual reconstruction efforts if the building was lost due to a conflict or war.
For instance, if the building was built and used by a society or a group that does not live in the area, discussions around heritage ownership might occur.
Consequently, new and improving digital technologies allow advanced virtual reconstructions of lost architectural heritage visualizations.
The use of digital technologies in lost heritage visualization and its link to heritage and identity relations enable new research areas that this study will further investigate.
ACKNOWLEDGEMENT
This paper is an outcome of an ongoing PhD research titled ‘ Heritage of the ‘other’: The Future of Lost Architectural Heritage’ with the supervision of Dr. Aylin Orbasli and Dr. Henry Abanda in the Faculty of Technology, Design & Environment of School of Architecture in Oxford Brookes University, Oxford – UK.
INVESTIGATION OF TEMPORAL CHANGE OF LAND USE IN ELMALI BASIN USING REMOTE SENSING AND GEOGRAPHIC INFORMATION SYSTEMS
KEY WORDS: Land Use, Elmalı Basin, Geographic Information Systems, Remote Sensing.
ABSTRACT:
The sustainable use of water and soil, which are indispensable for living things, is closely related to the concept of land use.
While land use is becoming gradually modern as a necessity of the age, urbanization and industrialization are also gaining great importance.
So much so that, in Turkey, where agriculture-based economy was emphasized until the 1980s, interest in exports of industrial products has increased in recent years and industrial investments especially in the Marmara region have increased considerably.
This situation has increased job opportunities due to the industrialization developing in the region and has led to an increase in the population in parallel.
The structure process has accelerated in order to meet the needs of the increasing population.
Basin areas, which contain a wide variety of classes, are among the regions that are highly affected by these changes.
Within the scope of the study, Elmal? Basin, which is used to supply potable and utility water to the province of Istanbul, was chosen as the study area.
Within the scope of the research, the temporal change of the land use in Elmal? Basin has been investigated by using Landsat-8 satellite with a spatial resolution of 30 meters for the years 2013, 2015, 2018 and 2020 taking advantage of Geographical Information Systems (GIS) and Remote Sensing (RS) technologies.
Classification was made with support vector machines, one of the controlled classification methods on satellite images, and the changes in land use were evaluated by comparing the images of working years.
1. INTRODUCTION
Human beings actively use many riches that exist throughout their lives.
It is seen that among these riches, land use comes to the fore (Sar? and Ozsahin, 2016).
As a requirement of the age we live in, land uses have become increasingly modern, as well as urbanization and industrialization have gained importance.
Many negative situations have emerged with urbanization and industrialization, and changes that are contrary to natural use cause the traces of climate change to increase (Ozdemir, 2015).
Therefore it is increasing interest in the problems arising from the abuse of land use and land (Elmastas, 2008).
Each of the basins has its own natural (wild life, geological, vegetation, geomorphological, climate and hydrological), sociocultural and economic (transportation, infrastructure, administrative structure, land use, population) characteristics (Tan?k, 2017).
Basin areas, which contain complex layers such as settlement area, water area, forest, land areas, mine and quarry areas, agriculture/pasture area, industrial area, park and access roads, are among the main regions that should be used carefully and planned in terms of land use activities (Garipagaoglu, 2012).
Geographic information systems are an information system that is created by location-based observations according to the requirements of the institution it is associated with, provides the collection, storage and analysis of graphic and attribute data, and contains more than one data type (Yomral?oglu, 2000; Ulugtekin and Bildirici, 1997).
At the same time, it provides images in maps and 3D environments and presents these results to the user.
Remote sensing provides a great advantage to the user as it monitors and measures large areas at once, achieves faster output and does not require much labor (Cakaroz et al, 2020).
At the same time, the data recorded in remote sensing forms an important data base for GIS (Gozukara et al, 2015).
It is not possible to detect mistakes made in land use before determining the current state of the land situation (Ilgar and Koca, 2006; Yuksek, 2018).
In addition to this, it will be an important step to establish a road map by determining the temporal changes in land use in the past, in order to reverse the mistakes made (Dengiz and Demirag Turan, 2014).
When the changes that the land has undergone are investigated and examined for their reasons, the accuracy of the study will increase and it will provide a suitable use for the structure of the land (Yuksek, 2018).
In the studies carried out in this direction, it has been emphasized that the basins are affected by the constructions that occur due to the increasing population in the city (Geymen, 2013; Geymen, 2017).
This study was carried out to determine the changes in the land use of the Elmal? basin over the years.
In line with the results obtained, it is likely to be an important guide to the arrangements and practices planned for the study area in the future.
The planning of land use will form an important base for the changes in time and even for the protection of natural resources.
The subject of this study has been determined as the temporal change of land use of Elmal? Basin, which produces drinking water for the people of Istanbul, with the help of satellite images using Geographical Information Systems and Remote Sensing methods.
Using satellite images is important because it can cover larger areas and save time (Caglak and Ozelkan, 2019).
2. METHOD AND MATERIAL
Within the scope of the study, satellite images of the relevant years were obtained from the USGS website for the determination of land use change.
Afterwards, these satellite images were subjected to processes such as band fusion, image classification and accuracy analysis in ENVI Classic 5.3 software.
Finally, the necessary spatial analysis and queries and final products were obtained from the layers related to land use obtained in ArcGIS 10.4 software.
It was preferred to use the satellite images of Landsat-8 2013, 2015, 2018 and 2020 of the Elmal? Basin.
Among the reasons for this; It is low in cost, remains up-to-date because it can be obtained quickly, and images can be displayed as covering very large areas (Erbay, 2005).
Since almost all satellite images are digital and contain location information, it forms a basis for analysis and queries for GIS and for almost every study in remote sensing (Erbay, 2005; Geymen, 2016; Siart et al, 2009).
Figure1. shows the Landsat-8 satellite images of the years 2013, 2015, 2018 and 2020 used in the study.
The Landsat-8 satellite was developed as a collaboration between NASA and the US Geological Survey (USGS) and was launched in California on February 11, 2013 (URL 1).
The payload of the Landsat-8 satellite consists of two instruments, the Operational Terrain Imager (OLI) and the Thermal Infrared Sensor (TIRS) (URL 12).
The satellite's radiometric resolution is 8 bits and the strip width is 185 km (Kurnaz, 2019).
2.1 Method Followed
The method followed in the study is given in Figure 2.
2.2 Study Area
The Elmalı Basin, chosen as the study area, is located within the borders of the Istanbul province of the Marmara region and on the Anatolian side of the province (Geymen, 2017). 
The total area of the basin includes the settlements of Ümraniye and Ataşehir on the west, Sancaktepe and Çekmeköy on the east, Maltepe on the south and Beykoz on the north (Özonat, 2017).
Figure 3. shows the study area.
Elmalı dam in the basin area consists of two parts. 
The construction of the first of these was completed between 1891- 1893 (Gurbetoğlu, 1996). 
The second Elmalı Dam was started to be built on Çavuşbaşı Stream in 1952 and was completed in 1955 (Özonat, 2017).
3. RESULTS
3.1 Land Uses Maps And Change Detection
As can be seen in Figure 4., the beginning of the construction of the previously planned structures from 2013 to 2020, the transportation roads completed in this period, the organized industrial zone in the basin area and the newly created structures caused the settlement area to increase. 
However, considering the increasing population due to the job opportunities they provide and the urbanization situation that comes with it, it is inevitable to see an increase in settlement areas. 
Considering the land use status of the study area, it is seen that the forest area contains a large amount of area compared to other classes due to the dense forest area around the water area of the basin. 
Despite this, it has become a remarkable point that the southern parts of the city create quite a lot of residential areas. 
Table 1. the land use cases for 2013, 2015, 2018 and 2020 are given. 
As can be seen in Table 1., the settlement area, which was 2528.01 hectares in 2013, increased to 3026.70 hectares in 2020. 
While the water area was 61.65 hectares in 2013, it decreased to 39.24 hectares in 2018 and then increased. 
While the forest area was 3300.39 hectares in 2013, it decreased to 3170.25 hectares in 2020 and showed a continuous decrease over the years. 
Likewise, the agricultural/pasture area, which was 939.51 hectares in 2013, decreased to 931.32 hectares in 2020. 
The dark soil area, which was 1222.38 hectares in 2013, decreased to 1072.71 hectares by 2020. 
Similarly, the light soil area, which was 296.01 hectares in 2013, decreased gradually until 2020. 
Kappa coefficient, which was formed from error matrices, was used in the accuracy evaluation stage, where the accuracy of the classification was examined (Çetinkaya and Toz, 2007). 
Kappa value takes a value between 0 and 1, and it is desirable for the Kappa value to be 1 (Çetinkaya and Toz, 2007). 
Applications where the Kappa coefficient exceeds 0.75 and the overall accuracy rate exceeds 80% are considered reliable (Kaya and Toroğlu, 2015). 
Table 2. shows the overall accuracy and kappa value. 
Kappa value for 2013, 0.94; the 2015 kappa value for 0.97; the 2018 kappa value for 0.94, the 2020 kappa value for 0.97, and the values are quite successful. 
Considering these values, it is concluded that the support vector machines method used in the controlled classification of the working years has reached a high level of accuracy.
4. CONCLUSION 
In line with the data in Figure 4. and Table 1., it is seen that the settlement area, which was at its lowest level in 2013, increased in 2015 and 2018 and approached the forest area in 2020.
As a result of the data obtained in Figure 4. and Table1., it has been interpreted that forest, agriculture/pasture and light soil areas have decreased over the years, and that the dark soil area has changed depending on the land use.
Figure 6. shows the graph created to compare the land uses of the working years.
According to the information given in the study, the settlement area in 2013 increased by approximately 5% to form the settlement area in 2015, and the settlement area in 2013 increased by approximately 20% to form the settlement area in 2020.
Although there was an increase of almost 14% in the water area from 2013 to 2015, there was a decrease of 44% from 2015 to 2018.
According to the data obtained for each year included in the study, the forest area decreased from year to year.
It also decreased by almost 4% from 2013 to 2020.
Likewise, the agricultural/pasture area has decreased over the years, with a decrease of approximately 0.9% in 2020 compared to 2013.
Considering the dark soil class, although it increased by 21% from 2015 to 2018, there was a 12% decrease from 2013 to 2020.
It has been commented that the reason for this change may be due to the confusion with the agricultural/pasture area in some areas and due to the ongoing works.
If we take a look at the light soil class, it decreased gradually compared to the previous year and showed a decrease of approximately 73% in 2020 compared to 2013, which is the most striking value in the table.
The perception of the transportation roads under construction as light soil from the satellite image and the progress of the road works in the following years explain the reason for this situation.
It is seen that the settlement areas have increased gradually during the working years and the areas such as forest, agriculture/pasture have decreased.
Such problems are caused by many reasons such as uncontrolled changes in land use, use of fertile lands in various structural activities without making the necessary distinction instead of agricultural/pasture areas that have lost their function.
In addition, the destruction of forest and agricultural/pasture areas in order to open settlements and access roads are among the reasons.
In order to leave a sustainable environment for the future, attention should be paid to land use activities in basin areas and action should be taken in line with land use classes.
Although Elmal? Basin has smaller water storage area compared to other basins in the province where it is located, it is seen that it has the highest density in terms of population and a large amount of settlement area (Kaya, 2008; Ozonat, 2017; Budak and Tuzun, 1993).
Accordingly, the unplanned construction in the region and the high level of pollution of the streams that bring water to the basin also affect the pollution rate of the basin waters (Senol, 2012).
The main reason for the changes in land use, especially the decrease in forest and agricultural/pasture areas, is of course the uncontrollability of construction at some points.
Because people need single or multi-storey buildings to work and live.
In order to meet these needs, they feel a sense of migration to places with a large workforce and put this into action.
While the population factor contributes to the development of the region and the preservation of its urban vitality, it also brings with it negativities.
A branch of the Istanbul ring road connecting to the Fatih Sultan Mehmet Bridge passes through the basin area, at the same time, there is the Dudullu Organized Industrial Zone in the region and there are many structures such as the UskudarUmraniye-Cekmekoy Metro Line Segment Factory and Warehouse Area.
Therefore, the area has moved away from its rural features.
Remote Sensing and Geographic Information Systems technologies, which are very beneficial in terms of cost and time, should be used effectively and efficiently by considering the basin area as a whole.
Care should be taken to work multidisciplinary, as the basin areas are composed of a wide variety of ecosystems.
By taking the opinions of different disciplines, the most correct joint decisions should be formed for the basin area.
In order to prevent floods, strengthen groundwater and increase soil permeability, it would be beneficial to use concrete and asphalt with high absorption power in order to leave a sustainable basin area, although it is costly, and to regulate atmospheric relations.
It is thought that when the site gardens, parking areas, sidewalks and roads are formed with this permeable material, the damages of concretization will be alleviated to some extent.
Finally, the Elmal? basin, which was studied as a result of the findings obtained from the spatial analyzes prepared by using the Landsat-8 satellite images of the years 2013, 2015, 2018 and 2020, should be used according to the skill classes, taking into account the natural environment conditions.
Of course, this also applies to other basin areas.
Citizens should be made aware of this issue.
For a sustainable basin area, studies should be carried out with relevant public institutions and organizations, as well as non-governmental organizations, and new generation breakthroughs should be made in land use and basin area use planning.
Impact of the b-Learning Model on University Teaching
Abstract-In this work, the impact of the implementation of B-Learning and traditional face-to-face models is analyzed, by comparing the academic results achieved by four groups of university students in the electronics course, by using objective and subjective measurements.
Analysis of the results based on objective data demonstrated a lower student dropout rate and an improvement in the general grade point average under the B-Learning model.
The subjective comparison collects student's opinions through a formal evaluation instrument.
This analysis concluded that students perceive the teaching-learning process under the B-Learning model more motivating, more useful and that it meets their expectations in a better way.
Finally, the Student's t test is carried out to demonstrate that the analyzed research variables present statistically significant differences.
Index Terms—B-learning, information technology, higher education, educational innovation, professional education.
I. INTRODUCTION
At the beginning of the 21st century, there was a relevant increase in the registration of domains on the Internet related to education, which resulted in an excessive offer of distance educational courses that led the rise of the online learning model (e-learning) [1].
The trend in educational institutions was to develop online learning courses that would provide the opportunity for students to obtain knowledge from different geographical locations, in a flexible schedule and with the reduction of some costs.
However, completely transforming face-to-face courses to meet the specific attributes of online learning environments was an effort and resource-intensive activity.
Learning quality and objectives were affected when students exclusively used e-learning as a teaching method, principally due to a) lack of motivation to read all the online learning material, b) procrastination and c) lack of interaction with the teacher and other students.
In this way, the practical use of e-learning model generated serious questions about its effectiveness and efficiency, and it was the implementers themselves who detected the need to combine technological resources implemented with face-to-face reinforcements to encourage and motivate the development of professional skills of students. [2].
The offering of e-learning model together with face-to-face activities was reflected in greater efficiency, which made possible a new model for the specific needs of the teaching-learning process: hybrid learning or Blended Learning model (B-Learning).
At present, both terms are used without distinction [3].
In the literature we can find different definitions of the B-Learning model.
There are two definitions of B-Learning that are most frequently cited in the literature.
Graham [4] defines B-Learning as "a system that combines traditional face-to-face learning with computer learning".
Garrison & Kanuka [5] define B-Learning as "the reflective integration of face-to-face learning experiences in the classroom with online learning experiences".
As can be observed, there is general agreement that the key components of B-Learning are face-to-face learning and online learning supported by information technologies.
In this way, we can define the B-Learning as a model that combines virtual and face-to-face teaching and is a valuable tool to improve the learning process by using information technologies [6].
However, as we can see in Fig. 1, this model begins from two possibilities, the first is to incorporate face-to-face activities to E-Learning model, but it also arises when virtual activities supported by information technologies are included to the face-to-face education [7].
Pinpoint the exact moment of appearance of B-Learning model is not easy, nor is it easy to specify its first execution.
The B-Learning model has been established as an emerging educational option, whose evolution has occurred naturally, based on constant experimentation to achieve improvements and perfect educational techniques from a personal and group perspective.
The implementation of the model responds to a social and cultural context that goes hand in hand with the evolution of information technologies.
In this way, this new technological context merges with the need to renew the pedagogical organization and make educational innovations.
The B-Learning model is a topic of great current interest that implies a change of paradigm where the emphasis shifts from teaching to learning [8].
A great challenge is how the information technology tools are used while the participant engagement is successfully ensured considering individual characteristics of students [9].
According to previous studies, the B-Learning model can bring several advantages to the teaching-learning process, among which are the following: a) This model provides flexibility to the student, allowing the student to access the online content and develop the academic activities available within the course at his own pace.
Not all students will be able to learn at the same pace, it depends on multiple individual factors, and this model allows knowledge to be consumed at a convenience, b) One of the most tangible advantages is the possibility of accessing online resources from anywhere (ubiquitous learning) and through different mobile devices with Internet connection, such as a desktop computer, a laptop, a tablet or a smartphone, c) The coverage of attention to a larger number of students becomes possible through educational platforms, which allow programming activities with targeted evaluation and feedback.
In this way, the platform allows several students to interact at the same time with the different contents of the course, d) Through the management of contents arranged in technological platforms, it is possible to establish levels of depth in the knowledge that the student will be able to consume considering his individual requirements, e) The student eliminates some transportation expenses, since it will not be necessary for him/her to attend all face-to-face academic sessions.
This represents some cost savings, as well as time savings due to the transfers that can be used for other academic or personal activities, f) From the point of view of the administration of academic resources, different types of learning content can be stored and accessed in an equitable manner and through different media, which will allow better results to be obtained, and g) B-Learning model privileges the interaction among students, in whether face-to-face or virtual.
For this purpose, the final objectives of this interaction must be considered, and the necessary pedagogical tools must be made available to the students for this purpose.
In the case of virtual tools, the model relies on information technologies to include communication forums, blogs, chats, e-mail, and asynchronous message posting [10].
Nevertheless, only few of these studies are focused on higher education and do not consider the effects of B-Learning from the perspective of students.
It should be considered that users who experiment complications with technology can result in the abandonment of the learning program and the eventual failure of technological applications.
Students are important partners in any learning model and thus their backgrounds and characteristics influence their ability to continue learning effectively.
This work focuses on resolving these two current issues.
The objective is to compare the implementation of the B-Learning and traditional face-to-face models in university teaching.
Since a desirable indicator in the teaching-learning process is that students develop knowledge and skills that are reflected in better results in their evaluations, the academic results achieved by students are analyzed, particularly the degree of dropout and grades achieved during the course, which are considered objective measurements.
On the other hand, derived from the fact that one of the most important aspects in the implementation of a teaching model is the experience of the students, a collection of information is carried out to measure the way in which the students perceive the implementation of the B- model.
Learning, which is considered a subjective measurement.
II. METHODOLOGY
This research evaluates the academic results obtained in the Electronics course, taught in the third semester of the Information Technology Engineering educational program, at the Tecnologico de Monterrey University.
Four groups of students divided into two sets are evaluated, two groups under the traditional face-to-face model and two under the B-Learning model.
The data obtained oscillate between the years 2017 and 2021, where the traditional face-to-face model corresponds to the courses taught in 2017 and 2019, and the B-Learning model corresponds to the courses taught in 2020 and 2021.
It is important to note that the study plans of the subject were not modified during the research period, therefore, the learning topics are the same in the application of both methods.
The B-Learning model integrates educational activities organized for individual and group work, supported using information technologies, with the aim of consolidating the knowledge of face-to-face lessons.
The teachers encouraged the execution of these activities, highlighting the objectives of the approach and its relationship with the face-to-face lessons.
In general, we can classify the activities carried out as part of the B-Learning model, such as laboratory practices, creation of support videos, and participation in virtual spaces for collaboration and evaluation activities.
Fig. 2 illustrates an example of the activity "Module 2 / Practice 2: Circuits with diodes".
The upper part of Fig. 2 shows an example of the work carried out in a traditional face-to-face class making use of the laboratory facilities and equipment for the design of a full-wave rectifier circuit, while the lower part shows the same activity, but carried out with the Tinkercad R simulator, which is a free online tool that, among other functions, allows to model circuits with great realism.
The use of the tool is preceded by an explanation of the subject to be dealt with, in face-to-face mode, and its use allows students to get a virtual, ubiquitous, safe laboratory with unlimited material; and from which they can get the results and acquire the expected skills on the operation of the rectifier diodes.
Finally, in the implementation of the B-Learning model, not only the traditional face-to-face activities were modified, but also the way of presenting course contents to the students.
For this modality, a series of support videos were created for all the course topics, and they were posted on the YouTube R platform (Fig. 3).
Several studies have shown that the use of educational videos takes advantage of the communicative potential of images, words, and sounds, for the construction of meaningful knowledge through the construction of an experience that promotes the stimulation of the human senses and the different types of learning of students [11].
However, the teacher will be the one who must determine how, when and for what purpose to use the concepts and thus give them meaning and educational value.
In general, this type of material is very well accepted by the students.
This can be proved through a feedback exercise on the course, where students were asked what aspects, they liked most about the B-Learning model and it was found that 14 out of 20 comments allude to videos as a desirable content within the classes.
A. Sample Selection and Variables 
The total sample corresponds to a total of N = 55 students. 
This sample is divided into two data sets. 
The first set N1=27 corresponds to students who took the Electronics course with the traditional face-to-face model and the second set N2=28 to those students who took the same course but using the B-Learning model. 
Regarding the research variables, two hypotheses were generated to be evaluated according to the obtained results:
1) Hypothesis 1: The application of the B-Learning method has a positive effect on the student's academic results.
2) Hypothesis 2: Students perceive their experience in the course in a more positive way with the application of the B-Learning method compared to the same course under the traditional face-to-face modality.
To evaluate the first hypothesis, we use two variables: 1) the non-dropout rate (TASA_ND), denoted by the percentage of students who remain from the beginning to the end of the course and calculated as the ratio of students who present the final exam of the subject against those originally enrolled and; 2) the average of grades obtained in each school year (PROM_CALIF), in which all students who take the final exam are considered regardless they pass or not the course.
Regarding the second hypothesis, we use an institutional instrument applied to students of Tecnologico de Monterrey University at the end of each school year to evaluate their perception of the teaching-learning process, which is known as the Student Opinion Survey (ECOA).
The ECOA is divided in three blocks of two questions each to determine the SATISFACTION, MOTIVATION and UTILITY, perceived by the students.
Each question has a scale of 5 to 10, where 5 indicates the lowest level of evaluation and 10 the highest level.
Table I shows the elements that set up the evaluation instrument proposed for this research.
The non-dropout rate (TASA_ND), is represented as the percentage of students who remain throughout the course and is measured as the ratio of students who take the final exam, against those originally enrolled in the course.
It should be remembered that within the educational model of TEC de Monterrey, students have the possibility of withdrawing or withdrawing subjects during the first six weeks of the course, for the reasons that the student considers necessary and that is convenient for him.
Historically, it has been determined that these reasons are usually due to the desire to lighten a high load of subjects by choosing to drop out of those where there is minimal motivation or the worst learning experience.
It can also be due to personal, family and/or financial problems, but this occurs to a lesser extent.
Fig. 4 shows that B-Learning model has a notable contribution to the non-dropout rate of the course.
In 2020, which was the first year of incorporation of the B-Learning model, there was a non-dropout rate of 95%, which meant the absence of only one student to the final exam of the subject, and for 2021 the rate was 100%, that is, there were no dropouts.
This data shows an important improvement compared to the years 2017 and 2019 where there were non-dropout rates of 81% and 87% respectively.
Subsequently, the average of grades obtained in each school year is calculated.
This data allows to know the level of knowledge acquired by students in the field of Electronics.
Fig. 5 shows that results obtained in the implementation of the B-Learning model are much higher compared to the traditional face-to-face model.
It should be noted that grade point averages consider the results of all students who take the final exam, even if they do not pass the course.
To measure the students' perception regarding the teaching-learning process, the results of the application of each model should be compared from the questions formulated within the ECOA.
In this sense, it is necessary to process the data obtained to determine the contribution of each question of the ECOA survey towards the research variables and to standardize the research variables in the evaluated periods.
It should be noted that the ECOA establishes the results for each question of the survey applied to the students, but not for the research variable.
For example, the ECOA returns the results of the ETEVA and ETASE questions, but these must be combined to calculate the SATISFACTION variable.
In this specific example, this is accomplished by combining the mean and standard deviation values from the ETEVA and ETASE ID questions.
To do this calculation, the mean and standard deviation values of the ETEVA and ETASE questions are considered as values from two different populations.
Thus, to combine the mean values of two different populations, we use the following formula:
Once the mean and standard deviation values of two questions have been combined, these values will match to the research variable.
Specifically, combining the mean and standard deviation values obtained for the ETEVA and ETASE questions will give us the mean and standard deviation of SATISFACTION variable, ETAPR and ETRET questions give us the values of the MOTIVATION variable and the combination of the ETMET and ETASE questions will give us the values for the UTILITY variable.
Once you run this procedure, you will get two mean and standard deviation values for each variable.
These two values will correspond to each educational period evaluated.
To standardize these results and find a single value of mean and standard deviation, the procedure is applied a second time.
The result is presented in Table II, where we can observe the values of each research variable for each educational model.
In this case, the average (M) of the students' opinion is consolidated, as well as its standard deviation (S).
To carry out a simpler comparison, Fig. 6 graphically shows the result of the mean for each research variable, in each educational model applied.
Fig. 6 shows that the B-Learning model obtains a lot better results for the three research variables in comparison with the application of the traditional face-to-face model.
It is relevant to note that the variable with the greatest difference in this comparison is MOTIVATION.
The difference between the motivation generated by the application of the traditional face-to-face model and the B-Learning model is very noticeable if we consider that the difference between the values of both variables is close to twenty percentage points.
Even though both models considered the same thematic content, the incorporation of information technologies generated greater motivation in the students.
This finding can be related to the results obtained in the non-dropout rate and the grade point average, since when there is motivation the learning results are increased.
Regarding the UTILITY variable, the results indicate that students understand the relevance of the electronics course under the B-Learning model has for their university studies and their professional trajectory.
The use of information technologies in the B-Learning model allows the student to use a laboratory with unlimited material and in a ubiquitous way by using computer simulators.
Also, collaborative work platforms and the creation of videos on the content of the laboratory practices, allow the student to reflect on the knowledge acquired.
The difference found between the perceived utility in the traditional classroom model and the B-Learning model represents ten percentage points.
As for the SATISFACTION research variable, it reflects the well-being that students experience in relation to the academic expectations of the course.
In this case, the activities carried out under the B-Learning model allowed these expectations to be met and this is reflected again, in better results for this model compared to the traditional face-to-face model.
Finally, the data obtained for each research variable in the models were statistically analyzed with the Student's t-test to formalize the research results and determine the existence of significant statistical differences.
The Student's t-test is used as a statical tool to evaluate the mean value of one or two groups through hypothesis tests.
Conceptually, the value obtained through this test represents the number of standard units that are separating the means of the two groups evaluated.
At the time to calculate the Student's t-test, it is assumed continuous data, that have homogeneity variance and have normal distribution.
An important note is that the Student's t-test requires that the populations to be compared have the same length and originally in our research we have that the population of students who received the electronics subject under the traditional face-to-face model is N1 = 27 and the total of students who received the same subject using the B-Learning model is N2 = 28, therefore, we have that N1?N2.
To solve this problem, it was necessary to standardize both populations by randomly removing a data from the N2 population.
The significance criterion used to perform the Student's t-test was p <0.05 and this was carried out using the Minitab R version 19.2020.1 software for Windows R. Table III shows the result of the Student's t-test.
We can observe that there is a statistically significant difference for all the research variables since the initial condition established p <0.05 is met in all cases.
IV. CONCLUSION
Educational innovation implies a notable improvement in teaching-learning process that promotes changes in aspects such as pedagogy, academic tools, didactics, processes, and people.
Consequently, students should feel motivated in the usage of improved academic resources, perceive that the expectations of the expected value are satisfied, and perceive the utility and relevance that this innovation will bring to their professional lives.
In this context, within this research we analyze the academic impact of the implementation of the B-Learning model, as part of an educational innovation in the subject of Electronics in the third semester of the Information Technology Engineering educational program, at the TEC de Monterrey University.
In this research, it has been verified in a practical way that the integration of B-Learning model in the teaching-learning process provides several advantages through the addition of educational practices in the field of Information Technology.
It has been confirmed that the application of the B-Learning model generates a positive impact on the average of grades obtained by students in contrast to those obtained in the traditional face-to-face model, which reflects a higher level of success and skills acquired by students during the course.
Also, it was found that students perceive the teachinglearning process by using B-Learning, as a more motivating model, more useful and that better meets their expectations, compared to the traditional face-to-face model.
This reflects that the students positively perceive their learning experience and feel more satisfied under B-Learning model.
CONFLICT OF INTEREST
The authors declare no conflict of interest
AUTHOR CONTRIBUTIONS
Antonio Cedillo-Hernandez was responsible for teaching, led the execution of the entire project, and wrote the paper; 
Lydia Velazquez-Garcia guided the research orientation, perform the data analysis, and complete the paper review process. 
All authors had approved the final version.
ACKNOWLEDGMENT
The authors would like to acknowledge the financial support of Writing Lab, Institute for the Future of Education, Tecnologico de Monterrey, Mexico, in the production of this work.
On the (In)Security of 1090ES and UAT978 Mobile Cockpit Information Systems—An Attacker Perspective on the Availability of ADS-B Safety- and Mission-Critical Systems
ABSTRACT Automatic dependent surveillance-broadcast (ADS-B) is a key air surveillance technology and a critical component of next-generation air transportation systems.
It significantly simplifies aircraft surveillance technology and improves airborne traffic situational awareness.
Many types of mobile cockpit information systems (MCISs) are based on ADS-B technology.
MCIS gives pilots the flight and trafficrelated information they need.
MCIS has two parts: an ADS-B transceiver and an electronic flight bag (EFB) application.
The ADS-B transceivers transmit and receive the ADS-B radio signals while the EFB applications hosted on mobile phones display the data.
Because they are cheap, lightweight, and easy to install, MCISs became very popular.
However, due to the lack of basic security measures, ADS-B technology is vulnerable to cyberattacks, which makes the MCIS inherently exposed to attacks.
Attacks are even more likely for the MCIS, because they are power, memory, and computationally constrained.
This study explores the cybersecurity posture of various MCIS setups for both types of ADS-B technology: 1090ES and UAT978.
Total six portable MCIS devices and 21 EFB applications were tested against radio-link-based attacks by transmission-capable software-defined radio (SDR).
Packet-level denial of service (DoS) attacks affected approximately 63% and 37% of 1090ES and UAT978 setups, respectively, while many of them experienced a system crash.
Our experiments show that DoS attacks on the reception could meaningfully reduce transmission capacity.
Our coordinated attack and fuzz tests also reported worrying issues on the MCIS.
The consistency of our results on a very broad range of hardware and software configurations indicate the reliability of our proposed methodology as well as the effectiveness and efficiency of our platform.
INDEX TERMS Cybersecurity, attacks, ADS-B, ATC, ATM, UAT978, 1090ES, availability, DoS.
I. INTRODUCTION
THE demand for air transportation has been steadily increasing over the last few decades.
The Federal Aviation Administration (FAA) predicts that the number of passengers in commercial aviation will increase to 1.15 billion by 2033 [1].
On the other side of the Atlantic, Eurocontrol predicts 1.6 billion air passengers in its sky per year by the early 2030s [2].
In addition, air cargo transportation, military aircraft, and unmanned aerial vehicles are expected to boost air traffic in the coming years.
As a result, the number of aircraft in the airspace will continue to increase, and the airspace will become even more crowded.
For reasons such as the safety of navigation, increased airspace capacity, improved flight safety, and future navigation needs, in 2004 the FAA initiated the Next Generation Air Transportation System (NextGen) project.
NextGen focuses on the modernization of America's air transportation system to make flying even safer, more efficient, and predictable.
One of its aspirations is to gradually transform the current obsolete and imprecise radar-based air traffic control (ATC) and air traffic management (ATM) systems into a fully digital and satellite-based navigation system.
To implement this, the FAA chose the Automatic dependent surveillance-broadcast (ADS-B) system to be a core part of future air navigation technology in the US.
In 2011, the EU also mandated a gradual ADS-B requirement starting in June 2020 [3].
The core idea of ADS-B is to periodically broadcast the position and other flight-related information of an aircraft to the ATC and other aircraft in the vicinity via radio frequency (RF) data link.
The ADS-B communication system's construction and maintenance costs are expected to be only one-tenth of radar-based navigation [4].
This simplified air navigation technology is gaining popularity all over the world.
Light weight yet effective ADS-B technology is easy to adapt and use.
For example, the ADS-B transceiver and smartphone-based mobile cockpit information system (MCIS) is very trendy in the general aviation (GA) sector.
In this system, a small ADS-B transceiver is connected to a smartphone or other smart device that displays the navigation data to the pilot through an electronic flight bag (EFB) application.
It also transmits global navigation satellite system (GNSS) location, flight information, and other useful information via the ADS-B antenna.
MCIS setups cost around 500-1000 dollars.
The affordable price and the ease of installation make such setups attractive to pilots of private planes.
Studies show that firmware vulnerabilities are quite common in Internet-of-Things (IoT) and embedded devices [5], [6] and this is also the case for ADS-B technology.
The main reason for this insecurity is that ADS-B does not utilize basic security measures such as authentication and encryption.
There have been many reports of ADSB exploitation in the industry [7], [8] and in academia [9]-[12]; therefore, MCISs can be labelled inherently insecure.
Even though many studies investigated the security of ADS-B, the security assessment of MCIS remains particularly under-researched.
Compared to the powerful transponder or desktop setups, these power-, memory-, and computationally constraint mobile setups could be more vulnerable against cyberattacks.
Nonetheless, the use of mobile setups is increasing rapidly.
The 21 EFB applications used in this study were downloaded more than 650,000 times from the Google play store, leaving alone other non-tested applications and iOS platform's download numbers aside.
Assessing the security of such safety- and mission-critical systems against modern cyberattacks has motivated us to conduct this research.
Our main contributions with this work are:.
1) We present a systematic and comprehensive study of the (in)security of different commercial-grade MCISs. 
2) We test the impacts of the attacks on a large number of EFB applications. 
3) To the best of our knowledge, we implement and demonstrate the first-ever ADS-B attacks over UAT978.
4) We demonstrate that the UAT978 and 1090ES implementations are comparably vulnerable to generic and available cyberattacks. 
The rest of this article is organized as follows: Section II introduces the relevant background on ADS-B and MCIS. 
Related studies are discussed in Section III. Details of our test platform and experiment setup are presented in Section IV. Attacks on MCIS are explained in Section V. 
Attack results are evaluated in Section VI. We discuss some solutions in Section VII. Finally, with Section VIII we conclude this article.
II. BACKGROUND
Modern aviation has relied only on primary surveillance radar (PSR) for a long time.
With PSR, the position of the aircraft is measured by the distance and the angle to the radar, but the identity of the aircraft remains unknown.
For this purpose, secondary surveillance radar (SSR) was developed.
SSR transmits interrogation pulses using RF signals, which are known as Mode A and Mode C.
These pulses allow the SSR to continuously interrogate the identity and the barometric altitude of an aircraft.
However, the SSR systems have reached the limit of their operational capability.
Mode A communication is limited to 4096 unique codes, which poses an issue for very busy modern air transportation.
Therefore, a more advanced aircraft communication protocol is needed.
Mode S was designed to solve these problems.
Mode S is an SSR process that allows selective interrogation of aircraft according to an aircraft's unique 24-bit code called the International Civil Aviation Organization (ICAO or ICAO24) address.
Based on Mode S, ADS-B's concept was evolved and it is now considered the future replacement of SSR.
ADS-B is a surveillance technique that relies on aircraft broadcasting their identity, position, and other information derived from onboard systems periodically without the need for interrogation.
Besides the ground station, other aircraft also can receive the broadcast to have situational awareness and self-separation.
The most important part of the ADS-B is position information, which is determined by GNSS.
There are two main functionalities in ADS-B: ADS-B IN and ADS-B OUT.
ADS-B IN refers to receiving, processing, and displaying the ADS-B signals from the ATC, aircraft, and other ADS-B OUT-equipped vehicles.
ADS-B OUT refers to transmitting an aircraft's position, identity, velocity, and additional flight-related information.
For data transmission, two datalink solutions are used as the physical layer for the ADS-B: 1090 MHz Extended Squitter (1090ES) and Universal Access Transceiver at 978 MHz (UAT978).
Figure 1 depicts the ADS-B protocol in SSR.
A. 1090ES
1090ES uses the 1090 MHz radio frequency to transmit ADS-B OUT via a Mode-S transponder.
Squitter refers to a burst or broadcast of aircraft-tracking data transmitted periodically by a Mode S transponder without interrogation from the controller's radar.
There are two types of squitters: short squitter and extended squitter.
As short squitter includes downlink format capability, ICAO24 address, and cyclic redundancy check (CRC).
An extended squitter contains all the information of a short squitter but it also includes altitude, position, heading, and velocity.
To analyze all the information, we have focused on the extended squitter in this study.
The ADS-B 1090ES signal is modulated using pulse position modulation (PPM), which is 112 bits long.
A 0.8чs preamble should precede the data block.
B. UAT978
UAT978 applies to aircraft that fly below 18,000 feet in the US, mainly focusing on GA.
If an aircraft flies above 18,000 feet, it must be equipped with an ADS-B 1090ES transmitter.
Besides navigation, UAT978 also provides services such as flight information system-broadcast (FIS-B) and traffic information system-broadcast (TIS-B).
UAT978 uses continuous phase frequency shift keying (CPFSK) modulation with a modulation index of 0.6 and a data rate of 1.041667 Mbps.
There are two types of UAT ADS-B downlink messages: basic and long.
A basic message contains 144 bits, while a long message has 272 bits.
Forward error correction (FEC) is performed using a systematic Reed-Solomon error correction code.
For the basic message, the FEC should be 96 bits long, and for the long message, the FEC should be 112 bits long.
111010101100110111011010010011100010 is the default synchronization bit pattern for both types of messages in UAT978.
C. MOBILE COCKPIT INFORMATION SYSTEM
Compared to SSR, ADS-B is very handy and lightweight.
With this simplified version of the air navigation technique, many manufacturers offer portable ADS-B transceivers.
Some of these transceivers can fit in the plane's cockpit; some are hung on the window.
They transmit and receive the ADS-B signals with a built-in antenna or via the aircraft's antenna port.
EFB applications hosted on smartphones or tablets are connected to the transceiver device via WiFi.
EFB application displays all the necessary navigation data to the pilot.
These portable transceivers are programmable via computer or mobile application.
A needed change in the static information (e.g., ICAO24 address, flight number, squawk code) can be done via the nominated program.
In contrast, dynamic data (e.g., location, altitude) are changed automatically via the GNSS receiver of the device.
Figure 2 shows a MCIS setup, where data ADS-B data from the SkyEcho2 transceiver is displayed on OzRunways EFB application via WiFi network.
D. TERMINOLOGY CLARIFICATIONS.
During different phases of the experiment in this study, we observed different behavior from various tested configurations and MCISs. 
Below we explain the terminology and meaning of states, as used throughout this paper:
Crash: If a MCIS totally shuts down unexpectedly or ungracefully due to software misbehaviour from the Denial-of-Service (DoS) attack inputs, we classify that as a crash. 
Commonly, this is the first and immediate step before an attacker can perform remote code execution (RCE) or arbitrary code execution (ACE) attacksThis means the attacker can execute their own code (e.g., ransomware, malware) on the affected system (e.g., device, software, MCIS). 
Unresponsive: Some setups did not crash but they could not handle the overwhelming amount of data. As a result, they hang on, which is described as unresponsive. 
Output clogged: Some setups, perhaps to avoid a system crash or due to design limitations, can decode or display a limited number of aircraft. 
The setups cyclically show the ADS-B message within that capacity. 
Sometimes the ADS-B messages from new aircraft replace the old ones within that limit, or new aircraft from valid ADS-B signals do not appear at all. 
This situation is called output clogged. 
Unreadable screen: When the system is flooded with attacker ADS-B signals, the very large number of data fields and aircraft icons make it impossible to read the screen. 
However, the system keeps functioning without crashing or becoming unresponsive, though most of the time, the system becomes slower.  
No effect: Despite the attack, if the system behaves normally without any visible/observable DoS or sideeffects, we called that no effect. 
However, none of the tested MCISs were able to handle a massive amount of ADS-B messages (e.g., 200,000 or more). 
For instance, we observed in many MCISs a significant amount of valid messages being dropped (i.e., the number of processed/displayed ADS-B messages is significantly lower than the number of input ADS-B messages we send). 
In such cases, we called this no effect but make a side comment that messages were dropped. 
III. RELATED STUDIES
Securing the ADS-B has drawn massive attention from researchers due to its direct connection to aircrafts' safe navigation and the effects that failures have on passengers' life.
As early as 2004, Krozel and Andrisani [15] reported that data dropouts, erroneous inputs, and deception might degrade data integrity from ADS-B-equipped aircraft.
They proposed verification and validation techniques to ensure data integrity using a Kalman filter.
The filter would smooth out noise in measured ADS-B signals, identify and suppress erroneous data, coast between data dropouts, and provide the current best state estimates.
Since then, there have been many kinds of studies to enhance ADS-B communication's authenticity, security, confidentiality, and integrity [16]-[18].
Sampigethaya [19] focused on the security of ADS-B and proposed a framework for broadcast data link-based navigation and surveillance for the ADS-B-enabled aircraft.
Costin and Francillon [9] presented the first public implementation and results of launching ADS-B message injection and spoofing attacks.
Strohmeier et al. [20] analyzed the 1090 MHz communication channel to understand the behavior of ADS-B 1090ES under increasing traffic load and security challenges.
They concluded that the cheap and easily available SDRs posed a significant threat to ADS-B communication and could be used for practical RF-based attacks.
Schafer et al. [11] implemented attacks on ADS-B 1090 using USRP N210 as the transmitter and SBS-3 as the receiver.
They showed that active attacks such as ghost aircraft injection, ghost aircraft flooding, ground station flooding, and virtual trajectory modification are easily implemented using low-cost devices.
McCallie et al. [21] analyzed the security vulnerabilities associated with ADS-B implementations.
They classified the attacks and examined the potential damage that the attacks may have on air transportation operations.
They stated that ADS-B exploitation could cause disastrous consequences, confusion, aircraft groundings, and in the worst case even plane crashes.
Manesh et al. [22] used Piccolo autopilot and a portable ground station to observe the autopilot's ghost aircraft injection response.
They injected fake ADS-B messages causing ghost aircraft to appear in the vicinity of the Piccolo autopilot (ownership).
This caused the autopilot to take evasive measures to avoid the collision.
Subsequently, they pushed the ghost aircraft very close to the autopilot.
The sudden appearance of false aircraft caused the pilot to execute a steep turn and start descending to regain well-clear as soon as possible.
Eskilsson et al. [23] demonstrated ADS-B and controller-pilot data link communications (CPDLC) attacks using HackRF.
They used freely available ADSB_Encoder.py Python script [24] to encode ICAO, latitude, longitude, and altitude information into an IQ file.
Later the file was transmitted over the air using a HackRF device and decoded by dump1090 software.
They stated that simple implementation, systematic documentation, and relatively inexpensive equipment could also result in an increasing number of people carrying out an attack.
The acquisition of more attacking devices can lead to a large-scale attack.
Tabassum et al. [25], [26] concluded ADS-B systems are prone to message and payload loss.
In their exploratory analysis, they found that message contents are sometimes inconsistent with nominal conditions.
They spotted message dropout, partial message content losses, data drift from the nominal value, and discrepancies between geometric and barometric altitude.
They suggested that prior to the complete implementation of ADS-B, it is important to address, understand and monitor these deficiencies.
Air communication modes are also a significant source of big data that must be handled securely and effectively.
Mink et al. [27] analyzed the unaddressed big data issues for NextGen.
They evaluated the NextGen system using five differentiated qualitative characteristics of big data: volume, velocity, variety, veracity, and value.
They estimated that all modes (Mode A, C, and S) combined would generate 41 TiB data per year at a velocity of 13 messages per millisecond with no encryption.
These findings indicate that the NextGen system has several big data challenges that must be addressed it it is to obtain its maximal potential.
However, no such study in Europe has been conducted yet.
Wu et al. [10] did a survey of the security issues of ADS-B.
They noted that the attack intention could be for economic benefit, terrorism, cyber warfare, or personal interest.
The authors modeled the attacker as professional hacking groups, terrorist organizations, military organizations, or amateurs.
The survey showed that a single solution does not fully protect the ADS-B system's security.
The public key infrastructure or spread spectrum technology can resist most attacks, but there are still deficiencies.
They proposed a multi-layered security framework.
Most recently, Leonardi et al. [28] studied the effect of jamming attacks in crowd-sourced air traffic surveillance.
They found that ground-based communication link jamming can disrupt ADS-B communication more easily and effectively than an air-based jammer and it is easy to implement the attack from the ground.
Their work complements our study in the sense that it analyzes DoS attacks on air traffic surveillance (including ADS-B).
However, they performed the DoS on the communication link where we performed it on the datalink ADS-B layers.
Dave et al. [29] reviewed the cybersecurity challenges in aviation communication, navigation, and surveillance.
According to them, as the aviation sector becomes digitized and increasingly reliant on wireless technology, cyberattackers in this sector are also increasing.
From old VHF, CPDLC, and PSR to today's ADS-B technology, all are proven to be vulnerable to cyberattacks.
Moreover, the unencrypted nature of ADS-B opens many other attack paradigms.
SDR availability is one of the most technical advantages for attackers.
Many GA pilots use MCIS, which is very handy and easy to install.
Lundberg et al. [30] found that this type of mobile setup is not a part of the onboard systems.
Thus, its reliability does not meet the standards applied to traditional avionics such as radio technical commission for aeronautics, aeronautical radio incorporated, and the European organisation for civil aviation equipment.
They tested three sets of hardware and applications: Appareo Stratus2 receiver with the ForeFlight app, Garmin GDL 39 receiver with the Garmin Pilot app, and SageTech Clarity CL01 with the WingX Pro7 application.
They reported that all of them were vulnerable, allowing an attacker to manipulate information presented to the pilot.
They recommended a device should sign the data sent from the receiver to the app and vice versa.
They also recommend regularly updating the firmware, implementing EFB updates, and to following security-aware software development in order to enhance the security of such mobile cockpit information systems.
Even though the security of ADS-B is heavily researched, Lundberg et al. [30] have provided as the sole contribution to MCIS security.
However, technology and the demand for MCIS have drastically changed since that study.
Many new ADS-B transceivers and software have been developed.
Attackers have new tools and ideas as well.
Therefore, evaluating the attacks on MCIS against current technology is essential.
In comparison with Lundberg et al. [30], our work provides comprehensive qualitative and quantitative security feature testing of MCIS.
Last but not least, the present paper complements our research work and the results in [14], [31].
Table 1 compares this article's attacks and contributions against the relevant attacks presented in the literature.
IV. EXPERIMENT SETUP
In this section, we describe our approach for attacking MCIS. 
We performed the experiments in well-controlled lab environments using low power, placing the receivers and transmitters in close proximity, and employing signal attenuators.
A. ATTACK PLATFORM
We used Python programming language to generate the attack payloads.
Then a program called GNU radio companion (GRC) was used to produce the IQ values, subsequently transmitted on the air using transmission-enabled SDR.
Three transmission-enabled SDRs were used: HackRF, BladeRF, and PlutoSDR.
One type of device was sufficient for the attacks in this study.
However, we tested three of them to check the feasibility of attacks by heterogeneous devices.
To encode the position and altitude into the ADS-B 1090ES signal, we used Yusupov's example script [24].
Later we extended the software's service by writing the codes for other necessary data fields of the ADS-B 1090ES, such as flight information, velocity, and squawk.
Yusupov also provided a UAT978 long-message generator [32], and we used that script to experiment with UAT978 data encoding.
We slightly modified Larroque's Reed-Solomon codec to generate the FEC [33].
Later, by adding synchronization bits and proper serialization, we generated the final UAT978 attack payload.
We used GRC's CPFSK block to transmit the UAT978 signal over the air.
Our written software can send 1-to-N 1090ES and UAT978 messages by 1-to-N transmitters.
It is controlled by several arguments in a command-line interface or with a graphical user interface.
To generate a fake ADS-B radio signal, we generate N messages to a CSV file.
Then, we create the IQ file of those messages for the 1090ES or UAT978 signal.
We duplicate each message 5-10 times to ensure that the tested receiver caught each one.
In the end, we transmitted all the N messages very quickly to push the receiving software to its limits.
Figure 3 shows how a Python-generated attack payload reaches the MCIS through a radio link.
Below we present the ADS-B fields and other parameters that can be set in our software to send individual or multiple messages using 1090ES or UAT978 protocols.
icao24: set an ICAO address to the message.
squawk: set a squawk code to the message.
flightnum: set a flight number.
velocity: set airspeed of the aircraft.
lat: set GPS latitude coordinate.
lon: set GPS longitude coordinate.
alt: set GPS altitude.
gain: set the transmit gain.
modetx set the protocol 1090ES or UAT978.
devtx: set a specific transmitter device.
file set the paths of attack file.
ts: set a timestamp in milliseconds.
crc: set a CRC checksum.
For UAT978, this argument refers to the Reed-Solomon FEC.
multiprocessing: set the number of parallel transmitters to use at once.
B. MOBILE COCKPIT INFORMATION DEVICES.
We tested six mobile cockpit information devices from different manufacturers. 
Some of them had ADS-B transmission capability, while others were limited to receive only. 
Because ADS-B is not fully functional in many parts of the world, some devices did not support transmission. 
Table 2 shows the list of tested devices.
C. ELECTRONIC FLIGHT BAG APPLICATIONS
A variety of EFB applications support the devices listed in Table 2.
However, not all the applications were compatible with all the devices, because some devices use proprietary protocols to exchange data with applications. 
The most popular protocol is GDL-90 [31], which most applications, such as AvPlan, FLyQ, OzRunways, use. 
However, the GarminPilot application worked only with the Garmin GDL-52 device, while SensorBox worked with their developed Horizon application. 
Table 3 shows the list of tested EFB applications. 
We included the world-wide installation number for Android platform applications (reliably available) to get an idea of how many users could be affected by an application failure. 
Some applications were not available for a specific platform, device, or our region. 
We cross-marked if we could not test it on a platform. 
Tested applications per platform are checkmarked. 
Missing information was marked NA (not available). All the EFB applications did not support all the tests (see VI-D).
V. ATTACKS ON MOBILE COCKPIT INFORMATION SYSTEMS
We implemented RF-link-based attacks on the MCISs.
Being portable and lightweight, MCISs have limited computation power, memory capacity, and screen size.
Therefore, an ADS-B packet-level DoS attack would be a good choice to check their resilience under a cyberattack.
In this study, we primarily focused on DoS attacks on the MCISs.
DoS attacks disrupt the availability of services by clogging or shutting down service entities or networks.
The intention is to prevent legitimate users from accessing the service or to prevent legitimate data from reaching its destination.
This is accomplished by crashing the service with malicious data or by flooding its input with garbage data or fake messages beyond its capabilities.
Because ADS-B does not use authentication or encrypted wireless traffic, it is virtually impossible for it to block a malicious source of fake signals.
Therefore, identifying and properly handling the messages is the key to defending against these attacks.
The effects of DoS attacks on wireless traffic and wireless sensor networks have been the subject of extensive and prolific research.
Osanaiye et al. [34] and Ghildiyal et al. [35] concluded that DoS attacks could be detrimental to the operation of the system, and defending against them is not trivial.
Strohmeier et al. [20] addresses the security issues of ADS-B broadcasts, stating that the system is sensitive to RF attacks.
DoS attacks can lead to an unresponsive or disabled system, which can lead to poor decision-making within ATC or the malfunction of automated systems because of the authentic information.
Our attack system operated on a click-and-run approach, where we first generated random yet valid ADS-B messages and transmitted those messages via transmissionenabled SDRs in a rapid burst.
While attacking, we visually observed the effect of the attack and recorded the observations.
We noted if the software had any crashes, errors, malfunctions, or unresponsiveness.
If not, we noted if the output of the software was clogged enough to miss ADS-B messages.
We also tested coordinated attacks on the MCISs.
In these attacks, multiple attackers targeted a single aircraft (or ICAO24 address).
Multiple attackers continuously sent sporadic information about the targeted aircraft.
To a receiver, this seems like the targeted aircraft is erratically changing its location or other important flight-relevant information.
We showed that such attacks lead to logical vulnerabilities [14].
Finally, we conducted fuzz testing for the EFB applications.
This is an automated software testing method for finding implementation and input sanitization bugs using intentionally malformed or randomized inputs.
The ADS-B devices communicated to the mobile application following some protocols.
Among them, GDL-90 is the most popular.
By following this protocol but using malformed input, we conducted fuzz tests of the EFB applications [31].
VI. RESULTS AND EVALUATION
We identified many candidate EFB applications for various tests. 
After a few trial-and-error setups (e.g., successful installation and configuration with hardware), 17 applications were selected for RF-link-based attacks, and 15 applications were selected for fuzz tests. 
Some applications supported both types of tests, while some were limited to only one. 
In total, 21 distinct EFB applications were tested in this study. 
Furthermore, six MCIS devices were tested. Of them, four supported UAT978, while all six supported the 1090ES protocol.
In the receive mode, compared with other MCIS devices such as SkyEcho2 or Sentry, the echoUAT receives and processes both 1090ES and UAT978 messages at a considerably lower (≈100 × −140× less) number of messages per minute. 
Subsequently, it forwards a significantly smaller number of ADS-B messages to the decoding application. 
Therefore, we construe this as being the main reason that none of the tested mobile apps crashed during the DoS attack tests while using echoUAT hardware. 
SkyEcho2 and Sentry can receive up to 55k distinct ICAO24 addresses per minute, but echoUAT surprisingly has a hardware limitation that processes approximately 400 distinct ICAO24 addresses per minute. 
We are not sure about the core reasons for this functional discrepancy. 
The maximum transmission rates of messages per second for 1090ES and UAT978 were 6.2 and 1, respectively [36], [37]. 
However, we have not found the maximum or minimum receiving resolution of the ADS-B system. 
In our experiment, we found that SensorBox and Garmin GDL-52’s decoding capacity was approximately 10,000 and 30 distinct ICAO24 addresses, respectively. 
Because these two devices work with their proprietary application only, we could not find out whether the limitation was in the hardware or the software. 
During the test, we found the the ADL 180 device displayed approximately 75 aircraft at a time in both Android and iOS applications. 
A. DoS ATTACK RESULTS FOR UAT978
DoS attacks on UAT978 were tested on a number of hardware and software combinations:
• 4 MCIS devices
• 2 mobile operating systems
• 9 EFB applications
• 24 different setup combinations
Overall, our DoS attack affected 9 out of 24 tested configuration for UAT978. 
The configurations crashed, clogged, or were unresponsive. Some applications were not affected during the DoS attack. 
Instead they dropped a significant number of legitimate messages and displayed only a tiny portion of the transmitted signal. 
In practice, with the limited memory, computational power, and display capacity, it is nearly impossible for the MCISs to display and update the ADS-B data for a huge number of distinct aircraft flawlessly (e.g., attack payload of 200,000 ICAO24 address or more). 
espite the applications not crashing, we believe that clogging the system and disabling the capability of the system to show all required signals to the user was a successful DoS attack as it disrupted the availability of required data. 
We marked these situations as nonimpacted to distinguish the systems that showed even some resilience to the attacks from the ones that crashed consistently. 
Therefore, we believe that the non-impacted setups are also not adequate for safety and mission-critical systems. 
Table 4 presents a summary of the results of the attacks.
B. DoS ATTACK RESULTS FOR 1090ES
The attacks on ADS-B 1090ES were tested on a number of hardware and software combinations:
• 6 MCIS devices and 1 RTL-SDR
• 2 mobile operating system
• 15 EFB applications
• 44 total configuration combinations
We found that some EFBs worked with the RTL-SDR through SDR driver v.3.10 in the Android platform. 
Thus, we used RTL-SDR as the RF front-end for EFBs. 
Overall, out of 44 tested configurations for DoS attacks on ADS-B 1090ES, 28 were affected. 
Table 5 presents a summary of the results of the attacks. 
C. ADS-B OUT IMPACT
We also investigated the impact of DoS attacks on the performance of ADS-B OUT. 
Among the MCIS devices, SkyEcho2 transmits the 1090ES signals and echoUAT transmits the UAT978 signals. 
Table 6 shows the results of the ADS-B OUT impact experiment. 
In all the ADS-B OUT scenarios, we performed the attacks with a burst of 10k unique ICAO ADS-B messages. 
However, changing attack intensity numbers (i.e., increasing to bursts of 20k or 30k ICAO24 address) did not significantly change the impact. 
Each test was were carried out 15 times for each scenario. 
The results show that the DoS attack on ADS-B IN reduced the ADS-B OUT capacity of SkyEcho2 by approximately 15%, while no significant impact was observed on the echoUAT. 
However, it is still unclear whether the described impact on SkyEcho2 ADS-B OUT also had a qualitative impact. 
In other words, it remains for future work to investigate if the decline was due to some critical ADS-B OUT messages being dropped or being sent with unacceptable delay. 
For example, if some ADS-B OUT packets are delayed or dropped altogether, this could dramatically impact the effectiveness of the traffic collision avoidance system.
D. FUZZING
The communication between the MCIS devices and the EFB was mostly conducted via WiFi using the GDL-90 protocol by Garmin.
However, the MCIS devices used insecure WiFi connections through which malformed data can be passed to the application, and this may affect the integrity and security of the overall system.
We performed extensive fuzz-testing for the EFBs by using the American fuzzy lop (AFL) Python implementation.
AFL was set up to send malformed data to the IP address of the EFB application host.
Table 7 highlights our fuzz-testing results.
To compare the result with the DoS tests, we also show in Table 7 the corresponding ADS-B DoS test result on corresponding EFBs.
In the table, we marked as NA whenever we could not configure an EFB for the test (e.g., due to unavailability, or some other limitation).
In addition, some applications (e.g., EFB apps, desktop software) did not work with our MCIS devices.
However, they worked with GDL-90 and the fuzzing setup, which can also be seen in Table 7 in their corresponding rows.
The results show that 3 out of 7 or approximately 42% applications were affected by the fuzzing test on the Android platform.
On the iOS platform, the impact rate was around 53% for 7 affected EFB applications out of 13.
Some EFBs applications, such as AvPlan, were crashed by both tests.
Some EFB were crashed by one of the tests, while only EasyVFR4 and Pilot Atlas survived both tests.
One particular observation from Table 7 is as follows.
If the tested application is vulnerable to ADS-B DoS attacks (e.g., crash), it is extremely likely that it will be found vulnerable by GDL-90 fuzzing with very likely the same consequences (e.g., crash).
Examples include AirMate, AvPlan, OzRunways, and Stratus Insight.
Likewise, if an application did not present any major issues during ADS-B DoS attacks, it will very likely pass the GDL-90 fuzzing tests.
Although, exceptions to this rule are iFlightPlanner and Levil Aviation.
This shows strong efficiency and correlation of cybersecurity testing by ADS-B DoS and/or GDL-90 fuzzing.
This means that insufficiently secured applications (e.g., see Table 7) that result in serious consequences (e.g., software/EFB crash) will eventually be discovered with sufficient testing when using the methodology and the pentesting platform design that we propose in this paper and in our related works [14], [31].
E. LOGICAL VULNERABILITIES
For an aircraft, ADS-B traffic information in the MCIS updates with the reference of the ICAO24 address. 
If multiple sources of ADS-B signal containing the same ICAO24 address emit the position information from different places, it appears that aircraft is changing its position erratically.
Also, we found that none of the tested MCIS setups check the received data’s integrity. 
For example, many aircraft might have the same flight number or irrational altitude and speed relationship [14]. 
Such a situation may raise logical vulnerabilities for the MCIS user.
VII. DISCUSSION
We did not observe any hardware crashes.
However, this does not mean the devices we tested do not have potential bugs or security vulnerabilities at their hardware or firmware level.
In fact, firmware vulnerabilities are quite common in IoT and embedded devices [5], [6] and as Muench et al. [38] demonstrated, when memory is corrupted in embedded devices, the results are different from desktop systems.
Looking more into the future, we argue that the possibilities of the presented attacks may have impacts beyond the ground-, and aircraftbased ADS-B systems and well into the aerospace domain.
The emergence and deployment of satellite-based ADS-B surveillance and receivers [39]-[41] and the increase of ADS-B application in unmanned aerial vehicles (UAVs) [42], [43] could increase the attack sphere and severely amplify the potential impact of attacks [44].
To address the security vulnerabilities of MCISs demonstrated in this study, we present some solutions.
First, the hardware, firmware, and software should be rigorously and continuously tested through automated means such as our platform.
The testing should start from the development environment and extend to the operational environment, because development environments do not fully represent the proper use cases.
Kacem et al. [45] proposed a crypto and radio-location-based hybrid solution to thwart ADS-B attacks.
Their proposed framework called ADS-Bsec provides authenticity and integrity for ADS- B packets by using a keyed-hash message authentication code (HMAC).
The minimum size of an HMAC is 128 bits which need to be distributed among several ADS-B messages.
Although their proposed framework supports backward compatibility with the current ADS-B protocol; however, the CRC checks must be disabled.
Kassab [46] surveyed safety-critical software development and concluded that although safety-critical applications are tested more frequently, quality assurance testing is mostly performed in the very late stages of software development.
According to him, the software development practices must be of a higher standard.
Possible attack vectors must be identified during software development, and mitigation must be implemented.
The iterative development cycle between testing and mitigation implementation should be enforced.
For example, a subset of DO-178B (Software Considerations in Airborne Systems and Equipment Certification) could be developed and explicitly required for MCISs.
Furthermore, proper memory management must be implemented in the software.
The software that crashed, hung up, or went unresponsive does not have appropriate memory management implemented.
Therefore, it can be assumed that the EFBs were not tested against DoS attacks during the software development.
Researchers have proposed several defense strategies against attacks on ADS-B.
However, the effectiveness of the proposed attack detection and prevention methods are yet to be tested in academia and industry.
Nonetheless, some defense strategies are available.
Li and Wang [47] proposed a sequential collaborative attack detection strategy based on ADS-B data.
According to them, time series and position, the law of motion, historical data, etc., can be used to detect injection, DoS, replay, and ghost attacks.
However, the authors did not consider the physical or signal pattern of the attacks.
They solely trusted the data.
The position-related data of a commercial aircraft change a bit within 30 seconds.
However, our study shows that a successful DoS attack can be performed within this short time.
In contrast, it may take much more time to apply their proposed method to establish collaboration among the nodes such as ground stations and aircraft in the vicinity to detect the DoS attack.
Ying et al. [48] proposed a deep neural network (DNN)-based spoofing detector.
That method allows a ground station to examine each incoming message based on physical layer features such as IQ samples and phases to flag suspicious messages.
The classifier predicts the ICAO24 address of the received ADS-B message and compares it against the claimed ICAO24 address.
The rate of the change in the signal phase indicates the carrier frequency offset, which is a sum of frequency offsets and the Doppler shift.
They used this feature for classification purposes.
However, the main limitation of their method is the supervised learning method for a dynamic environment.
An unknown legitimate aircraft flying over the region can initiate a false alarm.
Moreover, radio propagation, receiver characteristics, and measurement noise also can affect the system.
Our attacking approach can generate any ICAO24 address, which can be regarded as an aircraft flying for the first time in the air space with no historical data, thus bypassing the security or generating a false alarm.
Jansen et al. [49] proposed a non-invasive trust evaluation system to detect attacks on ADS-B-based air-traffic surveillance.
They used a ''Wireless Witnessing'' method to detect the attacks, which is essentially sharing the observations of geographically distributed sensors.
An ADS-B receiving sensor should always receive the signals within its coverage.
During a spoofing or an injection attack, sensors may receive such ADS-B signals that the signal's encoded position information exceeds the sensor's range.
Multiple sensors' wireless witnessing would increase the probability of attack detection.
By collecting scores from all the sensors, they calculated a total that indicated and ADS-B attack.
Their proposed method is a postprocessing method.
It is not suitable for a real-time attack.
As our study has shown, an attack can be made within a few minutes.
A quick DoS attack may cause substantial negative consequences.
VIII. CONCLUSION
This work performed the largest and the most comprehensive cybersecurity assessment of DoS availability attacks on popular MCIS setups by modelling the attacker via remote unauthenticated and unauthorized RF-link. 
We developed a cybersecurity pentesting platform consisting of a large and comprehensive list of ADS-B transceivers, SDRs, and different EFB applications. 
Furthermore, we developed a flexible software suite that allows us to perform cybersecurity tests. 
We tested 44 1090ES and 24 UAT978 MCIS setups, for a total of 68 test configurations. 
Our ADS-B packet-level DoS attack affected availability on approximately 63% and 37% of 1090ES and UAT978 setups, respectively. 
The most concerning finding of this study was the very high number of MCISs and ADS-B software that crashed as a result of the performed attacks, where such crashes further expose the affected systems to potential ACE attacks. 
The test results show that many, if not most, popular MCISs are vulnerable to many types of cyberattacks, including attacks on availability with resulting software crashes. 
Relevant overseeing and regulatory bodies (such as FAA, EASA, and ICAO) should investigate these issues further, and propose practical steps and approaches to ensure further resilience of MCISs to cyberattacks.
ACKNOWLEDGMENT
The authors acknowledge the grants of computer capacity from the Finnish Grid and Cloud Infrastructure (persistent identifier urn:nbn:fi:research-infras-2016072533). 
Major parts of this research supported by cascade funding from the Engage Consortium’s Knowledge Transfer Network (KTN) project ‘‘Engage-204-Proof-of-concept: practical, flexible, affordable pentesting platform for ATM/avionics cybersecurity’’ (SESAR Joint Undertaking under the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 783287). 
All and any results, views, and opinions presented herein are only those of the authors and do not reflect the official position of the European Union (and its organizations and projects, including Horizon 2020 program and Engage KTN). 
The authors thank Dr. Andrei Costin for facilitating and managing a partially-supporting grant Decision of the Research Dean on research funding within the Faculty (07.04.2021) of the Faculty of Information Technology, University of Jyväskylä (JYU). 
Hannu Turtiainen also thanks the Finnish Cultural Foundation/Suomen Kulttuurirahasto (https://skr.fi/en) for supporting his Ph.D. dissertation work and research (under grant decision no. 00221059) and the Faculty of Information Technology, JYU, in particular, Prof. 
Timo Hämäläinen, for partly supporting and supervising his Ph.D. work at JYU in 2021–2022. (Syed Khandker and Hannu Turtiainen are cofirst authors.)
Digital Detox
Keywords Digital detox  Technostress  Knowledge work  Information technology
1 Introduction
The increasing use of information technology (IT) has a pervasive impact on society, including the world of work and its boundaries. 
Individual professionals, and knowledge workers in particular, are exposed to digital devices during the bulk of their working hours (Orlikowski and Scott 2016). 
In addition, persuasively designed social media and digital entertainment applications occupy the leisure time of an unprecedented number of people. 
A recent study revealed that 33.1 million Germans use the Internet ‘‘multiple times a day’’, and 11 million even state to use it ‘‘constantly, almost the whole time’’ (Statista 2020). 
Scholarship clearly suggests that this compounded screen time can entail severe consequences to the wellbeing of individuals (Pflu¨gner et al. 2020a). 
In fact, using IT can lead to technostress, which is defined as ‘‘any negative impact on attitudes, thoughts, behaviors, or body physiology that is caused either directly or indirectly by technology’’ (Weil and Rosen 1997, p. 5). 
Technostress constitutes a pressing social issue, especially with regards to changes in work-life boundaries, potentiated by the COVID-19 pandemic (Thomas et al. 2020). 
According to a study conducted in 2019, 86 percent of participants claimed that the inability to switch off devices after regular working hours has a negative effect on employee wellbeing (Stewart 2020). 
The result is a personal feeling of being overwhelmed by communication content and interpersonal online connections, which negatively affects work and private life alike (Gui and Bu¨chi 2019). 
To counteract technostress and its negative consequences on individual wellbeing and productivity, the notion of ‘‘digital detox’’ has found its way into popular culture and, more recently, Information Systems (IS) scholarship (Vaghefi et al. 2018; Eichner 2020; Zhou et al. 2020). 
Digital detox describes a periodic disconnection from IT as well as strategies which help to reduce the engagement with IT (Syvertsen and Enli 2019). 
Both its conceptualization and empirical analysis, however, have so far remained vague. 
Early research presents mixed results concerning the effectiveness of digital detox to improve individual wellbeing (Wilcockson et al. 2019; 
Brown and Kuss 2020; Schmuck 2020). 
Yet, making a statement about its effectiveness largely depends on the way digital detox is defined in each individual study. 
Despite this ambiguity, however, the literature commonly stresses the importance of remaining absent from IT for specified periods and calls for more research on this matter. 
The growing demand for digital detox before, during, and most likely after the COVID-19 pandemic fundamentally questions the way we use IT. 
Individuals increasingly find themselves yearning for time without the pervasive presence of IT (Fu et al. 2020). 
Digital detox, we argue, poses a symptom of a serious problem, that is, detrimental effects of IT use on health and work satisfaction. 
How can IS research help to get to the root of this problem?
Digital Detox
Digital detox describes a periodic disconnection from IT as well as strategies which help to reduce the engagement with IT (adapted from Syvertsen and Enli 2019)
In this article, we demonstrate the rationale behind digital detox and the developments in organizational knowledge work that precipitate the increasing popularity of periodically refraining from the use of IT. 
Moreover, we propose a first conceptualization of digital detox to guide future research in IS and beyond.
2 Technostress in Organizations
IT pervasively affects individuals’ private and professional life (Tarafdar et al. 2019). 
Work arrangements built around steady IT use have become commonplace, in particular for knowledge workers (Kissmer et al. 2018). 
The latter are defined as workers whose occupation relies on ‘‘the creation, distribution or application of knowledge’’ (Davenport 2005, p. 9) Typical work arrangements allow knowledge workers to connect with people in geographical proximity, across greater distances, or completely remote—independently of time and space (Frick and Marx 2021). 
The COVID-19 pandemic has impelled a bulk of the workforce to switch to remote work arrangements, and home office respectively (Brynjolfsson et al. 2020). 
Whereas this development may be expected to be only temporary, leading firms such as Microsoft or Siemens have announced to preserve the ratio of remote work arrangements compared to regular office work beyond the pandemic (Newman 2020). 
This permanent shift means an empowerment of the knowledge worker in terms of her mobility and autonomy, while dismissing the paradigm of the corporate 9 to 5 job applied to knowledge work (Wang et al. 2020). 
The flipside of the coin is that, with increasing IT use due to remote work arrangements, knowledge workers are exposed to a higher risk of technostress (Chandra et al. 2019). 
This phenomenon refers to stress individuals experience because of their IT use and their inability to cope with it healthily (Riedl et al. 2012; Mahapatra and Pillai 2018; Sarabadani et al. 2020). 
As employees often have to adapt to new and changing IT implemented by their organization, a number of scholars in IS focus on employee and IT professional related technostress (Chiu 2018; Mahapatra and Pillai 2018; Sarabadani et al. 2020). 
2.1 Theoretical Underpinnings of Technostress Literature
Theoretically, the technostress literature heavily builds on the transactional model of stress (Lazarus and Folkman 1984). 
According to this model, individuals react cognitively to stimuli by assessing the motivational significance of a situation (primary appraisal). 
This may result in the perception of a situation to be irrelevant, benign-positive, or stressful. 
Subsequently, according to the model, individuals evaluate the assessment by contemplating possible actions to manage the situation (secondary appraisal). 
For example, one tries to find ways of alleviating possible harm in case of a stressful situation. 
The stressors perceived in the situation, in turn, provoke a stress reaction that can be of physiological, emotional, cognitive, or behavioral nature (strain). 
Finally, the individual may suffer from consequences caused by stress. So far, IS literature has built on this model in a threefold manner. 
First, the transactional model has been tailored to technostress in organizations, defining the dimensions job characteristics, technological environment, organizational environment, and social environment, in which stressors can occur. 
Moreover, consequences of technostress do not only affect individual wellbeing, but may also impair performance, productivity, and IT user satisfaction (Adam et al. 2017). 
Second, specific techno-stressors have been defined, as shown in Table 1. 
Third, technostress research has explored so-called coping strategies to mitigate technostress. 
With reference to the transactional perspective, these strategies can be problem-focused, e.g., stress-sensitive systems that provide live bio-feedback (Adam et al. 2017), or emotion-focused, e.g., mindfulness exercises (Pflu¨gner et al. 2021). 
What the technostress literature has in common is that proposed coping strategies set in only after the second appraisal, i.e., after a stressor has been experienced and assessed as such. 
This constraint opens a new theoretical angle to approach technostress.
2.2 A Digital Detox Perspective on Technostress
Research on technostress in organizations is guided by the assumption that individual exposure to techno-stressors is determined by job characteristics or technological, organizational, and social environments. 
Intervention through digital detox, we argue, can also start earlier, i.e., prior to the individual being exposed to the stimuli and technostressors, respectively. Figure 1 summarizes this supposed theoretical relationship. 
If understood as a periodic disconnection and strategies to reduce the engagement with IT, digital detox has been investigated sporadically in technostress literature in the form of coping strategies. 
Under the label of ‘behavioral disengagement’, Hauk et al. (2019) describe the phenomenon of an individual ‘‘breaking off any further interaction and withdrawing from the stressful situation’’ (p. 22).
Interestingly, the authors found this behavior to be counter-productive, with stress levels reducing in the shortterm, but then yoyoing back once the individual returns to the unresolved situation. 
Another study examined coping strategies which specifically answered to techno-invasion and techno-overload in organizations. 
Here, the authors proposed communication measures for employees and management that might be able to reduce the exposure to IT and its demands (Pflu¨gner et al. 2020b). 
Strategies specifically aiming at the reduction of stimuli and successive stressors in the context of technostress in organizations, however, have not been part of the debate. 
Instead of changing the independent variable (IT exposure), research has focused on finding appropriate moderating forces (coping strategies) that alter the stress reactions and consequences. 
Digital detox, in this sense, offers an additional perspective (prevention strategies) that alleviates the predetermined experience of technostress when performing knowledge work.
3 The Concept of Digital Detox
Recently, the notion of digital detox has received increasing attention in academia, popular culture, and the self-help industry. 
The term ‘detox’ itself describes ‘‘a process or period of time in which one abstains from or rids the body of toxic or unhealthy substances’’ (Oxford Languages 2021). 
In medicine, the scientific grounding for detoxification is controversial – it can rather be seen as a consumer buzz word in conjunction with healthcare products (Cohen 2007). 
In the digital context, however, the effectiveness of detox measures is currently being scrutinized. 
Extant literature on digital detox goes back to around 2015 (Ugur and Koc 2015) and is dispersed across disciplines such as Psychology (Schmuck 2020), Media and Communication Studies (Syvertsen and Enli 2019), and IS (Mirbabaie et al. 2020).
3.1 Digital Detox Research and Strategies
So far, empirical studies report mixed results concerning the effectiveness of digital detox (Wilcockson et al. 2019; Brown and Kuss 2020; Schmuck 2020), while also stating the importance of more research on this matter. 
The ambiguity of empirical findings, however, is partly due to an inconsistent and often vague conceptualization of ‘digital detox’. Moreover, viewed through a technostress lens, the phenomenon has been researched under the assumption of it being merely a coping strategy. 
This is reflected in recent definitions of digital detox, describing a process in which an individual abstains from objects that are perceived as unhealthy once exposure to them surpasses a certain point (Syvertsen and Enli 2019). 
Other terms like ‘‘digital diet’’ or ‘‘media diet’’ (Andersen et al. 2016) revolve around the same phenomenon as digital detox, which complicates consensus building. 
To establish the basis for a sound conceptualization of digital detox, Table 2 provides an overview of digital detox strategies that are prevalent in the literature. 
For the purpose of this article, we refer to digital detox as an integrated approach to temporarily refrain from IT use to improve overall well-being and mental health. 
In doing so, we want to emphasize the preventive element of digital detox in addition to the coping element, as proposed by the transactional model of technostress (Adam et al. 2017). 
Henceforth, this catchword paper aims to shed light on the different strategies of digital detox and to establish the concept as a proper subsumption to the technostress literature.
3.2 Toward a Conceptualization of Digital Detox in Organizational Contexts
In the following, we aim to provide a first conceptualization of digital detox in organizations.
As a first step, we take the identified digital detox strategies as shown in Table 1, and abstract three theoretical dimensions.
First, we propose that the 'length of the interval' should be subject of scrutiny when researching digital detox.
Second, we derive from existing strategies that they differ in the 'extent of intervention', that is in considering how much a digital detox strategy comes into conflict with organizational processes, norms, and behavior.
Third, digital detox can have different 'levels of IT-assistance', e.g., through mindfulness apps, disabled e-mail servers after working hours, or calendar reminders to practice digital detox.
Figure 2 provides three examples that range differently across the three dimensions of digital detox.
In this example, IT-free lunch breaks are minimal in length, intervention, and need for IT assistance.
No-distraction appointments, on the other hand, can be medium in length and high in intervention and IT assistance.
A mindfulness exercise may be chosen with short to medium length, no means of IT assistance and no interventions of organizational processes, norms, and behavior.
Below, we combine these dimensions with the transactional perspective known from the (organizational) technostress literature.
Here, we consider both the motivational component of digital detox as well as the characteristics of a given digital detox strategy.
Figure 3 depicts this integrated concept of digital detox.
This concept is useful for future research on digital detox for two reasons.
First, digital detox proves to be a valuable concept to technostress and adjacent literature as it expands the theoretical chain of causation prevalent in current literature (stressors-strain-consequences followed by coping strategies to stressors-strain-consequences preceded by prevention strategies).
Second, it allows for future research to determine a grading of digital detox strategies.
This means that each measure (preventive or coping in nature) can be assessed and compared along the three dimensions.
4 Research Agenda and Summary.
In conclusion, the interdependencies of knowledge work arrangements and technostress make a strong case for more research exploring the phenomenon of digital detox. 
Moreover, additional theorizing is necessary to understand, explain, and predict behavior related to digital detox. 
In this regard, it is imperative for IS research to examine how digital detox strategies can prevent technostress and what value this perspective adds to existing coping strategies (see Fig. 1). 
Possible research questions are:
What are individual motivators for knowledge workers to conduct digital detox?  
How do individual digital detox strategies differ when motivated by prevention as opposed to coping? 
Which techno-stressors can be mitigated by preventive digital detox strategies?
Methodical approaches to these types of questions can vary, however, qualitative-interpretivist inquiries will help to obtain a phenomenological angle to digital detox. 
It is crucial to understand how individuals experience digital detox, what motivates them, and how it changes their work processes and technology use. 
In addition, subsequent questions emerge when shifting to an organizational view on digital detox. 
As proposed by the transactional model of technostress, factors such as job characteristics, technological environment, organizational environment, and the social environment impact the exposure to techno-stressors (Adam et al. 2017). 
This line of argumentation yields further research questions:
How do job characteristics affect the conduct of digital detox? 
How is the technological environment (number and types of devices, software, etc.) interrelated with digital detox? 
To what extent is management responsible for ensuring an organizational environment that appreciates digital detox? 
What are the implications of digital detox for the social environment inside organizations, e.g., in times of social isolation?
Here, hypothetico-deductive methods using quantitative data will help to test the theoretical relationships of digital detox, e.g., building on the transactional model of technostress.
In existing studies, due to conceptual ambiguity, the effectiveness of digital detox has been assessed with mixed results.
Therefore, it will be important to clearly define what digital detox strategies are and how their impact can be measured.
We call for research that systematically creates a taxonomy for digital detox strategies, e.g., by considering the proposed dimensions 'length of the interval', 'extent of intervention', and 'levels of IT-assistance'.
Possible research questions in this regard are:.
What are digital detox strategies and how do they differ (e.g., from shortest and least interfering to longest and most interfering)? 
What are the implications of digital detox for IT design and the management of IT? 
Finding answers to these questions will allow organizations, individual professionals, and IT designers to find a common ground that considers all interests at stake, that is, wellbeing, productivity, and user satisfaction.
In this catchword article, we proposed a first conceptualization of digital detox.
We argue for digital detox as a phenomenon worthwhile to be examined as it adds a rather neglected perspective to technostress research.
Instead of approaching technostress solely with coping mechanisms, digital detox offers a preventive and strategic element to technostress avoidance on both individual and organizational levels.
We further stress the point that digital detox need not exclude IT dogmatically but should deploy it intelligently to support a user or organization to conscientiously implement digital detox strategies in their day-today operations.
This will not only allow them to cope with technostress once a certain threshold is surpassed but helps to preventively regain equanimity and balance with regards to IT use.
As Henry David Thoreau, the poet and philosopher we referenced in the opening quote to this article, would most certainly agree, digital detox may help us to elevate our lives by more conscious endeavors with and without IT.
Funding Open Access funding enabled and organized by Projekt DEAL.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.
The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.
If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.
To view a copy of this licence, visit http://creativecommons.
org/licenses/by/4.0/.