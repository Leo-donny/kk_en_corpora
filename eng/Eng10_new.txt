International Classification of Diseases, Tenth Revision, Clinical Modification social determinants of health codes are poorly used in electronic health records
Abstract
There have been increasing calls for clinicians to document social determinants of health (SDOH) in electronic health records (EHRs). 
One potential source of SDOH in the EHRs is in the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD10-CM) Z codes (Z55–Z65). 
In February 2018, ICD-10-CM Official Guidelines for Coding and Reporting approved that all clinicians, not just the physicians, involved in the care of a patient can document SDOH using these Z codes. 
To examine the utilization rate of the ICD-10-CM Z codes using data from a large network of EHRs. 
We conducted a retrospective analysis of EHR data between 2015 to 2018 in the OneFlorida Clinical Research Consortium, 1 of the 13 Clinical Data Research Networks funded by Patient-Centered Outcomes Research Institute. 
We calculated the Z code utilization rate at both the encounter and patient levels. 
We found a low rate of utilization for these Z codes (270.61 per 100,000 at the encounter level and 2.03% at the patient level). 
We also found that the rate of utilization for these Z codes increased (from 255.62 to 292.79 per 100,000) since the official approval of Z code reporting from all clinicians by the American Hospital Association Coding Clinic and ICD-10-CM Official Guidelines for Coding and Reporting became effective in February 2018. 
The SDOH Z codes are rarely used by clinicians. 
Providing clear guidelines and incentives for documenting the Z codes can promote their use in EHRs. 
Improvements in the EHR systems are probably needed to better document SDOH.
Abbreviations: ADI = area deprivation index, AHA = American Hospital Association, EHRs = electronic health records, ICD-10-C = International Classification of Diseases, Tenth Revision, Clinical Modification, PCORI = patient-centered outcomes research institute, SDOH = social determinants of health, US = the United States.
Keywords: electronic health records, International Classification of Diseases, tenth revision, clinical modification, social determinants of health
1. Introduction
In the past decade, there has been an increasing recognition of the powerful role of social determinants of health (SDOH) in shaping people’s health across a broad variety of health outcomes.[1] 
The World Health Organization defines SDOH as the conditions in which people are born, grow, live, work, and age.[2] 
These factors include social circumstances and environmental exposure such as education, employment, food, housing, social support, and psychosocial factors. 
There is a growing body of evidence demonstrating the significant impact of SDOH, such as education and employment, on a wide range of health outcomes.[1] 
It has been estimated that that SDOH could be responsible for up to 40 percent of all preventable deaths in the United States (US), whereas better medical care is responsible for a much smaller proportion, 10–15 percent, preventable deaths in the US.[3–5] 
All the evidence suggests that efforts to improve health need to look beyond the healthcare system as the key driver of health, and start to address the social and environmental factors that influence health outcomes. 
Given the strong evidence that SDOH impacts health, there have been increasing calls for clinicians to document and attend to these factors.[6] 
In 2014, the Institute of Medicine (IOM) of the U.S. 
National Academy of Sciences recommended that 10 social and behavioral domains be documented in electronic health records (EHRs).[7,8] 
These factors included race/ethnicity, education, financial resource strain, stress, depression, physical activity, nicotine use/exposure, alcohol use, social connections/ social isolation, exposure to violence, and neighborhood characteristics (e.g., census-tract median income).
[7] Since then, healthcare systems have explored ways to capture data on SDOH and integrate them with patients’ EHRs.[9,10] 
For instance, a set of EHR-based SDOH data collection tools have been developed and tested in several community health centers.[11] 
Beyond the efforts of creating new SDOH collection and integration tools, 1 potential source of SDOH data already exists in EHRs. 
There is a specific subset of the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) codes, the Z codes (Z55-Z65), that are intended to document patient’s’ SDOH related to their socioeconomic, occupational, and psychosocial circumstances. 
To promote the use of these Z codes, the American Hospital Association (AHA) Coding Clinic published advice in February 2018 that allows all clinicians (eg, nurses), not just the physicians, involved in the care of a patient to document SDOH using these Z codes.[12] 
In the same month (February 2018), this advice was officially approved by the ICD-10-CM Cooperating Parties and incorporated into the ICD-10-CM Official Guidelines for Coding and Reporting.[13] 
The goal of the current study was to examine the utilization of the ICD-10-CM Z codes between 2015 to 2019 using data from a large collection of EHRs in the OneFlorida Clinical Research Consortium,[14] 1 of the 13 Clinical Data Research Networks funded by Patient-Centered Outcomes Research Institute (PCORI). 
As a network, OneFlorida provides care for more than 50% of Floridians through 4,100 physicians, 914 clinical practices, and 22 hospitals covering all 67 Florida counties. 
In this study, we reported the utilization rates of the SDOH Z codes at both the encounter and patient levels, and tested whether the rates differed by period (before and after the approval of the AHA advice for documenting Z codes), age group, sex, race-ethnicity, encounter type, and payer type. 
We further compared the rates of selected Z codes to the rates of their corresponding social problems reported in the population using US census data. 
To our knowledge, there exists no studies that have examined the utilization of the ICD-10-CM Z codes using data from a large network of EHRs. 
One prior study has reported a low utilization of the ICD-9-CM V codes, the predecessor of the ICD-10-CM Z codes, based on inpatient discharge data from 2013.[15] 
Our study fills important knowledge gaps beyond the scope of that study because:
(1) the Z codes were expanded to cover more SDOH aspects than the V codes, 
(2) the prior study only included inpatient data whereas our study included more encounter types, and 
(3) our study included multiple years of EHR data, encompassing February 2018, the month when Z code reporting by all clinicians was officially approved.
2. Methods
2.1. Data source
This study was approved by the University of Florida Institutional Review Board. 
We obtained EHR data between October 1, 2015, the date of ICD-10 implementation, and October 31, 2019 (study period) from OneFlorida.[16] 
OneFlorida contributing to the national Patient-Centered Clinical Research Network funded by PCORI. 
As the largest health data repository in Florida, the scale of OneFlorida data is evergrowing with a collection of longitudinal and robust patient-level records of ∼15 million Floridians and over 463 million encounters, 917.6 million diagnoses, 1 billion prescribing records, and 1.17 billion procedures as of December 2018. 
OneFlorida follows the national Patient-Centered Clinical Research Network Common Data Model, including patient demographics, enrollment status, vital signs, conditions, encounters, diagnoses, procedures, medications, and lab results. 
We have also obtained data indicative of SDOH at the zip code or census tract level, including the Area Deprivation Index (ADI),[17] and education attainment, employment, and poverty data from the US Census Bureau’s American Community Survey.[18] 
The ADI is an area-level metric that describes neighborhood disadvantages in income, education, employment, housing quality, and other socioeconomic variables. 
It allows rankings of neighborhoods by socioeconomic status disadvantage, and can be used to inform health care delivery and policy. 
A higher ADI score indicates greater risks of deprivation, higher vulnerability, or SDOH problems.
2.2. ICD-10-CM Z Codes for SDOH
In the ICD-10-CM, the Z codes for SDOH are grouped into 9 categories: 
Z55 (Problems related to education and literacy), 
Z56 (Problems related to employment and unemployment), 
Z57 (Occupational exposure to risk factors), 
Z59 (Problems related to housing and economic circumstances), Z60 (Problems related to social environment), 
Z62 (Problems related to upbringing), Z63 (Other problems related to primary support group, including family circumstances), 
Z64 (Problems related to certain psychosocial circumstances), 
and Z65 (Problems related to other psychosocial circumstances). 
We summarized these codes and the descriptions of the corresponding risk factors in Table 1.
2.3. Statistical Analysis
We examined the Z code utilization at both the encounter and the patient level. 
First, at the encounter level, we identified all encounters in the OneFlorida EHR data during the study period (October 1, 2015 – October 31, 2019). 
The ICD-10-CM Z code utilization rate was defined as the number of encounters with an SDOH Z code per 10,000 encounters. 
We calculated the utilization rate overall as well as stratifying by age group, sex, race/ethnicity, encounter type, payer, and site type, respectively. 
Sites in OneFlorida were grouped into academic and nonacademic. 
Differences in the rates across categories in the stratifying variables were tested using chi-squared tests. 
Second, at the patient level, we calculated the percentage of unique patients in the OneFlorida EHRs who had any of the Z codes or each of the Z codes overall as well as stratifying by age group, sex, race/ethnicity, and site type. 
Differences in the percentages across categories in the stratifying variables were tested using chisquared tests. 
Lastly, to explore whether the use of Z codes in the EHRs was reflective of greater social problems measured in the neighborhoods (zip code), we (1) examined whether having the Z codes was associated with high ADI using logistic regression, and (2) compared the rates of Z code to the rates of corresponding social problems reported in the US census. 
In the logistic models, the dependent variable was ADI, and the independent variables included the presence of a Z code, age, gender, race/ethnicity, and number of visits. 
We used the 90th percentile as the cutoff point for ADI to define patients who had SDOH problems.[19] 
For social problems reported in the US census, we obtained data on education attainment (rates of 5th grade or less education), employment (unemployment rate), and poverty (poverty rate).
3. Results
We summarized the utilization rates (per 10,000 encounters) for the SDOH Z codes in Table 2.
In the over 710 million encounters identified, the overall Z codes utilization rate was 270.61 per 10,000 encounters. 
The most commonly used category of Z codes was Z59, problems related to housing and economic circumstances, for which the utilization rate was 265.28 per 10,000 encounters. 
The utilization rates ranged from 0.07 to 1.24 per 10,000 encounters for the other categories of SDOH Z codes. 
Since the reporting guideline was changed in February 2018, the overall SDOH Z codes utilization rate increased from 255.62 to 292.79 per 10,000 encounters (P<.001). 
On the other hand, the increase in utilization was not consistent across the code categories. 
The utilization rates increased for Z55 (P<.001), Z59 (P<.001), Z60 (P<.001), Z62 (P<.001), and Z63 (P<.001), but decreased for Z56 (P<.001) and Z64 (P<.001). 
Across the age groups, the utilization rate for the SDOH Z codes was the highest among adults aged 65 years or older (933.12 per 10,000 encounters). 
This rate was significantly higher than that among adults aged 18–64 (139.11 per 10,000 encounters; P<.001) and children (11.35 per 10,000 encounters; P<.001). 
Among adults, the most commonly used Z code category was Z59, with the rate being 931.48 and 135.40 per 10,000 encounters for adults aged 18 to 64 and adults aged 65 years or older, respectively. 
Among children, the most commonly used Z code categories were Z55, problems related to education and literacy (3.81 per 10,000 encounters), and Z62, problems related to upbringing (3.24 per 10,000 encounters). 
Further, the SDOH Z codes utilization rate was significantly higher among women compared to men (313.23 vs 210.27 per 10,000 encounters; P<.001). 
Across the race-ethnic groups, the SDOH Z codes utilization rate was the highest among Hispanics (338.77 per 10,000 encounters), followed by non-Hispanic whites (265.14 per 10,000 encounters). 
The rate was significantly lower among non-Hispanic blacks (195.17 per 10,000 encounters) and other races (87.48 per 10,000 encounters). 
Across the encounter types, the utilization rate for the SDOH Z codes was 12.60, 6.62, 7.17, and 1469.99 per 10,000 encounters for ED, inpatient, outpatient, and other type visits, respectively.
All the Z code categories were used significantly more often during outpatient visits compared to ED and inpatients visits. 
For all the encounter types, Z59 was the most commonly used Z code category. 
Across the payers, the utilization rate for the SDOH Z codes was the highest for Medicare (1259.96 per 10,000 encounters) and the lowest for private payers (3.87 per 10,000 encounters) and other public payers (3.64 per 10,000 encounters). 
Z59 was the most commonly used Z code category across all the payer types. 
In addition, the overall utilization rate for the SDOH Z codes was significantly higher in non-academic health centers (317.57 per 10,000 encounters) compared with academic health centers (8.18 per 10,000 encounters). 
At the patient level, a total of 8,789,207 unique patients were identified in the OneFlorida data during the study period. 
We summarized the number of patients who had records on any of the 9 categories of Z codes in Table 3. 
Overall, 2.03% of the patients had at least 1 Z code reported. 
The most common Z code was Z59, with 0.89% of the patients reporting problems related to housing and economic circumstances. 
The utilization rates ranged from 0.03% to 0.32% for the other categories of SDOH Z codes. 
Across the age groups, the utilization rate for the SDOH Z codes was the highest among adults aged 65 years or older (3.27%). 
This rate was significantly higher than that among adults aged 18 to 64 (1.89%; P<.001) and children (1.82%; P<.001). 
Among adults, the most commonly used Z code category was Z59, 1.06% for adults aged 18 to 64, and 2.98% for adults aged 65 years or older, respectively. 
Among children, the most commonly used Z code categories were Z55, problems related to education and literacy (0.71%). 
Different from findings at encounter level, the SDOH Z codes utilization rate was higher among men compared to women (2.20% vs 1.94%; P<.001). 
Across the race-ethnic groups, the SDOH Z codes utilization rate was the highest among non-Hispanic whites (2.40%), followed by non-Hispanic Black (2.20%). 
The rate was significantly lower among Hispanics (1.64%) and other races (0.73%). 
In addition, the overall utilization rate for the SDOH Z codes was higher nonacademic health centers (2.23%) compared with academic health centers (1.22%). 
We summarized results from the logistic regression in Table 4. 
Compared to those with no Z code, patients who had any Z code were more likely to have a high ADI (OR=1.65; 95% CI: 1.62– 1.68). 
For the association between each of individual Z code and ADI, patients with the Z code were more likely to have a high ADI compared to patients without the Z code, except for Z55, problems related to education and literacy (OR=0.90; 95% CI: 0.85–0.95), and Z57, occupational exposure to risk factors, (OR=0.98; 95% CI: 0.82–1.18).
We summarized the rates of selected Z codes and their corresponding social problems reported in the US Census Bureau’s 2017 American Community Survey in Table 5. 
According to the US Census, an estimated 1.9% of the adults had 5th grade or less education in Florida. 
In contrast, a mere 0.31% of the adult patients in the OneFlorida network received a code of Z55, problems related to education and literacy, between 2015–2019. 
A similar under-reporting of Z codes for employment and poverty data. 
The estimated unemployment rate in 2017 was 7.2% in Florida, whereas Z56, problems related to employment and unemployment, was only recorded for 0.10% of the adults in OneFlorida between 2015 to 2019. 
Further, it was estimated that 13% of the adults in Florida had an income below the federal poverty level. 
However, Z59, problems related to housing and economic circumstances, was only recorded for 0.89% of the adults in OneFlorida between 2015–2019. 
The percentage of the adults in OneFlorida with the Z codes was consistently lower than the rates of corresponding social problems reported in the US Census Bureau across the gender and race subgroups.
4. Discussion
In this study, we examined the utilization of the ICD-10-CM Z codes in the EHRs from a large PCORI-funded clinical data research networks. 
Although the Z codes have existed for a few years now, we found a low rate of utilization for these codes that could help document the social and environmental factors in the EHRs. 
We also found that the rate of utilization for these Z codes increased since the official approval of Z code reporting from all clinicians, not just the physicians, involved in the care of a patient by the AHA Coding Clinic and ICD-10-CM Official Guidelines for Coding and Reporting became effective in February 2018. 
Our results from the regression models show that the presence of the Z codes is associated with a high ADI, except for Z55 problems related to education and literacy and Z57 occupational exposure to risk factors. 
First, the non-significant relationship between Z57 and the ADI is expected since the ADI does not consider variables related to occupational exposure to risk factors. 
Further, many risk factors for Z55 are specific for children, such as underachievement in a school, educational maladjustment and discord with teachers and classmates, which might have led to the reversed ADI-Z55 relationship. 
Second, the significant relationships between the Z codes (other than Z57) and the ADI suggest that the presence of the Z codes is reflective of the variations in social problems in the population. 
Neighborhoods (zip codes) that are of high deprivation or high social vulnerability have higher rates of patients reporting the SDOH Z codes. 
On the other hand, although the Z codes are reflective of the variations in social problems in the population, they are severely underutilized considering the published rates of certain social problems. 
The rates of the selected Z codes were significantly and consistently lower than the rates of corresponding social problems (education, unemployment, and poverty) reported by the US Census Bureau. 
One reason for the underutilization of the Z codes in the EHRs is that clinicians are simply not screening for social problems in the clinical setting. 
Screening for health-related social problems is fundamentally different from screening for traditional medical problems, for which many screening and diagnostic tools are available. 
While clinicians are aware of the importance of SDOH on health, most of them have inadequate training on how to respectfully extract information related the sensitive SDOH issues, such as housing insecurity and unemployment, from their patients and how to respond to patients’ concerns. 
Further, screening for SDOH can detect adverse social circumstances that require resources beyond the scope of clinical care. 
Resources for resolving the social needs are often scarce, and clinicians do not always know the available referral resources for the detected needs. 
Garg et al. warned about the unintended consequences of screening for SDOH in clinical care, especially when referral resources are unavailable for addressing the identified social needs.[20] 
As a result, clinicians are often uncomfortable inquiring about patients’ social problems. 
Another reason for the underutilization of the Z codes in the EHRs is that, in some cases, clinicians do document SDOH in routine care, but they do so in clinical notes more often than using the Z codes. 
In a recent study, Navathe et al evaluated the prevalence of 7 social factors using both clinical notes and structured EHR data and found that all 7 factors were identified at significantly higher rates in clinical notes.[21] 
For example, the prevalence of poor social support increased from 0.4% using ICD codes and structured EHR data to 16.0% using clinical notes. 
This observed disconnect may be because clinicians often times do not perceive these social problems as directly affecting clinical care, and assigning an appropriate ICD code to the identified social problem requires additional effort with no incentives. 
Nonetheless, advanced methods such as natural language processing (NLP) are increasingly used to identify SDOH in clinical notes. 
To promote Z code use and better document SDOH in EHRs, providing clear coding guidelines can be a useful first step as we show that Z code use has increased since the AHA recommendation. 
However, solely relying on these encounter-level Z codes for SDOH documentation may not be the best strategy for several reasons. 
First, the Z codes only cover a subset of all SDOH factors. 
Second, ICD code use in EHRs is often driven by billing needs, limiting the use of EHRs for other purposes. 
Third, documenting SDOH at the encounter-level can be impractical due to clinical workflow constraints such as limited visit time. 
Improvements in EHR systems (e.g., allowing documenting some SDOH in patients’ social history) are needed to support a holistic and systematic approach for tracking SDOH.
4.1. Limitations
Our study has a few limitations. 
When analyzing the association between Z code utilization and ADI, we correlated patients’ individual level data with the neighborhood level measurement and made broad assumptions on ADI over time and space. 
For instance, the particular ADI measurement used in our analysis was constructed based on American Community Survey (ACS) 5 year estimates in 2011 to 2015. 
Also, due to the lack of residential mobility data, we used a single zip-code over all encounters, which may have introduced non-differential misclassification.
5. Conclusions
Although there is an increasing recognition of the importance of SDOH and calls for clinicians to document and attend to these factors in the EHRs, the SDOH Z codes are rarely used by clinicians. 
Providing clear guidelines and incentives for documenting the Z codes can promote their use in EHRs. 
Improvements in the EHR systems are probably needed to better document SDOH.
References
[1] Braveman P, Gottlieb L. The social determinants of health: it’s time to consider the causes of the causes. Public Health Rep 2014;129(Suppl 2):19–31.
[2] Social determinants of health. World Health Organization. https://www. who.int/health-topics/social-determinants-of-health. Published 2017. Accessed November 28, 2020. 
[3] McGinnis JM, Williams-Russo P, Knickman JR. The case for more active policy attention to health promotion. Health Aff (Millwood) 2002;21:78–93.
[4] Blackman PH. Actual causes of death in the United States. JAMA 1994;271:659–61. 
[5] Danaei G, Ding EL, Mozaffarian D, et al. The preventable causes of death in the United States: Comparative risk assessment of dietary, lifestyle, and metabolic risk factors. PLoS Med 2009;6:e1000058. 
[6] Institute of Medicine. Primary Care and Public Health: Exploring Integration to Improve Population Health. Washington, DC: National Academies Press (US); March 28, 2012. https://doi.org/10.17226/13381. Accessed November 28, 2020. 
[7] Institute of Medicine. Capturing Social and Behavioral Domains and Measures in Electronic Health Records: Phase 2. Washington, DC: National Academies Press (US); January 8, 2015. https://doi.org/ 10.17226/18951. Accessed November 28, 2020. 
[8] Adler NE, Stead WW. Patients in context–EHR capture of social and behavioral determinants of health. N Engl J Med 2015;372:698–701. 
[9] Bazemore AW, Cottrell EK, Gold R, et al. “Community vital signs”: incorporating geocoded social determinants into electronic records to promote patient and population health. J Am Med Inform Assoc 2016;23:407–12. 
[10] Gold R, Cottrell E, Bunce A, et al. Developing Electronic Health Record (EHR) strategies related to health center patients’ social determinants of health. J Am Board Fam Med 2017;30:428–47. 
[11] Friedman NL, Banegas MP. Toward addressing social determinants of health: a health care system strategy. Perm J 2018;22:18–095. 
[12] Resource on ICD-10-CM Coding for Social Determinants of Health j AHA. American Hospital Association. Available at: https://www.aha. org/dataset/2018-04-10-resource-icd-10-cm-coding-social-determi nants-health. Accessed March 30, 2020. 
[13] Centers for Medicare and Medicaid Services (CMS), National Center for Health Statistics (NCHS) USD of H and HS (DHHS). ICD-10-CM: Official Guidelines for Coding and Reporting - FY 2019 (October 1, 2018 - September 30, 2019).; 2018. 
[14] OneFlorida – Clinical Research Consortium. Available at: https:// onefloridaconsortium.org/. Accessed March 30, 2020. 
[15] Torres JM, Lawlor J, Colvin JD, et al. ICD social codes: an underutilized resource for tracking social needs. Med Care 2017;559:810–6. 
[16] Shenkman E, Hurt M, Hogan W, et al. OneFlorida clinical research consortium: linking a clinical and translational science institute with a community-based distributive medical education model. Acad Med 2018;933:451–5. 
[17] Kind AJ, Jencks S, Brock J, et al. Neighborhood socioeconomic disadvantage and 30 day rehospitalizations: an analysis of medicare data. Ann Intern Med 2014;16111:765–74. 
[18] Bureau UC. American Community Survey (ACS). The United States Census Bureau. Available at: https://www.census.gov/programs-surveys/ acs. Accessed March 12, 2020. 
[19] Flanagan BE, Gregory EW, Hallisey EJ, et al. A social vulnerability index for disaster management. Journal of Homeland Security and Emergency Management 2011;8:3. 
[20] Garg A, Boynton-Jarrett R, Dworkin PH. Avoiding the Unintended Consequences of Screening for Social Determinants of Health. JAMA 2016;3168:813–4. 
[21] Navathe AS, Zhong F, Lei VJ, et al. Hospital readmission and social risk factors identified from physician notes. Health Serv Res 2018;532:1110–36.
A Topical Collection on ICT for Health Science Research – EFMI Special Topic Conference
Medical Informatics (or health informatics) is considered the science of applying the methods of computer science to health-care. 
In particular, medical informatics provides methodologies for systematic organization, representation, and analytics of data that is collected in health and well-being. 
About 50 years ago, the goal has been worded by Peter L. Reichertz: “the right information at the right place at the right time”. 
In this regard, the European Federation for Medical Informatics (EFMI), which is composed of national member societies, such as the German Association for Medical Informatics, Biometry and Epidemiology (GMDS), organizes annually a special topic conference (STC) that specialize in current trends in medical informatics. 
The 2019 edition of EFMI STC was focused on information and communication technology (ICT) for Health Science Research. 
A major challenge in this feld is the syntactical and semantical integration of ICT systems, since much of the data for health science research is coming from healthcare. 
Nevertheless, research often requires data of higher resolution, precision, and quality than is typically available in healthcare ICT systems. 
Thus, healthcare data are extracted, transformed, and loaded into research data warehouses, which leads to duplication of data and might challenge data integrity from specifc individuals across research and healthcare systems, possibly hindering personalized medicine and translational research. 
ICT systems for health science research are used in application domains such as clinical trials, development of drugs and medical devices, as well as translational medicine, aiming at better prevention, diagnostics, and interventions in health and care. 
In addition, ethical, legal, and social aspects of health data are considered. 
EFMI STC 2019 was held at the Peter L. Reichertz Institute for Medical Informatics of TU Braunschweig and Hannover Medical School in Hanover, Germany. 
It was jointly organized with the celebration of the 50th anniversary of the appointment of Peter L. 
Reichertz to the Hannover Medical School, which founded medical informatics as a research feld in Germany in 1969. 
From the total of 87 paper submissions, the scientifc program committee (SPC) – which was strictly diferent from the local organizing committee (LOC) – selected 48 oral and 22 poster presentations, which have been published in the conference proceedings [1]. 
However, the authors could decide to publish only an abstract within the proceedings and submit the extended paper to this special topic at JOMS. 
Seven of these submissions fnally have passed the strict peer-review process, which, due to the Conora pandemic, lasts almost two years. 
These papers focus on research and development of information systems supporting biomedical, translational, and clinical research, as well as interoperability across such systems for the purpose of data integration, improving fndability, and supporting analytics of cross-system data. 
They all have been submitted before the pandemic, but have been published during the pandemic. 
Therefore, we partly refect them in the light of COVID-19, too.
Pioneering medical informatics – is history still relevant?
“Professor Peter L. Reichertz is one of the most signifcant pioneers in the feld of medical informatics worldwide.” 
This quote is taken from Haux [2], the frst sentence of a paper, where the author tries to repute the work of Reichertz, who passed away already in 1987. 
Over the years, there is a clear shift from medical topics – as Reichertz was physician in internal medicine – towards medical informatics and computer science (except medical informatics) in his later work. 
The topics focus on (i) applications for diagnosis and decision support, (ii) hospital information systems, and (iii) information systems for outpatient care. 
The literature analysis is concluded by stating that Reichertz – unlike many others – did not only use ICT as if they were like microscopes or ultrasonic devices, but that he was “… visionary enough to very early see the revolutionary potential of informatics for all aspects of biomedicine and healthcare.” [2]. 
And what has been missing in the early phase of the pandemic? 
Information systems such as apps that track inter-human contacts [3], or semantically interoperable data sets, such as the German Corona Consensus Data Set (GECCO) [4], both according to Reichertz’s visions.
eHealth as game‑changer of health service research
Health services research has beneftted from the increasing availability of electronically available administrative data for a period of 50 years [5, 6]. 
Administrative data can be used to describe patterns of healthcare without eforts for data acquisition. 
Furthermore, a standardization within a healthcare system is guaranteed through nationally defned regulations. Van Laere et al. go a step further [7]. 
Their research started with electronically available data about prescriptions. 
Such data has been available for years in Germany, for example, based on a collection of paperbased prescriptions from pharmacies or downstream data centers. 
As many other countries, Belgium strives for a complete dematerialization of the prescription process (ePrescription). 
On the one hand, more information about healthcare becomes accessible, making analyses possible on a broader as well as a more detailed level. 
On the other hand, metadata about electronic services enable health services research to evaluate electronic processes. 
Van Laere et al. used metadata about the behavior of pharmacies to identify problems in the prescription process as well as errors in the underlying ICT infrastructure. 
Thereby, health services research got a double role in analyzing the patterns of ePrescriptions in Belgium, not only investigating the daily routine of electronic services in healthcare, but also evaluating the digitization itself. 
Interestingly, the latter was responsible for more than a third of the reasons for treating ePrescriptions as paper-based by Belgian community pharmacists. 
However, the new opportunity for health services research creates special challenges. 
For example, the case of interoperability must be extended to technical metadata [8] about electronic services in healthcare. 
Furthermore, the community behind health services research must be empowered to understand digitization issues, not only to understand healthcare.
In the light of the pandemic, pharmacists may become more active in vaccine administration, as – among others – European countries have already adopted their legal role [9]. 
Therefore, ICT for health science research remains a challenge for both perspectives, for enablers providing the technology and for researchers analyzing patterns that refect healthcare as well as technological aspects.
Patients as stakeholders in health science research
Traditionally, patients had the role of an observational unit in empirical health science research. 
This paradigm has changed. 
On the one hand, the patients’ perspective about effects of interventions got more attention in terms of patient-related outcome measures (PROMs) [10]. 
Measuring the efect of interventions on quality of life is one example. 
On the other hand, patients get involved in the planning, the design, and the management of research projects. 
With this regard, Rauter and colleagues [11] analyzed the perspective of German patient organizations (PO) towards their involvement as stakeholders in research projects. 
They identifed four diferent roles of POs: mediator, cooperator, fnancer, and independent. 
Moreover, Rauter and colleagues pointed out that “involvement” addresses diferent levels of detail, ranging from the passive consent to the role of a sponsor. 
However, patients as stakeholders do not necessarily need POs as intermediate. 
Patients themselves create research platforms and contribute actively and independently from academic institutions to health science research [12].
Consequently, the US-American Agency for Healthcare Research and Quality (AHRQ) called the academic community to move to patient-centered care and patient-centered research [13]. 
The AHRQ divided the involvement of patients in four categories: 
increasing focus on the patient, engaging patients as partners, digital health with patient reported data and automatic vital signs recordings, and direct-to-patient activities, e.g., by self-recruitment of patients. 
These trends create new challenges for health sciences. 
For instance, the management of ten or twenty study sites in a clinical trial may turn towards thousands of self-recruited patients who act as observational unit as well as study site. 
Monitoring and verifcation of PRO with query alerts fail if data recording is up to the patients in their daily life [14]. 
The work of Rauter and colleagues is an important contribution to understand the challenges of patient-centered research, even more, since PROM has been established as method and means after Covid-19 disease [15, 16].
Automatic recording of patient’s healt parameters
However, measurements on patients not only include vital signs. 
The World Health Organization (WHO) defnes environmental, behavioral, physiological, and psychological parameters to assess the quality of life [17, 18]. 
A crucial point is the automatic assessment of psychological parameters, which by nature can be captured only indirectly. 
Ganapathy et al. measure the electrodermal activity to estimate the emotional state of the subject [19]. 
They propose a multiscale deep convolutional neural network to score valence and arousal each into two groups of low and high level, according to Russell’s two-dimensional emotion space [20]. 
The number of layers and the signal length are the determinants for the classifer performance. 
The approach provides endto-end learning and classifcation of emotional states without additional signal processing. 
Such method can further be useful to assess the emotional states and their tendencies, in particular when applied to children sufering from pandemic threat, quarantine, and social distancing [21].
Focus on interoperability in clinical information systems
Standards play an important role in research, especially in the area of interoperability. 
But good standards do not only impress by a good specifcation and a high acceptance in the scientifc community, they also prove their importance and usability by practical implementation. 
One recent approach for achieving interoperability are Health Level Seven (HL7) Fast Healthcare Interoperability Resources (FHIR). 
The main goal of FHIR is to build a versatile platform that can implement common scenarios, but without getting lost in epic details. 
The extensible open-source imaging informatics software platform (XNAT), on the other hand, is a web-based software for archiving, managing, and sharing medical images and associated data in a research context. 
It is used for central research data repositories as well as for multicenter studies. 
XNAT can handle Digital Imaging and Communications in Medicine (DICOM) data and generally utilize the eXtensible Markup Language (XML). 
Khvastova et. al. address the question of enabling XNAT to work with FHIR data using the patient artifact as an example [22]. 
Patients are a central construct in many information systems of subject-oriented research, largely containing demographic and project administrative data. 
The authors developed an XNAT plugin that maps data elements from FHIR to their corresponding parts in the XNAT schema. 
By doing so, they not only improve accessibility of the data and interoperability between software instances. 
They also improve semantics because of the characteristic of FHIR to heavily rely on external terminologies like the Systematized Nomenclature of Medicine – Clinical Terms (SNOMED CT). 
These are important aspects of FAIR data [23, 24], too. 
On the other hand, the approach would not have been possible if the developers of XNAT had not chosen a modern, open-source architecture that ofers both Representational State Transfer (REST) interfaces and the ability to extend the internal data format with custom data types. 
This shows the necessity in the design of information systems not to see them as isolated solutions that can serve exactly only the originally intended purpose. 
It is easy to imagine the advantages in this case of an automated and updatable transfer of patient master data in terms of workload reduction and data quality for larger studies, as it became necessary for other scenarios in the wake of the COVID-19 epidemic.
New approaches in clinical research informatics
Varghese et al. report about a study portal to visualize the geographic distribution of study research networks [25]. 
This system helps to fnd clinical studies for a specifc disease in the vicinity of a certain location. 
The portal applies public data and semantic annotation. 
It is another medical informatics approach to support clinical studies with new digital health systems. 
Of note, patient recruitment is a well-known challenge for clinical research: on a global perspective, only about one third of clinical trials can be completed on time. 
The COVID-19 pandemic clearly demonstrated that speed matters in medical research. 
Thus, routine care and research need to be much more connected to establish a learning health system. 
Open metadata and data integration (data FAIRifcation [24]) are key success factors to leverage information systems to speed-up and improve clinical research.
Common data models in practice
The question of interoperability holds also for database schemas. 
A natural approach is to design a database schema oriented to the needs of the specifc use case. 
Schema designs often refne over time to refect new requirements and thus become more complex. 
Common data models (CDMs) are schemas that claim to cover a broader range of use cases. 
They are based on a model that has been consented in a circle of experts and developed over a longer period of time. 
Although new functionalities are added to CDMs, their core is often stable. 
This leads to the development of an ecosystem of diferent tools from the user community. 
However, the major advantage of using a CDM is the uniformity of data in distributed repositories. 
This allows for distributed, reproducible analyses across multiple sites from diferent regions or domains with large numbers of subject and thus tends to have greater statistical power. F
urthermore, the clearly defned semantics allow results from diferent studies to be compared. 
If existing data is to be transferred to a CDM, complex schema matching is necessary. 
This process is called data standardization. 
Haberson et al. [26] examine to what extent the Observational Medical Outcomes Partnership (OMOP) CDM is suitable to represent the data of the Austrian Health Claims Database GAP-DRG, which contains entries from 95% of the population. 
For their purpose, they select a manageable subset of the data. 
The actual transformation is preceded by a vocabulary mapping, since OMOP is based on standard medical terminologies like the International Classifcation of Diseases (ICD) or SNOMED CT and the locally used catalogs have to be mapped to the preferred universal concepts, which can be semantically challenging. 
As a result, certain variables cannot be mapped if, for example, the algorithmic basis difers too much. 
Although the proof of feasibility has not yet been demonstrated beyond doubt and requires further research, a plea for the use of international terminologies that would make such mapping steps obsolete can be gleaned from the issues described.
Lessons learned
In conclusions, ICT for health science research is an ongoing feld, and several new aspects have been added due to the current pandemic. Still, there is a “revolutionary potential of informatics for all aspects of biomedicine and healthcare” [2]. 
In particular:
Data matters: FAIR reusable and interoperable data is required to improve health and well-being. 
This includes further standardization to achieve semantical interoperability between the healthcare providers, not only on a national level, but also internationally, as we are facing today global health crises. 
In this context, the concept of a “broad consent” [27] becomes further important. 
Security matters: Exchanging health data requires reliable and secure ICT infrastructure, not only for clinical trials but also for contact tracing and tracking. 
Good medical informatics practice can seed trust such that individuals participate and donate their health data to the public. 
Time matters: 
After one year of pandemic, still relevant data is transported paper-based. 
This holds for Germany (e.g., the invitations for vaccination are snail-mailed) as well as for other countries. 
This causes delays and bias to statistics, false interpretation of data, and wrong actions. Improved ICT can resolve this problem, too.
References
1. Shabo, A., Madsen, I., Prokosch, H.U., Häyrinen, K., Wolf, K.H., Martin-Sanchez, F., Löbe, M., Deserno, T.M., ICT for Health Research: Proceedings of the EFMI 2019 Special Topic Conference. Stud. Health. Technol. Inform. 258, 2019.
2. Haux, R., Analysing the scientific publications of Peter L. Reichertz: refections from the perspective of medical informatics today. J. Med. Sys. 44:23, 2020.
3. Chen, H., Yang, B., Pei, H., Liu, J., Next generation technology for epidemic prevention and control: data-driven contact tracking. IEEE Access. 7:2633-42, 2018.
4. Sass, J., Bartschke, A., Lehne, M., Essenwanger, A., Rinaldi, E., Rudolph, S., Heitmann, K.U., Vehreschild, J.J., von Kalle, C., Thun, S., The German Corona Consensus Dataset (GECCO): a standardized dataset for COVID-19 research in university medicine and beyond. BMC Med. Inform. Decis. Mak. 20(1):341, 2020.
5. Iezzoni, L.I., Assessing quality using administrative data. Ann.
Intern. Med. 127: 666-674, 1997.
6. Winter, A., Funkat, G., Haeber, A., Mauz-Koerholz, C., Pommerening, K., Smers, S., Stausberg, J., Integrated information systems for translational medicine. Meth. Inf. Med. 46: 601-607, 2007
7. Van Laere, S., Cornu, P., Dreesen, E., Lenie, J., Buyl, R., Why do Belgian community pharmacists still treat electronic prescriptions as paper-based? J. Med. Sys. 43:327, 2019.
8. Riley, J., Understanding metadata: what is metadata, and what is it for?: a primer. Baltimore: NISO Press, 2017.
9. Merks, P., Jakubowska, M., Drelich, E., Świeczkowski, D., Bogusz, J., Bilmin, K., Sola, K.F., May, A., Majchrowska, A., Koziol, M., Pawlikowski, J., Jaguszewski, M., Vaillancourt, R., The legal extension of the role of pharmacists in light of the COVID-19 global pandemic. Res. Social Adm. Pharm. 17(1):1807-12, 2021.
10. Nilsson, E., Orwelius, L., Kristenson, M., Patient-reported outcomes in the Swedish national quality registers. J. Intern. Med. 279: 141-53, 2016.
11. Rauter, C.M., Wöhlke, S., Schicktanz, S., My data, my choice?: German patient organizations’ attitudes towards big data-driven approaches in personalized medicine: an empirical-ethical study. J. Med. Sys. 2021. in press.
12. Wicks, P., Massagli, M., Frost, J, et  al. Sharing health data for better outcomes on PatientsLikeMe. J. Med. Internet. Res. 12:e19, 2010.
13. Gliklich, R.E., Dreyer, N.A., Leavy, M.B., Christian, J.B., eds. 21st century patient registries. EBook addendum to registries for evaluating patient outcomes: a user’s guide, 3rd Edition. Rockville, MD: Agency for Healthcare Research and Quality, 2018.
14. Chang, E.M., Gillespie, E.F., Shaverdian, N., Truthfulness in patient-reported outcomes: factors afecting patients’ responses and impact on data quality. Patient Relat. Outcome Meas. 10: 171- 86, 2019.
15. Wong, A.W., Shah, A.S., Johnston, J.C., Carlsten, C., Ryerson, C.J., Patient-reported outcome measures after COVID-19: a prospective cohort study. Eur. Respir. J. 26;56(5):2003276, 2020.
16. Scotté, F., Minvielle, E., Mir, O., André, F., Barlesi, F., Soria, J.C., A patient reported outcome platform, a useful tool to improve monitoring and effective management of Covid-19-positive patients with cancer. Eur. J. Cancer. 132:1-4, 2020.
17. WHO Division of mental Health and Prevention of Substance Abuse (ed). Measuring quality of life: the WHOQOL-100 and the WHOQOL-BREF. 1997. WHO/MSA/MNH/PSF/97.4
18. Rivas, H., Wac, K., Digital Health; Springer International Publishing: Cham, 2018.
19. Ganapathy, N., Veeranki, Y.R., Kumar, H., Swaminathan, R., Emotion recognition using electrodermal activity signals and multiscale deep convolutional neural network. J. Med. Sys. in press, 2021.
20. Russell, J., A circumplex model of afect. J. Pers. Soc. Psychol. 39(6):1161-78, 1980.
21. Ye, J., Pediatric mental and behavioral health in the period of quarantine and social distancing with COVID-19. JMIR Pediatr. Parent. 3(2):e19867, 2020.
22. Khvastova, M., Witt, M., Essenwanger, A., Sass, J., Thun, S., Krefting, D., Towards interoperability in clinical research: enabling FHIR on the open-source research platform XNAT. J. Med. Sys. 44(8):137, 2020.
23. Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J.J., Appleton, G., Axton, M., Baak, A. et al. The FAIR guiding principles for scientifc data management and stewardship. Sci. Data. 3:160018, 2016.
24. Sinaci, A.A., Núñez-Benjumea, F.J., Gencturk, M., Jauer, M.L., Deserno, T., Chronaki, C., Cangioli, G., Cavero-Barca, C., Rodríguez-Pérez, J.M., Pérez-Pérez, M.M, Laleci Erturkmen GB, Hernández-Pérez T, Méndez-Rodríguez E, Parra-Calderón CL., From raw data to FAIR data: the FAIRifcation workfow for health research. Methods Inf. Med. 59(S 01):e21-e32, 2020.
25. Varghese, J., Fujarski, M., Dugas, M., StudyPortal: geovisualization of study research networks. J. Med. Sys. 44:22, 2020.
26. Haberson. A, Rinner, C., Schöberl, A., Gall, W., Feasibility of mapping Austrian health claims data to the OMOP common data model. J. Med. Syst. 43(10):314, 2019.
27. Maloy, J.W., Bass, P.F. 3rd., Understanding broad consent. Ochsner J. Spring; 20(1):81–6, 2020.
Medical Informatics in Indonesia: Importance, Development, and Future Directions
Abstract – Medical Informatics (MI) is an emerging multidisciplinary study in academic fields. 
Also known as ‘Health Informatics,’ this study combines three different well-known fields; Information Systems (IS), Computer Science (CS), and Health Science (HS). 
MI is crucial because it could help in bridging the gap that is commonly found in conventional Healthcare services with the usage of Information Technology (IT). 
However, despite its vital role, the development of MI in Indonesia is still not satisfactory. 
Therefore, in this paper, the importance of MI as an academic discipline will be briefly discussed, followed by the description of MI development in Indonesia, and some suggestions for its future development. 
Keywords – Informatics, Health Science, Indonesia, Information Systems, Medical Informatics.
1. Introduction
Medical Informatics (MI) is an emerging academic discipline that is essential for its role and contribution to improve Healthcare services by employing Information Technology. 
Also known as Health Informatics, MI consist of different kinds of fields, namely Computer Science or Informatics, Decision Science, Statistics, Medical Science, etc. [1]. 
However, the three most prominent fields in MI are Information Systems (IS), Computer Science (CS), and Health or Medical Science (HS). 
As a relatively ‘young’ discipline that emerge in 1970s, there has been changes, variations, and development of MI that leads to different definitions, concepts, and sub-disciplines. 
AMIA (American Medical Informatics Association) for example, tends to use the terms of biomedical informatics and healthcare informatics rather than medical informatics, which is used as a parallel term to other subareas, such as nursing informatics and dental informatics [2]. 
Other researchers have adopted the terms and definition given by AMIA, such as Sweeney [3], Valenta et al. [4], and Cummings et al. [5]. 
Meanwhile, the others prefer to use the medical informatics term as can be found in the works of Mihalas et al. [6], Collen and Shortliffe [1], and Wyatt and Liu [7]. 
Here, throughout this paper we used the latter term, i.e., the Medical Informatics (MI). 
In the next section, we will stress out the importance of MI as an academic discipline. 
Moreover, the description of MI development in Indonesia will be given in the following sections together with some solution and future directions for MI development in Indonesia.
2. Importance of Medical Informatics
Medical Informatics (MI) is important for its role and contribution to improve Healthcare services by employing Information Technology (IT). 
Its importance can be deducted from its nature as a multidisciplinary study, its contribution to society, and its future directions that open up many possibilities. 
The first factor that shows MI’s importance is its nature as a multidisciplinary study. 
Kim and Delen [8] define MI as a multidisciplinary field that implements IT to medical domain so that it could improve healthcare services and medicine. 
As a cross-section discipline of three big fields, MI could take advantage of each field and apply them to solve problems faced by other fields. 
As claimed by Cassell [9] for example, by using the help of a Computer Science programmer to build specialized software, a clinician (Health Science practitioner) could provide answers to complex questions about patients’ conditions. 
Another piece of research by Ahmed et al. [10] has shown the benefit of a self-management support system to enhance the life quality of asthma patients. The second factor that shows the importance of MI is its contribution to society. 
Much research has been done by notable researchers to tackle various problems by using MI’s approach. 
Hughes et al. [11] have presented a method to classify medical texts by using Convolutional Neural Networks (CNN). 
Jagannatha and Yu [12] have explored the usage of Recurrent Neural Networks (RNN) for medical event detection. 
Jayawardanu and Hansun [13] have also designed an expert system for cataract detection using C4.5 and Leonardo et al. [14] have built a fuzzy AHP based expert system for detection of pulmonary tuberculosis disease. 
Furthermore, a Could-IoT (a cloud-based Internet of Thing) based sensing service to monitor the health of elderly people has also been produced by Neagu et al. [15], while Motulsky et al. [16] have studied and reported the usage of mobile devices for inpatient rounding and handoffs. 
Some researchers even give their focus to the development of MI’s education, as can be found in the research of Saranto et al. [17], Valenta et al. [4], Cummings et al. [5], and Jaspers et al. [18]. 
All of these show the significant contribution offered by MI to the health of global society. 
The last highlighted factor of MI’s importance is its future directions. 
As an emerging discipline, MI could open up many possibilities and discoveries. 
Three main tracks of future directions which may be explored are (1) Education and management perspectives, that focus to adapt MI’s learning and teaching materials together with its management practices in health facilities; (2) 
Software and data analytics, that include all the software development and data analysis results using various algorithms in the CS field; 
and (3) Hardware and related technologies, that incorporate all devices’ utilization in the MI field including the related technologies being used. 
Moreover, MI also brings up the possibilities for the development of new subfields, such as Clinical Informatics, Medical Imaging Informatics, and Pathology Informatics.
3. MI Development in Indonesia
Even though MI has gained considerable attention worldwide from its importance as an academic discipline, the development of MI in Indonesia is still not satisfactory. 
There are only a few higher education institutions in Indonesia that offer MI course; the research publication numbers in MI are relatively low compared to the number of research publications in its parent domains; and there are only few Indonesia periodicals that have focus in this field. 
The low numbers of higher education institutions in Indonesia that offer MI as one of their courses have become its main drawback. 
There are more than 4,500 higher education institutions in Indonesia, but less than one percent of them have focus in MI. 
Moreover, many of them do not offer MI as a major, but rather as a minor or specialization subject in their respective programs. 
For example, Universitas Indonesia (UI) places MI as one of its 15 specializations in Public Health Master Program. 
Universitas Islam Indonesia (UII) offers MI as one of its minors in the Informatics Master Program. 
Universitas Gadjah Mada (UGM) also offers MI in its Public Health Master Program, specifically in the Health Management Information System Program. 
Universitas Pelita Harapan (UPH) offers MI at Undergraduate degree level but only as a specialization in its Informatics department. 
On the other hand, at least five different universities in Australia offer MI as a full course program, such as the Swinburne University of Technology, University of Wollongong, the University of Tasmania, Griffith University, and La Trobe University. 
From the mentioned examples, it can be clearly seen that MI has yet to become a major academic discipline in Indonesia’s higher education institutions. 
Another factor shows the unsatisfactory and slow development of MI in Indonesia is the low number of research publications compared to its parent domains, such as Information System (IS), Computer Science (CS) or Informatics (IF), and Health Science (HS). 
According to the ScimagoJR, one of the major publication-ranking systems, there is only 247 published documents in Health Informatics area from Indonesian authors during 2018-2019 [19]. 
Meanwhile, during the same period, there are 3,088 published documents in Medicine (HS) and 5,682 published documents in Computer Science (IF and IS) from Indonesian authors. 
So, there are significant differences in the publication numbers; it takes only around 8% of Medicine’s publication, 4% of Computer Science’s, and nearly 2% of both Medicine and Computer Science’s publications. 
Figure 1 depicts the bubble chart that relates the number of published documents in the Medicine field (where Health Informatics located) versus its H Index (a metric measuring the productivity and citations impact of a publication) for Indonesian authors during 2018-2019.
Lastly, there is a low number of academic journals from Indonesia publishers that give their focus on MI field. 
Out of 304 registered journals in SINTA (Science and Citation Index) [20] from the Health category, hardly 5% of them have related focuses and scopes in MI. 
Many of them still give broader focuses that fall in Public Health categories, such as Hospital Management, Health Policy, and Clinical Teaching. 
In fact, there are only two journals that clearly state their focuses in MI, i.e., the JEEEMI (e-ISSN 2656- 8632) and the IJEEEMI (e-ISSN 2656-8624). 
Both of them are published by Health Polytechnic Ministry of Health of Surabaya and Indonesian Electromedical Association (Ikatan Elektromedis Indonesia, IKATEMI) and had their inaugural editions in July and August 2019 respectively. 
However, since they are relatively new published journals, they are not registered yet as accredited journals in SINTA.
4. Solutions and Future Directions 
There are two possible solutions to address the unsatisfactory development of MI in Indonesia. 
Firstly, by founding a recognizable national organization in the MI field and secondly, by developing and implementing the MI curriculum as a major course in higher education institutions. 
The first suggested solution is by founding a recognizable national organization in the MI field. 
Actually, there are several international organizations in the MI field, such as the AMIA (American Medical Informatics Association), the HIMSS (Healthcare Information and Management Systems Society), and the IMIA (International Medical Informatics Association). 
IMIA even has a regional represented association called the APAMI (Asia Pacific Association for Medical Informatics), where Indonesia is one of its corresponding member countries [21]. 
However, the activities and contributions from Indonesia are hardly ever heard. 
Therefore, it would be better to have a nation-wide government-supported organization that could gather all Indonesian scientists and researchers from different fields related to MI and actively organize various events that could positively contribute to the development of MI in Indonesia. 
Another suggestion that could be given is to develop a standardized curriculum for MI in Indonesia. 
The curriculum should consider MI as a major discipline and should be assessed by acknowledgeable organizations and researchers in the MI field and supported by the Indonesian Government, specifically from the Ministry of Health and the Ministry of Education and Culture. 
By having a standardized curriculum, a higher chance of successful implementation of MI in higher education institutions is likely to be achieved. 
By having a higher number of higher education institutions offering MI, a greater number of students and researchers could be gathered and work productively together toward a better future development of MI in Indonesia.
5. Conclusion 
In conclusion, MI as an emerging academic discipline is essential due to its nature as a multidisciplinary study, its contribution through education and research breakthroughs, and its future directions that open up many possibilities and discoveries. However, MI development in Indonesia is not progressing as expected. 
The low number of higher education institutions offering MI, the low number of research publications, and the low number of Indonesia periodicals focusing on MI are some indications of this matter. 
Two possible solutions to handle this issue are by founding a recognized national organization and by developing and implementing MI curriculum as a major course in higher education institutions in Indonesia.
Evaluation in Life Cycle of Information Technology (ELICIT) framework: Supporting the innovation life cycle from business case assessment to summative evaluation
Keywords:
Health technology assessment
Implementation science
Health information technology
Clinical decision support
Human factors engineering
Evaluation framework 
ABSTRACT
Objective: Our objective was to develop an evaluation framework for electronic health record (EHR)-integrated innovations to support evaluation activities at each of four information technology (IT) life cycle phases: planning, development, implementation, and operation. 
Methods: The evaluation framework was developed based on a review of existing evaluation frameworks from health informatics and other domains (human factors engineering, software engineering, and social sciences); 
expert consensus; and real-world testing in multiple EHR-integrated innovation studies. 
Results: The resulting Evaluation in Life Cycle of IT (ELICIT) framework covers four IT life cycle phases and three measure levels (society, user, and IT). 
The ELICIT framework recommends 12 evaluation steps: 
(1) business case assessment; 
(2) stakeholder requirements gathering; 
(3) technical requirements gathering; 
(4) technical acceptability assessment; 
(5) user acceptability assessment; 
(6) social acceptability assessment; 
(7) social implementation assessment; 
(8) initial user satisfaction assessment; 
(9) technical implementation assessment; 
(10) technical portability assessment; 
(11) long-term user satisfaction assessment; 
and (12) social outcomes assessment. 
Discussion: Effective evaluation requires a shared understanding and collaboration across disciplines throughout the entire IT life cycle. 
In contrast with previous evaluation frameworks, the ELICIT framework focuses on all phases of the IT life cycle across the society, user, and IT levels. 
Institutions seeking to establish evaluation programs for EHR-integrated innovations could use our framework to create such shared understanding and justify the need to invest in evaluation. 
Conclusion: As health care undergoes a digital transformation, it will be critical for EHR-integrated innovations to be systematically evaluated. 
The ELICIT framework can facilitate these evaluations.
1. Introduction
As the adoption of electronic health records (EHRs) has expanded rapidly in recent years, EHRs have taken center stage in modern healthcare. 
Moreover, EHRs have become platforms for the delivery of various health interventions. 
For the purposes of this paper, we define EHR-integrated innovations as health interventions that (1) have a software component, including clinical decision support (CDS) tools (e.g., EHR-integrated alerts and reminders, order facilitators, integrated information displays), specific modules of an EHR system (e.g., for order entry or clinical documentation), and interoperable externals apps; 
(2) are integrated with the EHR, which could be achieved through various mechanisms such as using EHR native functionality, proprietary interfaces, and/or standards-based interfaces; 
(3) address a specific healthcare domain and/or process, such as colorectal cancer screening, medication ordering, or neonatal bilirubin management; 
and (4) aim to improve healthcare outcomes such as quality of patient care, provider experience, or healthcare value. 
Examples of EHR-integrated innovations include provider and patient reminders for preventive care procedures, an evidence-based institutional order set for community acquired pneumonia, and an interoperable external app supporting shared-decision making for lung cancer screening. 
EHR-integrated innovations have been identified as having great potential for improving health and health care.[1] 
However, to achieve their full potential, EHR-integrated innovations must meet multiple requirements. 
Ideally, EHR-integrated innovations must integrate into an existing EHR system; 
meet clinical needs as defined by organizations, national bodies, and/or professional societies; be acceptable to end users including clinicians and patients; 
integrate seamlessly into existing clinical workflows and information systems; 
alter clinician or patient behavior; 
do no harm; 
and be portable across EHR platforms. 
The need to meet these various requirements makes development, implementation, and evaluation of EHR-integrated innovations quite challenging. 
Recent publications show that EHR-integrated innovations do not always meet these requirements.[2–4] 
Thus, it is important to ensure the evaluation of EHR-integrated innovations is supported by an evaluation framework that can help address these various requirements. 
Given the complexity of EHR-integrated innovations, we have identified three main requirements for an evaluation framework for EHR-integrated innovations: 
(1) support the full information technology (IT) life cycle; 
(2) address measures at society, user, and IT levels; 
and (3) address issues specific to EHR integration. 
Justification for these requirements is shown in Table 1. 
While many comprehensive evaluation frameworks for health information systems have been described in literature reviews,[12–17] none of them meet the three requirements summarized in Table 1. 
Most evaluation frameworks developed for health information interventions do not meet the first requirement (i.e., support the full IT life cycle). [11,18–21] 
For example, the Health Information Technology Researchbased Evaluation Framework (HITREF) developed by Sockolow et al. describes 6 evaluation concepts (i.e., structural quality, quality of information logistics, unintended consequences/benefits, effects on quality process, and barriers and facilitators to adoption).[18,19] 
While HITREF acknowledges the importance of context, it does not address the “how” and “when” of evaluation throughout the IT life cycle phases.[15]
We identified a few evaluation frameworks aligned with the IT life cycle, [22–29] but these frameworks did not meet the second requirement (i.e., address society, user, and IT-level measures). 
Among these evaluation frameworks, many focused mainly on one level such as society-level,[25] user-level,[22,23] or IT-level measures. [24] 
Finally, the evaluation frameworks that did focus on multiple IT life cycle phases and multiple levels either omitted one of the IT life cycle phases or one of the measure levels. [26–29] 
Moreover, all existing evaluation frameworks failed the third requirement (i.e., address EHR integration issues). 
More specifically, the existing frameworks do not address the quality of back-end EHR integration, user-interface EHR integration, and the changes in EHR workflows caused by the innovation. [17] 
Hence, there is a need for a framework describing an appropriate evaluation for every phase of the EHR-integrated innovation life cycle on society, user, and IT levels. 
To address this emergent need for a holistic approach to the evaluation of EHR-integrated innovations, we developed the Evaluation in Life Cycle of IT (ELICIT) framework by incorporating concepts from human factors engineering, software engineering, and social sciences domains across the entire IT life cycle. 
The purpose of the ELICIT framework is to guide evaluation efforts during the planning, development, implementation, and operation of EHR-integrated innovations. 
The intended users of the framework are innovation research and development teams such as those from academic health systems and industry.
2. Objective
The objective of this paper is to describe an evaluation framework for supporting the whole life cycle of EHR-integrated innovations. 
EHR – electronic health record, IT – information technology.
3. Methods
3.1. Definitions
To enable shared understanding across stakeholders grounded in different domains with divergent terminologies, we define the following terms for the purpose of this paper (Box 1).
3.2. Study design 
This work is presented as a case study of the development of a new evaluation framework derived from the review of theoretical frameworks, the convening of an expert panel, and multiple real-world evaluations. 
All included projects were approved or exempted as quality improvement by the University of Utah (UU) Institutional Review Board (IRB).
3.3. Setting
This work was conducted at UU Heath, an academic medical center with five hospitals and 12 community clinics. 
UU Health has used the Epic® EHR in its primary care clinics since 1999 and enterprise-wide since 2016. 
As interoperability standards and application programming interfaces (APIs) were increasingly required by government regulations and adopted by healthcare systems and EHR vendors, [35] UU Health launched the ReImagine EHR initiative in 2016 to harness the promise of interoperable apps based on standards such as Substitutable Medical Applications, Reusable Technologies on Fast Healthcare Interoperability Resources (SMART on FHIR). [36–39] 
The ReImagine EHR initiative developed, implemented and evaluated over ten EHR-integrated innovations (e.g., to support neonatal bilirubin management guidelines, lung cancer screening guidelines, smoking cessation, and chronic disease management guidelines), established productive corporate partnerships, and secured over $35 million in grant funding. [40] 
ReImagine EHR’s innovations are seamlessly integrated with the Epic® EHR; 
are supported by native CDS components such as preventive care reminders; 
and are designed using interoperability standards to enable integration with other EHR products and dissemination across healthcare organizations. 
The initiative is led by the Associate Chief Medical Information Officer (KK). 
The technical development team consists of nine core team members and dozens of project-based contributors. 
3.4. Evaluation infrastructure
When initially established in 2016, the ReImagine EHR initiative focused primarily on the design and development of EHR-integrated innovations. 
The Director of the UU Department of Biomedical Informatics Sociotechnical Core (CW) oversaw the evaluation aspects of this new initiative, with support provided by seed research grants. 
Given the number of requested innovations and resource constraints, only limited evaluation was performed. 
However, as more innovations entered clinical use, we recognized the need to invest further in evaluation and to establish a formal evaluation team. 
In 2018, the initiative hired an evaluation scientist (PVK) with training in biomedical informatics, biostatistics, health technology assessment, and data science to oversee and coordinate the data-focused aspects of evaluation with statistical support from the UU Study Design and Biostatistics Center. 
Other key members of the evaluation team include project leads, clinical informaticists, human-factors engineers, software engineers, clinical champions, and implementation scientists. 
The engagement of these stakeholders is supported by a combination of operational funding, training grants, research grants, and industry collaborations.
3.5. Framework development
3.5.1. Overall approach to framework development
The four-year iterative process of framework development is summarized in Fig. 1. 
Our evaluation framework was iteratively developed through focused literature review, practical experience evaluating innovations, and expert consensus. 
Practical experience included the evaluation of traditional EHR-integrated innovations [41–46] as well as interoperable EHR-integrated innovations [40,47–51] including a neonatal bilirubin management app, 
[47,48] a diabetes pharmacotherapy outcome prediction app, 
[49] a chronic disease management app, 
[50] a lung cancer screening shared decision app, [51] and a clinical calculator app. [52] 
These projects involved all the clinical informaticists noted below who worked together developing and evaluating the applications. 
The expert consensus was based on this shared experience and was formally reached through five meetings in 2020 where the model went through discussion, review, and further development each time. 
Additional asynchronous review and feedback of the framework continued in 2021 involving 10 experts with training in biomedical informatics (PVK, CW, GDF, TT, TJR, HSK, KK), human factors engineering (TT, HSK, CW), software engineering (GDF, HSK, KK, CN), implementation science (GAA, CRR), and biostatistics/effectiveness research (PVK, CW). 
During the meetings PVK presented the framework draft, asked specific questions about the model and elicited experts’ feedback regarding relevant outcomes, questions, methods and theoretical perspectives. 
Based on the outcomes of each meeting, PVK revised the framework for further review by these experts. 
After 5 meetings and the achievement of initial consensus, we transitioned to individual meetings and asynchronous communication until we reached consensus on the final framework.
3.5.2. Approach to defining the IT life cycle phases
We defined the IT life cycle phases based on two sources: 
Ammenwerth et al.’s planning, development, implementation, and operation phases [6] and the Exploration, Preparation, Implementation, Sustainment (EPIS) implementation science framework phases. [7,32,53] 
The EPIS framework describes four life cycle phases for the implementation of health services interventions. [53] 
In the Exploration phase, stakeholders consider emergent or existing health needs and identify the best guidelines or clinical interventions to address those needs. [53] 
In the Preparation phase, the potential barriers and facilitators of implementation are identified for external (outer) and organizational (inner) contexts, adaptation needs are further assessed for both the innovation (e.g., innovation) and for system or organizational structures or processes (e.g., reimbursement, changes in clinic workflows), and a detailed implementation plan is developed. [53] 
In the Implementation phase, the intervention is rolled out into clinical practice. 
In the Sustainment phase, the intervention continues to be delivered, with or without adaptation as needed. [53]
3.5.3. Approach to defining the society, user, and IT-level measures
In creating our evaluation framework, we sought to incorporate key insights, theoretical perspectives, and practices from health informatics as well as inter-related domains of social sciences, human factors engineering, and software engineering (Fig. 2). 
Relevant frameworks, models, theories and reporting guidelines are described below in the context of three levels of EHR-integrated innovations: society, user, and IT.
3.5.3.1. Society-level measures. 
To select exemplar society-level measures, we focused on two specific social sciences: effectiveness research and implementation science. 
Effectiveness research is the systematic evaluation of health technology to demonstrate the impact and inform stakeholders’ decision making related to the dissemination of health technologies based on their effectiveness.[54] 
Effectiveness research perspectives are a loose collection of evaluation models that include clinical trials, heath technology assessment, comparative effectivenessresearch, health services research, health economics and outcome research methodologies. [55] 
Effectiveness research is critical for identifying return on investment and summative assessments of process, health, [21,54] economic, [56] and safety outcomes. [57–59] 
Reporting guidelines in effectiveness research include Consolidated Standards of Reporting Trials (CONSORT) [60], Standards for QUality Improvement Reporting Excellence 2.0 (SQUIRE 2.0), [61] STAtement on Reporting of Evaluation studies in Health Informatics (STARE-HI), [62] and Consolidated Health Economic Evaluation Reporting Standards (CHEERS). [63] 
A specific branch of social sciences called Implementation Science is especially useful for evaluating society level measures. 
Implementation science is “the scientific study of methods to promote the systematic uptake of research findings and other evidence-based practices into routine practice, and, hence, to improve the quality and effectiveness of health services”. [64] 
Implementation science evaluation frameworks assess implementation strategies and outcomes such as adoption, reach and implementation fidelity. [7,31,32,53,65,66] 
The main evaluation frameworks in this domain include Reach, Effectiveness, Adoption, Implementation, and Maintenance (RE-AIM), [65] Nonadoption, Abandonment, Scale-up, Spread, and Sustainability (NASSS), [67] and the Implementation Outcomes Framework. [66] 
Process-oriented frameworks such as EPIS are useful in describing processes such as implementation or development. 
Determinant frameworks such as the Consolidated Framework for Implementation Research (CFIR) [31] describe multi-level factors which influence implementation outcomes. [66] 
Reporting guidelines in implementation science domain include Standards for Reporting Implementation Studies (StaRI) [68] and Framework for Reporting Adaptations and Modifications to Evidence-based interventions (FRAME). [30] 
Traditionally, implementation science frameworks have focused on the implementation of social and health services interventions, [65,69] with less emphasis on digital technologies. 
However, there is an increasing demand for the adoption of implementation science frameworks in innovation development and evaluations. [70–72]
3.5.3.2. User-level measures. 
To select exemplar user-level measures, we focused on human factors engineering research. 
Human factors engineering entails studying user’s characteristics, needs, abilities, and limitations, and designing software around the user needs. 
Commonlyused human factors theories aiming to understand and modify end user behavior include the Social Cognitive Theory (SCT), [73] the Situational Awareness (SA) framework, [74] and the Unified Theory of Acceptance and Use of Technology (UTAUT). [75] 
User centered design includes three steps. 
First, users’ needs, tasks, values, and workflows are identified using observations, interviews, or focus groups. 
These activities are grounded in models of contextual design, [80] targeted Cognitive Task Analyses, [81,82] and representational analysis to identify the informational display formats that are meaningful to users. [83] 
Second, low and high fidelity prototypes are designed through an iterative process that involves journey mapping, story boarding, or concept mapping. 
Third, usability testing is conducted on the prototypes. [22,84,85]
3.5.3.3. IT-level measures. 
To select exemplar IT-level measures, we focused on the software engineering domain. 
Software engineering entails conceiving, specifying, designing, programming, documenting, testing, and refining software. 
Software engineering evaluation activities in health informatics address the soundness of the software architecture, [24] functionality, [86] security, [87] interoperability, [48,88] and readiness for clinical use. [89] 
Process-oriented frameworks in this domain include models for the software development life cycle. [90] 
The DeLone and McLean Information Systems Success Model specifies system quality, information quality and service quality as the main indicators of success at the technology level.
3.6. Example: Representative EHR-integrated innovation project
We illustrate use of the ELICIT framework by describing the evaluation of a representative innovation – the Neonatal Bilirubin Management App. [47,48] 
This innovation focuses on implementation of the 2004 American Academy of Pediatrics guideline for identifying and treating newborns with elevated bilirubin levels, which is a common condition affecting newborns that can lead to permanent brain damage.
4. Results
4.1. ELICIT framework
The resulting ELICIT framework can support the whole IT life cycle. The first element of the ELICIT framework is the four often-overlapping and iterative IT life cycle phases: 
I. Planning, II. Development, III. Implementation, and IV. Operation (Fig. 3). 
In the Planning phase, the software idea is conceived and prioritized, and the software development decision is made. 
In the Development phase, the software is designed and developed, the software rollout decision is made, and governance approval is secured. 
In the Implementation phase, the implementation is planned and the software is gradually rolled out into clinical practice. 
In the Operation phase, the software is maintained, updated as needed and disseminated to other clinical sites. 
The ELICIT framework describes three levels of measures: society, user, and IT, which could be remembered with an acronym SUIT. 
Examples of measures corresponding to different levels are displayed in Fig. 4. 
Society-level measures include impact on patient health, organizations, business processes, and return on investment. 
User-level measures focus on user experience, IT usability, and EHR workflows. 
Finally, IT-level measures focus on system, information, and service quality. 
The ELICIT framework is shown in Fig. 5 and expanded in Tables 2–5. 
The ELICIT framework consists of 12 evaluation steps fully covering the four phases of the IT life cycle and society, user, and IT measure levels. 
While evaluation is an iterative process and one in which different activities may occur in parallel, we provide in Fig. 5 a typical progression across the 12 evaluation steps of the ELICIT framework. 
In the planning phase, evaluation starts with the problem being addressed and moves from social needs to user needs to more specific technical requirements. 
In the development phase, the direction of evaluation is generally reversed, beginning with verifying that the prototype meets minimum technical requirements then progressing to user acceptance testing and evaluation of intention to use by the intended social group. 
The implementation phase then generally starts with the assessment of organizational readiness for the innovations, moves on to evaluating initial user satisfaction, and the technical implementation is monitored as it is deployed at a larger scale and enhancement requests are implemented. 
Finally, the operation phase includes the technical assessment of the innovation to determine whether it could be disseminated to other health systems, as well as the evaluation of long-term user satisfaction and the impact of the innovation on clinical and financial outcomes important to society. 
ELICIT framework steps, potential evaluation questions, and methods for data acquisition and analysis are included in Tables 2–5. 
While depicted in a linear form, steps within phases can occur in parallel or in different orders, and projects may move back to earlier phases (e.g., to take an implemented innovation back to software design and development based on user feedback). 
The proposed questions and methods are labeled as “Exemplar Questions” and “Exemplar Methods” as they are clearly not comprehensive. 
The listed methods are a subset of approaches from a much larger superset of potential questions and methods, as there are many factors beyond those discussed in the paper that should be considered when selecting specific evaluation questions and methods. 
Actual questions and methods should be selected based on societal and user needs discovered during the planning phase and adapted based on available resources. 
The planning phase involves innovation idea conception, prioritization, coordination with stakeholders, requirement gathering, and project governance review and approval. 
ELICIT framework steps 1, 2 and 3 fall into this phase (Table 2). 
The development phase involves software design, software development, and governance review and organizational approval of the implementation. 
ELICIT framework steps 4, 5, and 6 fall into this phase (Table 3).
The implementation phase involves implementation strategy design, staff communication, education and training, and pilot innovation rollout followed by wider rollout with iterative innovation improvements as needed. 
ELICIT framework steps 7, 8, and 9 fall into this phase (Table 4).
The operation phase involves conducting a clinical trial to assess effectiveness, maintaining and monitoring innovations after the changes in the innovation have stabilized and disseminating the innovations to other sites. 
ELICIT framework steps 10, 11, 12 fall into this phase (Table 5).
4.2. Example: Using ELICIT framework for a representative EHRintegrated innovation
We describe below use of our evaluation framework for a representative innovation, the Neonatal Bilirubin Management App. [47,48]
4.2.1. Planning phase
4.2.1.1. Step 1. Business case assessment. 
The business case for this innovation was to improve the management of neonatal hyperbilirubinemia.[92] 
The healthcare guideline to address neonatal hyperbilirubinemia has been available since 2004 and was adopted as the standard of care at the UU Health newborn nursery shortly thereafter.[92] 
However, there was wide variability in guideline adherence among the providers. 
To reduce this variability, universal bilirubin screening was implemented at UU Health in 2016, and an external standalone Web tool known as BiliTool was used to assess whether bilirubin levels required treatment by manually entering patient data abstracted from the EHR. 
The goal of the innovation project was to improve the fidelity of the guideline implementation and to save provider time through automation. 
Other motivators for pursuing this project included a prototype solution available through the SMART gallery app store,[105] a manageable scope, the engagement of enthusiastic clinical champions (JHS and CHS), and the availability of key data requirements in the EHR.
Moreover, no clear alternatives were available for fully meeting stakeholder needs within the EHR.
4.2.1.2. Step 2. User requirements gathering. 
The user requirements were gathered through Cognitive Task Analysis with clinical champions performed by JHS, CHS, and KK. 
The provider tasks and decisions included gathering relevant EHR data about mother-infant dyad; assessing newborn bilirubin levels in relation to patient-specific thresholds;
assessing the probability of rebound hyperbilirubinemia following phototherapy;[106] 
and making guideline-based care decisions.[92] 
4.2.1.3. Step 3: Technical requirements gathering. 
The technical requirements were identified through the analysis of business and user requirements by KK and other members of the ReImagine EHR software development team. 
Key identified requirements included app loading times of under 5–10 s and limiting the data elements to the US Core FHIR APIs as much as possible to improve interoperability.
4.2.2. Development phase
4.2.2.1. Step 4. Technical acceptability assessment. 
Technical acceptability was ensured through automated regression testing and manual integration testing in the test EHR environment to assess accuracy and response time performed by KK and the software development team. 
As is the case with all of our innovations proceeding to clinical use, the security risks were formally reviewed by the Information Security Office, and these risks were deemed by the institutional leadership to be outweighed by the project benefits. 
4.2.2.2. Step 4. User acceptability assessment. 
User acceptability assessment was conducted through multiple rounds of user feedback on lowand high-fidelity prototypes developed by the software development team. 
Heuristic evaluation based on Neilsen’s design principles [84] was also used to identify and address usability issues by TT, PVK, HSK, and CW. 
4.2.2.3. Step 6. Social acceptability assessment. 
For the social acceptability assessment, we focused on determining the efficiency gains. 
The efficiency assessment was conducted through a randomized, controlled, IRB-approved experiment in which 12 pediatric resident physicians were asked to manage bilirubin in 2–4 newborns, who were randomized to receive care supported by our EHR-integrated innovation or the previous standalone alternative (BiliTool) performed by PVK, HSK, and CW in 2018. 
This study showed a time savings of 66 s per bilirubin assessment (95% CI, 53–79) compared with usual time required for neonatal bilirubin management.
4.2.3. Implementation phase
4.2.3.1. Step 7. Social implementation assessment. 
Implementation factors assessment was done with clinical champions and the EHR team by JHS, CHS, and KK in 2017. 
Implementation facilitators included leadership engagement, the previous adoption of the underlying clinical guideline in the nursery, a positive implementation climate, a strong culture of quality improvement, and newborn nursery educational infrastructure supporting guideline implementation. 
The resulting implementation strategy had three parts: 
(1) an email communication to end users of the EHR by the institutional IT leadership with information on the innovation, 
(2) socialization among attending providers through clinical champions, 
and (3) attending physicians and senior residents teaching incoming residents to use the innovation. 
Implementation outcomes assessment started with beta users (JHS, CHS, and other attending physicians added gradually) who provided continuous feedback to help improve innovation usability and accuracy through weekly meetings. 
The Neonatal Bilirubin Management App was rolled out in the nursery and 12 community clinics in 2017. 
Implementation outcomes included innovation adoption, reach and implementation fidelity and were assessed by PVK in 2019. 
According to analysis of the app logs, in 2018, innovation was used at least once (adoption) in the nursery and 12 outpatient clinics. 
Innovation reach extended to all newborns born at the UU Health because the app was available system-wide. 
Most uses (86%) occurred in the nursery with physicians, nurses, as well as a variety of other stakeholders including medical students and medical assistants using the innovation. [27] 
Innovation implementation fidelity was high, with bilirubin levels clinically managed via the innovation for 90% of babies born at UU Health. 
We continue to monitor reach and implementation fidelity through the weekly analysis of the EHR logs. 
4.2.3.2. Step 8. Initial user satisfaction assessment. 
PVK, HSK, TT, and CW evaluated initial user satisfaction through qualitative analysis of 12 physician interviews. 
User response to the intervention was positive and some unintentional uses were uncovered such as using the app for resident education. 
4.2.3.3. Step 9. Technical implementation assessment. 
Technical implementation assessment involved performance, scalability, and uptime analysis. It was performed by the software development team.
4.2.4. Operation phase
4.2.4.1. Step 10. Technical portability assessment. 
Technical portability was evaluated through review of FHIR APIs that were not yet universally supported through US Core FHIR APIs performed by PVK, KK, CN, and the software development team, including through a formal analysis published in 2019. [48] 
4.2.4.2. Step 11. Long-term user satisfaction assessment. 
PVK, TT, and CW evaluated long-term user satisfaction through quantitative analysis of 109 SUS surveys of 208 recent innovation end users identified through system logs (52% response rate). 
The mean SUS score for attending providers was 91.05 (95% CI, 86.31–95.79) which corresponds to “best imaginable” usability. [47] 
4.2.4.3. Step 12. Social outcomes assessment. 
Social outcomes assessment involved process, health, and economic outcomes analysis by PVK.
CFIR – Consolidated Framework for Implementation Research, EHR – electronic health record, EPIS - Exploration, Preparation, Implementation and Sustainment, FRAME – Framework for Reporting Adaptations and Modifications to Evidence-based interventions, SCT – Social Cognitive Theory, SUS – system usability scale, UTAUT - Unified Theory of Acceptance and Use of Technology.
Process impact assessment was conducted as a before-and-after retrospective clinical trial from April 1, 2016 to April 30, 2019. With regard to process outcomes, this study showed that orders for clinically appropriate phototherapy during hospitalization increased for newborns with bilirubin levels above the guideline-recommended threshold (odds ratio, 1.84; 95% CI, 1.16–2.90; P = .01). [47] 
In the interviews with 12 users conducted by PVK and HSK, we identified that users were using the app for teaching. 
No negative unintended process consequences were found. 
We did not find a significant difference in measured health outcomes including length of stay, intensive care unit admissions, and readmissions. [47] 
No negative unintended health consequences were found. 
A preliminary economic evaluation was conducted in 2019. 
If deployed through the entire United States, given over 3.4 million eligible births in 2018, 66 s saved per use, and 5 uses per newborn, the innovation could save over 300,000 h of clinician time annually, [47] which for a $50–100 hourly pay would translate to $15–30 million in savings. 
We are exploring the conduct of a more formal costeffectiveness analysis in the future.
5. Discussion
In this paper, we described the ELICIT framework, an evaluation framework that can be used to support all phases in the EHR-integrated innovation life cycle through a comprehensive, replicable process.
ELICIT framework provides potential evaluation questions and methods for each measure level (society, user, and IT). 
ELICIT framework incorporates constructs and methods from health informatics and three related domains: social sciences, human factors engineering, and software engineering. 
Leveraging these measures, ELICIT framework builds on the EPIS implementation framework, with consideration given to outer and inner implementation contexts, bridging factors, intervention characteristics, and intervention adaptations after implementation. [7,32,53] 
As a holistic evaluation framework, ELICIT framework may enable shared understanding and enhanced coordination among stakeholders – a necessary foundation for good team function. 
Moreover, it may promote evaluation continuity across the IT life cycle by explicitly linking the findings from the initial steps of gathering requirements (Table 2),
to designing and developing the software (Table 3), 
to clinical implementation (Table 4), 
and to evaluating implementation, effectiveness, and economic outcomes (Table 5). 
ELICIT framework also supports moving back to earlier phases as needed, which is important in the healthcare domain as both EHR-integrated innovations and their implementation contexts are constantly co-evolving. 
Evaluators do not need to complete all 12 steps and should prioritize the ones which matter the most to them based on the detailed questions provided in Tables 2–5. 
The ELICIT framework was specifically designed to support the evaluation of EHR-integrated innovations. 
ELICIT’s IT-level measures address issues unique to EHR integration, including interoperability and portability across EHR platforms – a pervasive problem. 
Similarly, ELICIT’s user-level measures address the challenges associated with integrating EHR-based innovations into existing EHR user interfaces and clinical workflows. 
Furthermore, ELICIT’s society-level measures take into consideration how the EHR-based innovations affect organizational culture, business processes, patient outcomes and finances. 
These society-level measures are important for catalyzing further investments in, and wide dissemination of, EHR-integrated innovations. 
Compared to other health informatics evaluation frameworks, the ELICIT framework adds (1) coverage of the entire IT life cycle, 
(2) description of measures specific to society, user, and IT levels, 
(3) measures specific to EHR-integrated innovations, 
(4) attention to clinical context perspectives, 
(5) attention to implementation science, 
and (6) a checklist of relevant questions and methods that could be considered at each phase. 
The ELICIT framework is also more concrete than existing frameworks because it provides a checklist of relevant exemplar questions and methods that could be considered at each phase. 
These concrete questions and methods can serve as a starting point for evaluation activities. 
While some aspects of the framework are similar to existing frameworks, none of the existing frameworks provide a matrix which covers every intersection between the four IT life cycle phases and the three levels of measures (i.e., society, user, and IT). 
For example, ELICIT framework includes all the constructs defined in HITREF: 
[18,19] structural quality and quality of information logistics are covered in steps of the IT level; 
unintended consequences/benefits, effects on quality process, and barrier and facilitators to adoption are covered in the society level. 
Moreover, ELICIT framework incorporates elements from human factors engineering, software engineering, and implementation science which are not included in HITREF. 
In addition, we believe ELICIT framework is easier to follow because evaluation steps are assigned to discrete IT life cycle phases. 
To our knowledge, this is the first evaluation framework for EHRintegrated innovations that focuses on appropriate evaluation processes throughout the full IT life cycle that specifically includes the implementation phase, a much neglected aspect of information technology projects. [72,107] 
Many evaluation frameworks focus on the evaluation of only one part of the IT life cycle such as human factors engineering. [22,23] 
Even evaluation frameworks that focus on fullcycle approaches do not integrate all the different steps, especially the last few steps of clinical implementation. [5,26–29] 
Yet, effective implementation strategies are likely to be as important for overall innovation success as design quality or clinical value. 
Thus, although existing innovation evaluation frameworks have commonalities with ELICIT framework such as a focus on accuracy, usability, safety and effectiveness, [5,26–29] the ELICIT framework adds to the existing research by explicitly focusing on clinical implementation strategies and outcomes. 
Researchers in the field of implementation science also speak to the need to engage stakeholders, organize resources, and address user needs. [7,31,32,53,65,66] 
However, they tend to underemphasize the IT aspects, EHR workflow, communication channels, and user mental models that are required for gathering functionality requirements, designing system components, and designing effective implementation strategies. [34] 
Thus, ELICIT framework adds to existing implementation science research as it focuses on software engineering, user requirements, human factors engineering and value assessment.
5.1. Implications
Subject to further empirical testing, ELICIT framework could be applied to evaluation facilitation, coordination, and budgeting. 
First and foremost, we believe that ELICIT framework can facilitate effective, holistic, scalable evaluations of EHR-integrated innovations, thereby increasing their collective capacity to improve patient care, the provider experience and healthcare value. 
Second, ELICIT framework can facilitate planning and coordination for the evaluation of EHR-integrated innovations by identifying the evaluation needs across the IT life cycle. 
Third, we believe ELICIT framework can help justify the investment in evaluation efforts by explicitly mapping out the required evaluation steps and by supporting successful dissemination. 
Therefore, we hope that this framework will help innovative healthcare organizations and health IT companies to establish evaluation programs, select and pursue research questions and methods, and report the results of their development and evaluation efforts. 
We believe that this framework could potentially be generalized to all health IT innovations including EHRs, consumer-facing mobile apps, and artificial intelligence, and we encourage others to adapt and test our framework in different settings. 
We are planning to continue evaluating and validating this framework in the future. 
If the ELICIT framework is successfully validated, it would suggest that our methodology for framework development based on combining literature review, expert consensus, and practical experience might also be generalizable.
5.2. Limitations
A limitation of this study is that ELICIT framework was developed predominantly at one institution and has not been tested by external investigators. 
However, this framework was developed based on our experiences with a wide range of projects including operationally and grant funded projects in a wide range of clinical domains and settings. [41–48,50] 
We also collaborated with investigators from many other organizations and are familiar with their evaluation needs and approaches. 
As a second limitation, it is possible that some aspects of our evaluation framework may be difficult to implement in settings with less access to relevant experts. 
Finally, this is a work still in progress. 
As such, further enhancements may arise based on the experience of ourselves and others.
6. Conclusion
While EHR-integrated innovations show great promise, inadequate evaluations may result in poor usability, adoption, safety, or impact. 
A 12-step multi-level multi-phase evaluation framework for EHRintegrated innovations was iteratively developed to integrate insights from relevant domains and to help bring impactful innovations to market in an efficient and safe manner. 
This comprehensive and integrated framework covers the entire IT life cycle across the society, user and IT-measures, whereas previous frameworks focused on specific aspects only. 
We hope that this work will support evaluations undertaken by other digital health innovation teams while fostering discussion and further refinement of the framework.
Synthetic data – the new holy grail for patient-focused and science-led health data use?
The explosion of health data
According to one estimate, a single patient produces around 80 megabytes of health data annually (Huesch and Mosher, 2017). 
Another estimate suggests that nearly 2.3 zettabytes (= 2.3 Trillion gigabytes) of new health data are produced globally each year (Statista, 2018). 
And the accumulation of health data continues to grow with enormous speed. 
By 2025, Reinsel et al. (2018) anticipate a compound annual growth rate of 36%, which, for example, is 10 percent points higher than in financial services and 11 percent points higher than in media and entertainment. 
These health data hold the potential to revolutionize health care and enable dynamic, learning and sustainable health systems. 
On the one hand, health data are able to increase direct value for patients through optimized diagnostics, personalized therapy approaches, higher coordination and evidence-based treatment across the entire patient pathway. 
They can also accelerate the development and approval of new therapy approaches. 
On the other hand, health data ensure sustainability at the systemic level, e.g. through the possibility of reorienting payment systems to value and outcomes as opposed to volume, economic evaluations of care interventions or faster research. 
They also enable benefit assessment of medical products and medicine, even under “real world” conditions.
Data privacy in the area of big health data
But the exponential growth of interconnected health data does more than bring enormous potential to improving health care provision and research. 
These highly sensitive data also come with higher data risks. Data security and data privacy protection are therefore a critically important task. 
One technique for securing personal data and ensuring data privacy is anonymization in all cases where person-specific data are not required. 
Anonymous information is defined by the European General Data Protection Regulation (GDPR) Recital 26 as “[…] information which does not relate to an identified or identifiable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identifiable. […] 
To determine whether a natural person is identifiable, account should be taken of all the means reasonably likely to be used, such as singling out, either by the controller or by another person to identify the natural person directly or indirectly.” 
This means that all directly identifying elements, such as name, social security number or address, have to be erased or encrypted. 
But this is not sufficient, especially in the area of big data, since the multitude of interlinked data points per person could easily allow for re-identification (Rocher et al., 2019). 
Sweeney (2018; 2000) has shown through her research, for example, that 87% of the U.S. population are unique only through three elements: 
5-digit zip code, gender, and date of birth. 
That’s why GDPR requires that these quasi-identifiers also be addressed. 
At a minimum, “account should be taken of all the means reasonably likely to be used” [GDPR Rectical 26]. 
However, it is precisely here where the GDPR and other data privacy regulations lack of a quantifiable and universally agreed definition of identifiability. 
In practice, this raises major challenges for data guardians and data users (for an in-depth discussion see: Yoon et al., 2020). 
After all, on the one hand, in the area of big data any seemingly insignificant element of person-related information could potentially become sensitive information in combination with other elements. 
On the other hand, the more quasi-identifiers you are able to eliminate, the less useful the data becomes. 
Adequate data anonymization, which maximizes data-driven healthcare value, is a formidable challenge. 
Data guardians are reluctant to share appropriate data because of (understandable) data privacy concerns. 
This, in turn, creates an ethical conundrum, as it hinders data access that could generate value for patients and the healthcare system. 
Synthetic data may be capable of delivering a solution to this complex problem and also provide additional benefits for health data use.
Synthetic data: The promise to deliver “true” health data anonymization
Synthetic data refers “to the generation of artificial data with the aim of reproducing the statistical properties of an original dataset” (van der Schaar and Maxfield, 2020). 
As synthetic data aim to validly capture the original statistical distribution without revealing the original patient data, a synthetic data approach could potentially significantly decrease patient privacy concerns, while permitting data guardians to share these data more openly (e.g. via a synthetic data clearing house, as proposed by van der Schaar and Maxfield, 2020 Fig. 1). 
Producing high fidelity (= realistic) synthetic data and guaranteeing privacy – with synthetic data being (differentially) private in comparison to the original real data – is a challenging endeavor (for an in-depth examination of differential privacy, cf. Dwork and Roth, 2014). 
This is all the more true for health data, as these data are normally high-dimensional and result from complex statistical distributions. 
In addition, outlier populations (e.g. high-cost, high-need patients) are especially relevant, as the improved care for these patients could increase efficiency and overall value within healthcare systems (Tanke et al., 2019). 
However, because of the small number of relatively unique features of these patients, it is even more difficult to estimate realistic synthetic data with accurate, unique, and highdimensional distributions in a way that does not replicate the original individual (van der Schaar and Maxfield, 2020). 
The field of standardized evaluation and auditing metrics for synthetic data is still evolving and needs further research and standardization (Alaa et al., 2021). 
Despite these obstacles, it has been shown in different use cases that machine learning models can produce valid results. 
One commonly and successfully used machine learning approach, for example, is Generative Adversarial Networks (GANs). GANs are generative models with two adversarial neural networks: 
a generator that tries to produce realistic new data and a discriminator that learns to distinguish between (potentially) true and fake outputs of the generator. 
GANs are widely used for image generation with astonishing results (e.g. synthetic faces, medical images: real vs synthetic, Fig. 2). 
Modified GAN models demonstrate promising results and can produce high fidelity synthetic data that guarantee privacy for different use cases (Jordon et al., 2018; Xie et al., 2018; Yoon et al., 2019, 2020).
Synthetic data: Benefits for patient-focused and science-led health data use beyond data privacy and data access
The early successes show that synthetic data approaches make it possible to achieve truly anonymous data and thereby enable broader data access. 
In this way, the full potential of big data can be harnessed to improve care and research. 
Synthetic data could potentially provide the following additional benefits for patient-focused and science-led health data use:
Increase robustness and adaptability of machine learning models
Machine learning models need representative, big data to produce high quality applications. 
Despite the explosion of health data, data privacy, standardization, the problem of data linkage, and other reasons have resulted in a lack of real world data to fulfill this need. 
Synthetic data offer as a solution the possibility of data augmentation, which can improve machine learning models. (Bolaños et al., 2021; Chen et al., 2021; Mahmood et al., 2020; Waheed et al., 2020; Wan and Jones, 2020). 
This is especially helpful when dealing with small sample data sets (e.g. underrepresented patient subgroups, rare diseases). 
For example, by generating synthetic histology images of renal cell carcinomas (Fig. 1) to augment the training data set for a neural network, rare renal cell carcinoma subtypes such as the chromophobe subtype (only 5% of all cases) can be identified more accurately (Chen et al., 2021).
Facilitate reproducibility of clinical studies and analyses
As synthetic data imitate real world data and, at the same time, allow for anonymity, they may be shared more openly. 
This could strengthen reproducibility and scientific progress (Azizi et al., 2021; Chen et al., 2021).
Synthetic control arms to accelerate treatment evaluation and value-based reimbursement models
Even though traditional randomized controlled trials are still the gold standard for evaluating treatment efficacy, single arm trials may be the only ethical or feasible option in some cases, such as rare diseases. 
And regulatory authorities have begun to recognize the value of augmenting such single arm trials with synthetic control arms. 
For instance, the approval of cerliponase alfa for a special form of Batten disease by the FDA or an EU approval for a label expansion for alectinib, a non-small cell lung cancer treatment, were based on single arm trials with a synthetic control arm (Baumfeld Andre et al., 2020; Thorlund et al., 2020).
In addition, while value based care models struggle with a fair and transparent method to determine quality and cost effects (Bailit and Hughes, 2011; DeLia, 2017; Pimperl et al., 2015; Weissman et al., 2012), synthetic control arms might also be an option for addressing this challenge.
Address imbalances and minimize bias
Synthetic data generated from biased datasets would replicate this bias toward over- or underrepresented conditions. 
However, augmenting underrepresented patient subgroups on purpose in the synthetic data production process could help to address such imbalances and generate “fair” representations of the real statistical distributions (Bhanot et al., 2021; Chen et al., 2021)
Test algorithms before applying to real data or real AI applications
Synthetic data can be a good proxy for real world clinical trial analyses (Azizi et al., 2021). 
As access to real world data is sometimes restricted or limited in the manner (e.g. only onpremise access in a data hub) or time they may be accessed, synthetic data can offer a test data set for algorithms that are later run on real data. 
Another valuable use case lies in the generation of synthetic data (e.g. sensor data) to develop digital health AI applications. 
Synthetic data could help to cheaply develop first proof-of-concepts and minimal viable products without the need for instant access to large and diverse real data sets (Dahmen and Cook, 2019; Stolfi et al., 2020).
Conclusion: Are synthetic data the new holy grail?
Synthetic data offer a multitude of opportunities for patients and our health care system from a practical, data privacy and ethical perspective. 
In practice, there is still a set of challenges that have to be addressed. 
ne is to provide a large, high quality, linked and uniformly coded, and diverse real data sets to initially generate synthetic data. 
Another is to develop a standardized approach to defining and measuring the quality and anonymity of synthetic data. 
Finally, there is pressing need to adequately meet multiple and diverse requirements regarding data privacy by data guardians or data/study quality by regulatory bodies. 
Solving these complex challenges is undoubtedly a daunting task – but given the potential benefits of synthetic data, it is obviously a worthwhile and exciting endeavor.
A Domain-Adaptable Heterogeneous Information Integration Platform: Tourism and Biomedicine Domains
Abstract: In recent years, information integration systems have become very popular in mashup-type applications. 
Information sources are normally presented in an individual and unrelated fashion, and the development of new technologies to reduce the negative effects of information dispersion is needed. 
A major challenge is the integration and implementation of processing pipelines using different technologies promoting the emergence of advanced architectures capable of processing such a number of diverse sources. 
This paper describes a semantic domain-adaptable platform to integrate those sources and provide high-level functionalities, such as recommendations, shallow and deep natural language processing, text enrichment, and ontology standardization. 
Our proposed intelligent domain-adaptable platform (IDAP) has been implemented and tested in the tourism and biomedicine domains to demonstrate the adaptability, flexibility, modularity, and utility of the platform. 
Questionnaires, performance metrics, and A/B control groups’ evaluations have shown improvements when using IDAP in learning environments.
Keywords: information systems; recommenders; information fusion; Semantic Web; intelligent agents
1. Introduction
Information integration is the method by which integral access is provided to distributed, heterogeneous information sources with different formats (structured, semistructured, and unstructured) [1], thus affording users with the sensation of examining a centralized, homogeneous information system [2]. 
A major problem of access to digital information is its heterogeneity, a consequence of the notable growth it has experienced in recent years. 
This has given rise to a wide variety of interfaces, such as Open Database Connectivity (ODBC), web services, Structured Query Language (SQL), XML Path language (XPath), and the Simple Object Access Protocol (SOAP) [3]. 
Unfortunately, the solution— the development of information integration systems—is hampered by the variety of formats in which this information is stored: 
relational databases, eXtensible Markup Language (XML) format, proprietary applications formats, data feeds, social networks, etc. [4]. 
Hence, the challenge is to produce tools that can be used for the rapid development of applications that can harvest information from different sources and may be stored in different formats [5]. 
Information integration, understood as the study and unification of multiple sources of heterogeneous information into a single system, has a long history, [1] and, since its beginnings, the unification of database schemas has been an enormous challenge [6]. 
The representation of information in databases is usually structured, but, with the advent of e-mail, webpages, and digital audio and video, a huge amount unstructured information has appeared, making it more difficult to find and access. 
The representation of information in XML format was an early attempt to bridge the gap between structured and unstructured information [7], 
and the later development of information integration technologies finally led to semantic integration, which is able to integrate databases, files in different formats, and ontologies [8]. 
The emergence of the Semantic Web [9] has been making an excellent revolution of smart web applications and adds reasoning capabilities for information accessing and integration. 
According to the World Wide Web Consortium (W3C), the Semantic Web refers to the vision of the web of linked data enabling people to create data stores on the Web, build vocabularies, and write rules. 
Some of the technologies used for those purposes are Resource Description Framework (RDF), SPARQL, the Web Ontology Language (OWL), or Simple Knowledge Organization System Namespace (SKOS). 
To effectively use heterogeneous sources of information in large information systems, such as the Internet, advanced and automatic systems of information acquisition, organization, mediation, and maintenance are required. 
These automated agents are designed to make resources available in a proactive fashion, resolve the difficulties of accessing information, and offer value-added services, such as automatic recommendation [10]. 
Intelligent agents provide knowledge on a chosen topic based on pertinent information found in different information sources. 
These form the nucleus of information integration systems capable of collecting information and making appropriate recommendations [11]. 
One major difficulty that information integration systems face is the integration of information that may differ depending on the domain. 
Moreover, most of the information is provided in text expressed in natural language and sometimes using specific context symbols or expressions, as in social media. 
To process them, there is branch of computer science called natural language processing (NLP) that gives computers the ability to understand text as human beings do. 
Therefore, a domain-adaptable platform that uses NLP is an appealing solution for this context. 
The design and development of recommendation systems to complement the information integration systems links areas such as machine learning, data mining, and information recovery [12], and the resulting systems need to analyze the data accessing the complete dataset (being centralized) to exploit the big data capabilities. 
Hence, the creation of mashups in an enriched format is needed to analyze distributed and heterogenous data. 
While data integration has been historically linked with expert owners of data to connect their data together in well-planned, well-structured ways, mashups are about allowing arbitrary parties to create applications by repurposing a number of existing data sources without the creators of those data having to be involved. 
In this context, linked data provides some relevant characteristic for the creation of the mashup and its exploitation. 
Linked data refers to a set of best practices for publishing and linking structured data on the Web and allows the redefinition of concepts to make data format homogeneous and interconnects resources facilitating the access to a set of databases using the resource description framework [13]. 
As far as the authors know, there has not been any work that gives a holistic perspective of the current technologies and applications of linked data mashups as well as the challenges for building linked data mashups [14]. 
The overall goal of our work is to achieve the integration of both structured and unstructured information sources by constituting an architecture that can be adapted to different domains and that could accommodate different technologies of the area of machine learning. 
We propose a platform architecture that is information domain-adaptable, similar to those described by other authors [15], including not only the specific functionalities but also the technology that makes them a complete framework. 
Moreover, the proposed architecture overcomes some of the problems related with centrality and the use of a unique central data system by: 
Implementing a set of functionalities that deals with heterogeneous information by using NLP technologies and concept recognition. 
Meeting W3C Semantic Web criteria. Most mashups applications do not use W3C standards and cannot be automatically accessed, reducing their functionality. 
Automatically incorporating machine learning higher-level functionalities by integrating the recommendation of information and enriching this recommendation via sentiment analysis. 
This architecture has been validated in two different domain use cases—(i) a tourism domain, and (ii) a biomedicine domain. 
Moreover, in order to validate the end-user experience and the provided functionalities, we performed some functionality and performance tests. 
The tourism use case aims to create a system that provides valuable and personalized tourism information in the format of context-dependent recommendations based on users’ profiles via the integration of information from Freebase, DBPedia, and Expedia. 
Regarding the biomedicine use case, it aims to produce a platform that allows users to provide personalized medicine and learning via the categorization of clinical cases and the recommendation of similar cases, integrating information from MedlinePlus and Freebase. 
These are two specific domain applications, but the overall architecture can be applied and enhance other big data applications rather than recommendations; 
for instance, providing early warning systems to prevent and anticipate environmental impacts on health [16], using social media data for nowcasting flu activity [17], or drug discovery and repurposing using data and knowledge from already known drugs [18]. 
The selection of these two use cases was based on: (i) in the tourism domain, the consultations usually involve systems that provide personalized information and recommendations; 
(ii) in the biomedicine domain, the search for information includes many areas of interest and sources of information from different population groups (such as the general population, medical professionals, or medical researchers). 
The rest of the paper is organized as follows: 
Section 2 discusses the related work and architectures; 
Section 3 introduces the proposed architecture; 
Sections 4 and 5 examine its use in the domains of tourism and biomedicine; 
Section 6 presents the assessment of the system by end-users; and, finally, Section 7 provides the conclusions obtained in this work.
2. Related Work
The concept of information integration has been studied in different ways at least during the last two decades. 
In this section, different types of architectures are introduced, explaining the information integration from a semantic and non-semantic viewpoint. 
In addition, the concept of a mashup is defined, and the most important methods and algorithms used in automatic recommendation are discussed. 
Many information integration architectures are based on the well-known serviceoriented architecture (SOA). 
This type of architectures is characterized by its flexibility with respect to resource integration and service implementation as its main advantages but provides a basic integration between the parts. 
In [19], the authors proposed an integration architecture that aims at exploiting data semantics in order to provide a coherent and meaningful view of the integrated heterogeneous information sources. 
They use five different layers, focusing on the standardization of the data to conform an inter-exchangeable format and homogenize the accessing methods. 
The combination of web services and software agents provides a promising computing paradigm for efficient service selection and information integration and has been deeply studied, showing is gracefulness [20]. 
Equally, in [21], the authors proposed an intelligent multi-agent information system (MAIS) architecture for proactive aids to Internet and mobile users. 
They also employed Semantic Web technologies for the effective organization of information resources and service processes in the tourist domain. 
AgenRAIDER [22] is another example and is designed to develop a comprehensive architecture for an intelligent information retrieval system with distributed heterogeneous data sources. 
The system is designed to support the intelligent retrieval and integration of information with the Internet. 
The current systems of this nature focus only on specific aspects of the distributed heterogeneous problem, such as database queries or information filtering. 
A more general framework can be established by integrating information systems using web-oriented integration architecture and RESTful Web Services [23], composing a method for the semantic integration of information systems. 
This approach follows the principles of Web 2.0, which strongly promote the simplest, most open, and best scaling software development approaches. For this purpose, it employs the web-oriented integration architecture (WOIA) that extends the concept of web-oriented architecture (WOA) that, in recent years, has been gaining attention as an alternative to traditional SOA. 
Other alternative approaches to those presented above include the crowdsourcing concept to create an architecture focusing on the data source acquisition process based on the concept of the ‘crowd as a sensor’ and providing natural language processing, semantic, and some machine learning capabilities [16].
2.1. Information Integration
Information integration appears as result of the need to unite heterogeneous data, i.e., unifying structured, semi-structured, and unstructured information from different sources. 
This term has been discussed vastly in the literature and rests on three principles [24]: 
• Data exchange. 
This involves the transformation of information depending on how well the database schema from which the data are extracted is defined, and on how well the destination database is defined (how the data are to be arranged). 
• Data integration. 
The data to extract may be in databases or other sources (with other schemas), but it must all end up in a single schema. 
• Peer to peer integration. 
All of the peers are autonomous and independent; therefore, there is not any schema. 
Moreover, information integration can imply or not imply semantic integration. 
Over the last ten years, information integration has largely been undertaken without semantic tools, with service-orientated architecture (SOA), XML, web services, universal discovery, description and integration (UDDI), and scrapping techniques as the main tools for the development of integration platforms. 
Despite the fact that the appearance of new tools for handling and visualizing data has improved the integration of heterogeneous schemas, they still fail to map data correctly, leaving them in dispersed database columns or in columns where they should not be. SOA provides links to resources upon demand, thus allowing access to resources in a more flexible manner than traditional architectures [25]. 
Within SOAs, web services are described in Web Services Description Language (WSDL) to define their functionality and how to invoke them and publish this information in UDDI registry format. 
Developers access these descriptions in WSDL and select those that satisfy their integration needs, invoking the service required via XML/SOAP messages. 
Web services have proven to be an ideal tool for integrating information, largely via the standards XML, SOAP, WSDL, and UDDI [26]. 
In addition, the high data exchange speeds now available with mobile phones provide a new method—the mobile web service—for integrating data [27]. 
Although XML, SOAP, WSDL, and UDDI provide acceptable interoperability and integrity, much effort is still required to integrate information in real systems [28]. 
The use of semantic technology in information integration, for example the use of ontologies (information models that specify the semantics of the concepts used by defined and unambiguous heterogeneous data), increases the interoperability between sources. 
A good way to achieve efficient information integration and interoperability is the use of W3C standards (http://www.w3.org/standards/semanticweb/, accessed on 8 August 2021). 
Many integrated applications use domain ontologies as their main operational tool, accomplishing the mapping in agreement with the concepts of the ontology [29]. 
This structure has been used to develop systems that combine web services with an ontology Information 2021, 12, 435 5 of 21 and thus facilitate access to different data sources, such as OpenStreetMap, Yahoo Where on Earth, and Wikipedia [30,31]. 
To remedy the data integration issues of the traditional web mashups, the Semantic Web technology uses linked data based on the RDF data model for combining, aggregating, and transforming data from heterogeneous data resources to build linked data mashups [14]. 
In recent years, ontologies have been defined using different web semantic languages, including RDF, resource description framework schema (RDFS), and OWL, e.g., to extract information from Wikipedia. 
Moreover, a widely used source of structured information in information integration applications is Freebase [32]. 
However, it cannot be classified as a true ontology but rather as a kind of folksonomy since concepts are structured in schemas and expressed as types grouped by domain and properties. 
Many projects are currently underway that use Freebase as their main operational tool, e.g., BaseKB (http://basekb.com/, accessed on 8 August 2021), which converts Freebase into a complete ontology by converting its content into RDF. 
SPARQL protocol and query language is the standard for consulting RDF data repositories, such as those required in the ReSIST (http://www.resist-noe.org/, accessed on 8 August 2021) project, or for consulting RDF triplets databases. 
They have been used to produce a rather weak integration of information via SPARQL–SQL and, in more intensive, ways how SPARQL queries are rewritten dynamically depending on the data store to be accessed [33]. 
This allows for a more flexible and more automatic integration. 
Finally, Semantic Web services combine web services and the Semantic Web to increase automation in the digital environment. 
Semantic Web services add meaning to the definitions of web services and thus improve the integration of distributed systems. 
In [34], the authors provided a framework for integrating information using Semantic Web services to create a semantic description of the services automatically generated by their system.
2.2. Mashups
Nowadays, many of the developed information integration systems are based on mashup architectures. 
Mashups incorporate data from different sources into a single application, bringing together different technologies and languages in order to integrate information [35]. 
Mashup architectures are composed of three layers: 
• A provider of content (data layer). 
Sources usually provide their data via an application programming interface (API) or web protocols, such as really simple syndication (RSS), representational state transfer (REST), and web services. 
RDF modelling is performed in this layer, the data are filtered via a SPARQL query, and the output elements are grouped under a SPARQL design and then published. 
• A mashup site (processing layer). 
This web application offers integrated information based on different data sources. 
It extracts the information from the data layer, manages the applications involved, and prepares the output data for visualization via languages such as Java or via web services. 
• A browser client (presentation layer). 
This is the mashup interface. 
Browsers find content and display it via HTML, Ajax, Java Script, or similar functionality toolkits.
2.3. Recommendation Systems
A recommendation system’s main objective is to provide the most suitable item that a user will like given a domain. 
Most of the existing recommendation systems fit into one of the following two categories: 
(i) content-based recommendation, 
or (ii) collaborative filtering (CF) systems. 
The first approach addresses the recommendation problem by defining a user profile model U that represents all the information available on a user. 
In a basic problem setup, U includes the user’s preferences for a set of items, later used to describe the user’s likes and dislikes. 
The second approach—simply abbreviated as CF—has achieved the most successful results and focuses on users’ behaviors as proposed by [12] rather than on the users’ characteristics. 
This method uses the similarities among users to discover the latent model that best describes them and retrieves predicted rankings for specific items [36]. 
In both cases, the users’ information can be collected explicitly via questionnaires or implicitly analyzing the online web behavior. 
Nowadays, due to the increase in the use of Web 2.0, social networks can also be exploited as a new source of information [37,38]. 
Prediction methods that incorporate some information from social networks are usually more accurate than those that do not because it allows getting a better user modelling characterization or information about users’ similarities [39]. 
The last is known as social recommendation and has been successfully applied by different authors [40,41]. 
An advantage of using social information in recommendation systems is the collecting of opinions. 
For example, sentiment analysis can enrich the recommendations made. 
It is important to highlight that the social information can appear in different forms, being heterogeneous, distributed, or unstructured. 
These types of data have increased enormously during the last decade, leading to a need for systems that group the large amounts of information generated by user communities. 
In addition, the language used may be ambiguous, hindering its use. 
Web 3.0 integrates ontologies and intelligent agents and is defined as a flexible and autonomous program with complete functionality that co-operates with other agents to attain goals, making decisions along the way [42]. 
Ontologies handle data represented semantically with the aim of organizing information in such a way that it can be exchanged easily by machines or intelligent agents capable of processing it and drawing conclusions, and in recommendation systems make use of ontologies for facilitating interactions. 
Centralized recommendation systems are usually found on servers, which limits their efficiency, scalability, and end-user privacy [43]. 
The alternative involves the distribution of recommendation systems, but the complexity of this could generate huge computational loads [44]. 
Multi-agent systems, which are composed of several intercommunicating and collaborating intelligent agents and are very useful in distributed systems, could help overcome such problems by focusing on community preferences rather than individual end-user preferences [45]. 
This architecture has been used in the LOCPAT project, a personalized recommendation system that provides recommendations to a requester in an agent network [46], and in an ACI project [47]. 
The architecture of the platform proposed in the present work makes use of several of the above aspects and follows the work of [48] and [49] by combining semantic and nonsemantic information integration and recommendation systems. 
The result is a mashup that not only provides information to end-users but can also recommend further information.
3. Intelligent Domain-Adaptable Platform (IDAP)
The proposed IDAP architecture tries to solve some of the problems presented in the previous section by allowing access to information via the identification of concept names in a source text and being capable of searching for semantic relationships and descriptive information that link them. 
This helps the user to gain insight into the detected concepts and discover potentially relevant information. Moreover, recommendations can be proposed based on those user preferences and the semantic association to the searches. 
It is worth highlighting the emphasis of the architecture permitting the inclusion of new sources of information and how this can be presented online, enriching the experience of end-users whenever they employ different final devices. 
The architecture is shown in Figure 1, and it aims to facilitate the integration of the following modules that provide the general framework and described functionalities: 
• End-user flow and access module
• Natural language processing and concept recognition module
• The Semantic Web module
• Recommendation intelligent agents module
• Semantic Web service
• In the next part, we describe these modules.
End-User Flow and Access Module
The proposed architecture has two forms of interacting with end-users or remote devices: 
via a web application or via a Semantic Web service (Section 3.4). 
Via the former, end-user flow begins by the introduction of text, and the system identifies key concepts related to the domain in which the end-user is immersed. 
End-users can get insights about these concepts by selecting them, obtaining information that is more detailed as well as a score (provided as a percentage) on how recommendable it is. 
In addition, some recommendations regarding related concepts that may be of interest are presented. 
The enduser can also provide feedback for those evaluations via the web application allowing the system to learn iteratively.
Natural Language Processing and Concept Recognition Module
Conceptual indexing automatically organizes all the words and phrases of a text into a conceptual taxonomy that explicitly links each concept to its most specific generalizations.
IDAP architecture includes the generic toolkit for text engineering, so-called GATE [50], an open-source software that provides a framework for text processing. 
It was conceived with the aim of facilitating scientists and developers’ daily tasks by implementing an architecture for the creation of linguistic engineering applications and to provide a graphic environment in which to develop the different components necessary in these types of applications. 
In GATE, all the elements making up the natural language processing (NLP) software system can be classified into three types of resources: 
• Language resources (LRs), which represent entities such as documents, corpora, or ontologies.
Processing resources (PRs), which represent entities that are mainly algorithms, such as analyzers, generators, and so on. 
• Visual resources (VRs), which represent the viewing and editing of graphic interface components. 
• GATE provides two operational mechanisms: 
one graphic and one consisting of a JAVA interface. 
The development environment can be used to display the data structures produced and consumed in processing, as well as to debug and obtain performance measures. 
Among the different programming options for integrating this software into the proposed platform, we selected the development and testing with the graphical user interface (GUI) ‘Developer’, which makes use of the logic for the NLP module.
GATE is distributed along with an information extraction system, a nearly new information extraction system (ANNIE), that incorporates a wide range of resources that carry out language analysis tasks. 
Gazetteer is also one of its components.
Based on predefined lists, Gazetteer allows the recognition of previously mentioned concepts. 
These lists, in turn, allow the inclusion of features for each concept and in the present proposal are primarily used to store the Freebase identifier.
3.3. Use of Freebase in Semantic Access
Freebase [51] is a large collaborative knowledge database. 
It structures content by “topic”, typically based on a Wikipedia article. 
A topic can be related to different things, introducing the concept of “types”, which, in turn, have sets of “properties”. 
The types are merged into “domains” (such as medicine, travel, biology, location, astronomy, music, sports, etc.), each of which is assigned an identifier (i.e., /medicine, /travel, /biology, /astronomy, /music, or /sports). 
Types and properties also have unique identifiers based on the concatenation of the type or topic name. 
For example, (A) the type gene belongs to the biology domain; 
hence, its identifier is /biology/gene (B), while the type tourist attraction belongs to the travel domain; 
thus, its identifier is /travel/tourist attraction. 
Freebase has already been used in Web 2.0 and 3.0 software development for name disambiguation, query classifications, and faceted browsing, among other applications. 
Freebase also offers different APIs for consulting information. 
Among them, the search API and an API that returns data in RDF format are used to recover semantic information from each concept in Turtle format (http://www.w3.org/TR/turtle/, accessed on 8 August 2021). 
Freebase service handles queries are made to, and answers are supplied by, the Freebase system. 
The input is a chain of text that includes the name of a concept. 
This chain is used to perform a search of Freebase to identify the resource that corresponds to it. 
Once located, the RDF API is used to recover the information associated with the concept. 
The different information sources used in the proposed platform provide a sufficient number of concepts for the two studied use cases, as shown in the evaluation section (Freebase itself provided 14,000 concepts for the biomedicine domain, and over 4000 for the tourism domain).
3.4. The Semantic Web Module
There are two important components related to semantics in the platform related to how the data is stored and how we can make inferences and reasoning with that data. 
Triplet storage. 
All the Semantic Web-based applications possess a storage system in which RDF triplets are stored in Turtle format. 
This storage system is a file that initializes when the system comes online using the data in the application and content ontologies. 
It fills with new triplets (if there are none already in the repository) provided by the data obtained from external sources at each query. 
The reasoner. 
This element, provided by Jena (a Java framework for Semantic Web), is used for making applications based on web semantics and linked data. 
The system used in the proposed platform makes use of the possibilities of Jena for: 
• controlling the storage and recovery of the information (cities, attractions, means of transport, users, valuations) stored within the triplet storage system. 
handling the information recovered from Freebase to filter the data. 
• converting the data recovered from DBPedia as triplets. 
Jena allows the programmer to express all the queries using terms in the application ontology (a paradigm of the Semantic Web), even when the data are harvested from external sources using the corresponding content ontology. 
Thus, the planner supports the semantic integration of data from external sources. 
When SPARQL queries are made to the RDF model, the results are provided via the production of a JavaScript object notation (JSON) file.
This is returned to the browser for viewing. 
This system is open and understandable by other systems as required by the W3C Consortium standard.
3.5. Recommendation Intelligent Agent’s Module
All the users of the proposed platform are in possession of their own user agent. 
This contacts a mediating agent, the job of which is to place the user’s agent in contact with those of other users. 
This allows the receipt of valuations from these other users and thus the formation of recommendations. 
As used by [52], the proposed platform employs Java agent development framework (JADE) technology to produce the user and mediating agents. 
Communication between these agents was achieved via Foundation for Intelligent Physical Agents (FIPA) ACL standard messaging. 
The automatic recommendations produced indicate valuations for different concepts (see below) and make suggestions based on (i) the relationship of these concepts to other users with similar tastes (user-based, see below), and (ii) items. 
There are different criteria for measuring how similar two end-users are to each other (e.g., cosine similarity, L1, or L2). 
In this platform, a collaborative filtering approach has been adopted, identifying similar end-users given a user query and providing results that include item-based recommendations based on comparisons of preferences expressed by users with respect to a particular item (or concept). 
The recommendation engine that has been integrated in the platform is the open-source framework Apache Mahout. 
This software and its associated frameworks are implemented on Java and can, therefore, be executed on any modern Java virtual machine (at least Java version 6 is required), also providing easiness scalability. 
The data source model included in the platform is the CSV (fileDataModel). 
To provide recommendations, the following parameters are defined: 
• A data model: defines the input users’ data, and each line has following format: userID, ItemID, Preference_value. 
• A preference value: can be any real value; high values mean strong end-user preference. 
In the proposed model, a range between 1.0 and 10.0 was used; 
1.0 indicates little interest and 10 indicates items stored as favorites. 
• A similarity criterion: it measures the similarity between two different items and is defined by the Pearson correlation. 
• A recommender: includes the collaborative filtering recommendation model that can be defined as item–item- or user–user-based.
3.6. Semantic Web Service
A Semantic Web service is a web service enriched in meta-information for facilitating searches. 
The incorporation of semantics to web services facilitates the automation of discovery, invocation, interoperability, and service execution. 
Semantic information is integrated via the annotation of web services, which can be done using a series of technologies and frameworks, the most important of which are semantic annotation for WSDL (SAWSDL) and the WOL service (WOL-S). 
In the proposed model, the SAWSDL service was used. 
This involves a small set of extensions to WSDL XML schemas that allow easier mapping of XML-based web services to a semantic model (e.g., RDF). In fact, SAWDSL defines three XML attributes that are additions to WSDL: 
(i) modelReference, 
(ii) loweringSchemaMapping, 
and (iii) liftingSchemaMapping. 
The modelReference is used to annotate a WSDL interface with entities of a semantic data model, such as URLs for classes of an OWL ontology. 
LiftingSchemaMapping and loweringSchemaMapping are used to provide correspondence between types of XML data and semantic data models (and vice versa). 
The values of liftingSchemaMapping and loweringSchemaMapping are uniform resource identifiers (URIs) that identify documents that define the transformation of data. 
However, SAWDSL can work with a choice of languages when making correspondences. 
For example, when transformations are made between XML and RDF, the mapping necessary for “lifting transformation” can be performed using extensive stylesheet language transformation (XSLT), and SPARQL followed by XSLT used for “lowering transformations”. 
Summarizing, one of the characteristics of the architecture is its modularity making it flexible to change, guaranteeing that new functions can be incorporated inexpensively into the platform. 
This allows it to be adapted to different domains. 
To illustrate the adaptability of the architecture employed, the platform was prepared for the domains of tourism and biomedicine, offering new functionalities (downloads of the platforms are available at https://sourceforge.net/projects/touristmedicalface/, accessed on 8 August 2021), as shown in the next sections. 
The assessment of both platforms is presented in Section 6.
4. Tourism Domain Use Case
The enormous increase in information carried by the World Wide Web has made it particularly useful to travellers, who search for information associated with tourism destinations, attractions, such as museums and monuments, hotels, traveling, and restaurants. 
However, the list offered by current search engines and travel websites is usually too large, requiring the users to analyze multiple options and invest too much time to find the optimum choice [53]. Information systems that provide filtered information are, therefore, very helpful in this context. 
The proposed platform is able to analyze texts, filter information, and make practical recommendations (demo in http://youtu.be/AyPhzxbAoMc, accessed on 8 August 2021), improving users’ experience and usability. 
The tourism domain has many characteristics that make it particularly interesting for information integration and recommendation systems. 
The tourism lexicon uses a broad terminology taken from different areas (such as geography, economics, or art history), but there are also words related with specific areas, such as hotels and restaurants, transport, leisure, and entertainment. 
Therefore, a generic terminology includes large amounts of everyday vocabulary but also very specific words that include technical terms related to those areas. 
In addition, the users generate a large amount of data related to their preferences during regular use of the Web, which is very relevant to its homogenization and posterior analysis. 
The IDAP platform has been adapted for this domain and shows the following results classified by products of interest: 
• Tourist attractions: monuments, parks, museums, events, etc. 
This type of result also includes locations and events that are defined as attractions with changing locations. 
• Accommodation: hotels, bed and breakfasts, backpacker hostels, or any place to stay. 
• Travel destination: a location where a person can go for a holiday. 
Freebase has been used to retrieve tourist concept lists from texts (e.g., a tourist attraction, accommodation, or travel destination from the travel domain) and to link them with semantically related content. 
DBPedia and Expedia have also been used to harvest information on means of transport and to obtain information about hotels for the chosen destinations, respectively. 
As an extension, there is another source that can be included using the same methodology, which is the semi-structured data of TripAdvisor that offers 60 million valuations and independent opinions of users of tourist services (TripAdvisor, Inc. Needham, MA, USA, 2012). 
The implemented MLTour (Multi-Lingual Intelligent Platform for Recommendation in Tourism) is based on the one presented above (IDAP), and Figure 2 shows an example of the user interface for the entity of Barcelona (Spain). 
Particular emphasis has been placed on information enrichment and its online presentation to the user. 
The platform incorporates shallow and deep natural language processing for text analysis and components to ease the user interface generation (e.g., for mobile devices). 
The main purpose of this design is to render the integration of different components easier by grouping them into different modules. 
The modularization of the components brings together different mechanisms that allow end-users to access to the platform via HTTP protocol using the interface web or the Semantic Web service. 
The information retrieval module extracts the information from the runtime system, optimizing the online response time as well [54]. 
To adapt the IDAP platform to this domain, there is a need to define a specific application ontology that contains the terms of the following five concepts associated with the tourism domain: 
tourist attraction, travel destination, accommodation, hotel, and airport. 
For this, the Protégé (http://protege.stanford.edu/, accessed on 8 August 2021) tool has been used, an editing and ontology tool that includes a framework for the construction of intelligent systems. 
Freebase information source has also been used to extract information relevant to ‘travel_destination’, ‘accommodation’ and ‘tourist_attraction’. 
These categories provide the following fields: name, description and image. 
The ‘tourist_attraction’ information was obtained via its name, and the RDF information was consulted to provide the Freebase ID of the attraction. 
ML-Tour uses DBPedia to obtain transport information via its SPARQL endpoint and Apache Jena. Special emphasis was placed on the detection of airports associated with a specific destination. 
Elements of the type schema:Airport are sought and related to elements of the type dbpedia-owl:
PopulatedPlace via the dbpprop:cityServed property. PopulatedPlace elements must have a foaf:
name value that matches the name of the destination in order to obtain all the possible airports. 
The access to Expedia data has been achieved via the Restful API (http://developer.ean.com, accessed on 8 August 2021), and the returned XML data were transformed into RDF via XSL in order to integrate them with the data obtained from DBPedia. 
The recommendations of destinations, cities, events, or locations to visit are provided by the intelligent agent recommender. 
It is based on the similar items collaborative filtering method (using a 1–5 ascending rating scale), and it uses the comments and evaluations of users. 
Table 1 shows the association of subjective labels and the obtained scores of the evaluations.
The design of the Semantic Web service for the tourism domain was undertaken using Semantic Anotations for WSDL (SAWSDL). 
Annotations were made indicating the modelReference for the concepts City, Tourist_Attraction, Accommodation, and Transport, and the properties name, description, URL, URLimage, Transport, and webTransport. 
A sentiment analysis module was added, as proposed by [55], and it analyzes comments on TripAdvisor to obtain the sentiment as either negative or positive. 
The module is fed using the harvested information for three categories using the Mozenda (https://www. mozenda.com/, accessed on 8 August 2021) tool: 
Cities, Hotels, and Attractions. From Cities, the fields City and Comments were extracted; 
from Hotels, the fields City, Hotel, and Comments were extracted; and from Attractions, the fields City, Attraction, and Comments were extracted. 
The data are converted into XML and RDF files, adhering to Semantic Web standards. 
The output of the module is the evaluation of the sentiment on a 1–5 ascending rating scale.
5. Biomedicine Domain Use Case
The biomedicine domains, and, in particular, the subdomain of personalized medicine, require flexible information systems capable of providing accurate, up-to-date, and interrelated information based on stratified access to different sources of heterogeneous data [56]. 
The data sources that store information relevant to the effective teaching and practice of personalized medicine can be classified into three large groups: 
(i) the genomic information of individual patients, 
(ii) the medical history (electronic health records) of a patient and others like him/her, 
and (iii) large biomedical databases (general and specific) able to provide information valuable in the personalization of treatment. 
The resources offered by the US National Institutes of Health (http://www.nih.gov, accessed on 8 August 2021) (NIH) and by the National Library of Medicine (http://www.nlm. nih.gov/pubs/factsheets/nlm.html, accessed on 8 August 2021) (NLM) (which belongs to the NIH) include Healthfinder (http://www.healthfinder.gov, accessed on 8 August 2021) and Medline Plus, (http://www.nlm.nih.gov/medlineplus, accessed on 8 August 2021) both of which are available in English and Spanish. 
The resources offered to health professionals include a free search engine designed to seek out biomedical research papers (PubMed Central (http://www.ncbi.nlm.nih.gov/pmc, accessed on 8 August 2021)). 
The proposed platform makes use of Freebase, Medline Plus, and PubMed and is able to categorize texts using documents held at the Pathology Department of Pittsburgh University (path.upmc.edu/cases.html, accessed on 8 August 2021). 
Medline has over 18 million references to articles, covering the period from 1950 to the present. 
It grows by 2000–4000 new references every day, and it is impossible for even the greatest of experts to keep up with all of the information being published in their fields. 
However, the information required for personalized medicine to be effective is not limited to that held within scientific publications;
information provided by medical histories and medical reports is also required. 
The sources of such information are very different, and the documents they contain may differ widely in their structure. 
For example, they may contain free text or tables of experimental results, they may differ in length, and may be written in different languages. 
The proposed platform uses the ontology Medlineplus Health Topics to access to categorize data by the identification of relevant concepts and integrates it with the knowledge on these concepts available on Freebase. 
Texts are processed using computational linguistic techniques, indicating whether a disease, symptom, or treatment can be referred by Medline Plus, providing additional information relevant to them. 
The result is a system that shortens the time needed by the end-user to understand a text of interest and increase the information about it as demanded. 
An earlier implementation of this module was reported and tested, obtaining successful results [57]. 
The implemented CLEiM (Cross Lingual Intelligent Platform for Education in Medicine) is based on the one presented above (IDAP), and it incorporates some changes to adapt it. 
Figure 3 shows the interface of the platform.
The generic NLP, recognized concepts modules, and Freebase were used to obtain categories of diseases, symptoms, and treatments and the domain medicine to identify concepts in the examined articles. 
The categories used in the present demonstration were: 
Heart, Back, Eye, Brain, and Lung. 
Each category was provided with a set of frequent words, e.g., the category ‘Brain’ was provided with the words: concentration, planning, motor, frontal, broca, lateral sulcus, auditory area, memory, or temporal lobe. 
During a search query, the categorisation process uses a similarity algorithm based on TF–IDF (term frequency–inverse data frequency) vectorization of the text to obtain the most relevant blocks of information for that query.
6. Assessment of the Platform and Test of Adaptability
6.1. Assessment of the Platform in the Tourism Domain
Data quality is desirable for any information system; good quality data are required if a system is to provide any reliable output. 
The problem is particularly acute for information integration systems, which harvest information from different sources with different degrees of reliability. 
Keeping data up to date and the maintenance of consistency and traceability are ever-harder challenges. 
In this context, the quality of an information system can be measured in terms of several dimensions, including originality, accuracy [58], completeness, and reliability [59,60]. 
Measurements can be made using formal or informal methods. 
Formal methods generally provide a value expressed in a unit of measurement, or a mathematical expression, while informal methods may provide a score range or a qualitative opinion assigned by a user or designer. 
In [61], the authors established the dimensions of ‘ability to identify necessary data’, ‘accessibility of information’, ‘possibility of integration’, and ‘interpretation of data’. 
We have employed this approach for our assessment of the systems.
6.1.1. Assessment of the Platform: Methodology
Based on previous work [62,63], an A/B test assessment system was designed using questionnaires that collect information on end-user satisfaction with the platform, its usability, and its effectiveness. 
The evaluation of the system is proposed as an activity carried out voluntarily by students in the last year of a degree in computer science. 
Most of them have some previous professional experience and are sufficiently knowledgeable to understand what a system of this type entails. 
A total of 14 students were presented to the activity, which is considered sufficiently representative based on the low number of participants in evaluations in similar research projects, such as those seen in [63]. 
All the individuals received a 15-minute presentation to explain to them the platform in order to make them aware that it was an information integration system extended with a personalized recommendation system. 
The explanation included the information sources that the platform uses. 
For the first assessment, the subjects undertook a requested 20-minute task (Task A; see Appendix A) for the tourism domain. 
The goal of this exercise was to obtain how long the searches took and to compare the usability of a commercial (www.visiteurope.com, accessed on 8 August 2021) and the proposed platforms. 
A second assessment was performed after dividing the subjects into two groups for the A/B test. 
It was a 20-minute task (Task B; see Appendix A), and group A used an existing platform (YouTube), while group B used the proposed platform. 
Then, they were asked to fill in the questionnaire consisting of 15 questions, divided into two parts. 
The first part (nine questions, Q1–Q9, see Appendix B), Task A, on usability, capacity, intention, and confidence, referred to the end-user perception of the platform, while the second (six questions, Q10–Q15, see Appendix B), Task B, was about how the user felt about the functionalities related to NLP and concept recognition, information integration and sources, depth of extra information, basic recommendations, advanced suggestions, and concept evaluation entered into specific perceptions regarding the different modules making up the platform. 
All the questions were Likert type [64] using an ascending rating scale of 1–5 (strongly agree, agree, neutral, disagree, strongly agree).
6.1.2. Task A Results: End-User Usability and Capabilities Experience
Shown in Figure 4a,b are the responses obtained to questions Q1–9 (Task A) of the questionnaire. 
Overall, the results obtained for the end-user experience were positive. 
The lowest median and mode scores for the different questions were neutral, and, hence, we can affirm that most of the subjects believed that the platform helped them during the performance of the task. 
In contrast, the visiteurope.com platform received median and mode scores of strongly disagree for some questions (Q2, Q3, Q5, Q7, Q8). 
It can be observed that the users have agreed or totally agreed that the proposed platform provides optimal functionalities and capabilities for all the questions but for Q1 (is the interface friendly?), which may be due to the visual design rather than the human– machine interaction process. 
That is, none of the subjects agreed or totally agreed that the interface was friendly (question 1); 
50% agreed, and 14% totally agreed, that the system made information easy to find (question 2); 21% agreed, and 21% totally agreed, that they were comfortable using the platform (question 3); 
29% agreed, and 29% totally agreed, that the platform velocity was reasonable (question 4); 
and 36% agreed, and 21% totally agreed, that it was easy to learn how to use the platform. 
With respect to the capacity of the platform, 36% agreed, and 21% totally agreed, that the platform helped them find information (question 6); 
and 43% agreed, and 14% totally agreed, that the platform was faster than a traditional search system (question 7). 
With respect to intention, 43% agreed, and 14 totally agreed, that they would use the platform again.
For comparison purposes, shown in Figure 4c,d are the results obtained for the same Task A when individuals used the website www.visiteurope.com (accessed on 8 August 2021).
6.1.3. Task A Results: Platform Functionalities
Table 2 shows the obtained results for the functionality of the platform (Q10–Q15 of Appendix B). 
This evaluation focuses on the following functionalities: 
• Natural language processing and concept recognition (Q10). 
• Information integration sources (Q11). 
The system recovers information from different sources (Freebase, DBPedia, Expedia, and Trip Advisor). 
• Depth of information (Q12). 
The system shows detailed information on different concepts. 
• Basic recommendation (Q13). 
The system indicates whether a concept is recommendable or not. 
• Advanced suggestions (Q14). 
Depending on user preferences, the system shows new recommendations. 
• Concept evaluation (Q15). 
The end-user can provide feedback to the system and to his/her own profile through concept evaluation.
The values included in the table represent the sum of the percentages of individuals who agreed or strongly agreed with each of the functionalities. 
We can conclude that the end-users were satisfied with the functionality of the platform.
6.1.4. Task B Results: End-User Enrichment Functionalities Experience
The results obtained showed that Group A individuals made a conceptual map using an average of 17.1 concepts obtained from YouTube, while the Group B subjects made a conceptual map using an average of 15.5 concepts obtained using the proposed platform. 
Only 16.7% of the Group A subjects completed the task in the allotted 20 minutes using YouTube, whereas 37.5% completed the task using the proposed platform—nearly twice—and using a similar number of concepts in both cases. 
When they were asked about their resulting motivation to visit the destination in question or to search for further information on it, the Group A subjects returned a median and mode score of 2 (disagree) for both questions, while the Group B subjects returned a median and mode score of 4 (agree) for both questions.
6.2. Assessment of the Platform in the Biomedical Domain
For this domain, an evaluation has been conducted in order to gather degree teachers’ opinions about the benefits of using complementary methods supported by intelligent systems. These active learning methods provide some benefits by processing texts written in natural language that help teachers to find use cases and explore similar activities in their daily teaching experience. The study uses a mixed methodology, making use of closed questions and open ones to enable quantitative and qualitative analysis. 
The questionnaire is designed into five categories: 
(i) general information and knowledge of active teaching methods, 
(ii) knowledge related to concept annotation,
(iii) knowledge related to cross-lingual concept extraction, 
(iv) knowledge related to concept/mental maps, 
and (v) arrangement of the evaluated IIA systems in decreasing order by perceived usefulness with a total of 66 questions [65]. 
The study gathered information to elucidate how useful the system is. 
Moreover, the proposed system, CLEiM, was compared with others of the biomedical area: 
BioAnnote (http://sing.ei.uvigo.es/bioannote/demo.html, accessed on 8 August 2021) and MedCMap [66]. 
We selected 11 teachers with different training and experience backgrounds, all of them related with Faculty of Bio-Science (BS) and Health Science (HS) at Universidad Europea de Madrid (UEM). 
Of the 11 BS and HS teachers interviewed, nine were women and two were men. 
All of them taught degree classes (seven in the Faculty of BS, two in the Faculty of HS, and two in both). 
Six taught medicine, five taught dentistry, four taught physiotherapy, one each taught pharmacy, optometry, and nursing. 
The mean age of these respondents was 42.55 years (SD = 7.34; range 30–52 years). 
The mean number of years of teaching experience was 10.36 years (SD = 7.50; range 2–24 years) [65]. 
The use of active learning methods was not common, and only one individual used sophisticated software (for statistical analysis). 
Most of the teachers used bibliographic resources for the preparation of the activities, the most common being PubMed/Medline and Ocenet or Elsevier. 
None of the individuals used intelligent systems, or at least domain specific ones, but general tools for searching clinical practices, such as YouTube or a Google search. 
This also led to the conclusion that using specific tools for searching domain concepts is very useful for preparing and executing CBL activities and may also be relevant for brighter students to make it easier for them to find information (active student learning). 
Figure 5 shows the results for testing the perceptions of the three systems in this case-based learning context (CBL). 
It shows the classification of the three systems (i.e., first, second, and third place) by the respondents in terms of their perceived usefulness in teaching. 
The number that appears above the bar plots indicates the number of respondents who ranked each of the systems first (left), second (middle), and third (right).
Despite none of the teachers knowing about the natural language processing capabilities, they found their use interesting for preparing and executing learning activities. 
Table 3 shows the responses to the perceived utility of the systems, CLEiM being valued well by the users.
The present work proposes and defines an architecture for the integration of information— structured, semi-structured, and unstructured—from heterogeneous sources and provides complementary information access via recommendations. 
The platform brings together different technologies, including the automatic processing of texts, the recognition of named concepts, sentiment analysis, and the use of intelligent agents. 
In addition, the architecture of the platform adheres to Semantic Web standards according to W3C criteria. 
The result is an agile, modular system that is easily adaptable to different domains and offers access to complementary information and knowledge for analyzing complex texts. 
These characteristics have been tested on searching and learning tasks for tourism and biomedicine domains. 
The obtained results prove that end-users were able to perform the search tasks with a 2:1 ratio of success versus other baseline systems, and the end user perception on usability and capabilities greatly outperformed other systems. 
The end user experience regarding the enrichment functionalities was also positively evaluated, indicating an adequate selection of the recommended complementary information. 
After all these evaluations, we can also conclude that the platform can be useful to the general population, expert groups, and researchers.
Shifting machine learning for healthcare from development to deployment and from models to data
In the past decade, the application of machine learning (ML) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. 
his progress has emphasized that, from model development to model deployment, data play central roles. 
In this Review, we provide a data-centric view of the innovations and challenges that are defining ML for healthcare. 
We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. 
We also discuss data-focused problems in the deployment of ML, emphasizing the need to efficiently deliver data to ML models for timely clinical predictions and to account for natural data shifts that can deteriorate model performance. 
In the past decade, machine learning (ML) for healthcare has been marked by particularly rapid progress. 
Initial groundwork has been laid for many healthcare needs that promise to improve patient care, reduce healthcare workload, streamline healthcare processes and empower the individual1. 
In particular, ML for healthcare has been successful in the translation of computer vision through the development of image-based triage2 and second readers3. 
There has also been rapid progress in the harnessing of electronic health records4,5 (EHRs) to predict the risk and progression of many diseases6,7. 
A number of software platforms for ML are beginning to make their way into the clinic8 .
In 2018, iDX-DR, which detects diabetic retinopathy, was the first ML system for healthcare that the United States Food and Drug Administration approved for clinical use8. 
Babylon9 , a chatbot triage system, has partnered with the United Kingdom’s National Healthcare system. 
Furthermore, Viz.ai10,11 has rolled out their triage technology to more than 100 hospitals in the United States. 
As ML systems begin to be deployed in clinical settings, the defining challenge of ML in healthcare has shifted from model development to model deployment. 
In bridging the gap between the two, another trend has emerged: the importance of data. 
We posit that large, well-designed, well-labelled, diverse and multi-institutional datasets drive performance in real-world settings far more than model optimization12–14, and that these datasets are critical for mitigating racial and socioeconomic biases15. 
We realize that such rich datasets are difficult to obtain, owing to clinical limitations of data availability, patient privacy and the heterogeneity of institutional data frameworks. 
Similarly, as ML healthcare systems are deployed, the greatest challenges in implementation arise from problems with the data: 
how to efficiently deliver data to the model to facilitate workflow integration and make timely clinical predictions? 
Furthermore, once implemented, how can model robustness be maintained in the face of the inevitability of natural changes in physician and patient behaviours? 
In fact, the shift from model development to deployment is also marked by a shift in focus: from models to data. 
In this Review, we build on previous surveys1,16,17 and take a data-centric approach to reviewing recent innovations in ML for healthcare. 
We first discuss deep generative models and federated learning as strategies for creating larger and enhanced datasets. 
We also examine the more recent transformer models for handling larger datasets. 
We end by highlighting the challenges of deployment, in particular, how to process and deliver usable raw data to models, and how data shifts can affect the performance of deployed models.
Deep generative models
Generative adversarial networks (GANs) are among the most exciting innovations in deep learning in the past decade. 
They offer the capability to create large amounts of synthetic yet realistic data. 
In healthcare, GANs have been used to augment datasets18, alleviate the problems of privacy-restricted19 and unbalanced datasets20, and perform image-modality-to-image-modality translation21 and image reconstruction22 (Fig. 1). 
GANs aim to model and sample from the implicit density function of the input data23. 
They consist of two networks that are trained in an adversarial process under which one network, the ‘generator’, generates synthetic data while the other network, the ‘discriminator’, discriminates between real and synthetic data. 
The generative model aims to implicitly learn the data distribution from a set of samples to further generate new samples drawn from the learned distribution, while the discriminator pushes the generator network to sample from a distribution that more closely mirrors the true data distribution. 
Over the years, a multitude of GANs have been developed to overcome the limitations of the original GAN (Table 1), and to optimize its performance and extend its functionalities. 
The original GAN23 suffered from unstable training and low image diversity and quality24. 
In fact, training two adversarial models is, in practice, a delicate and often difficult task. 
The goal of training is to achieve a Nash equilibrium between the generator and the discriminator networks. 
However, simultaneously obtaining such an equilibrium for networks that are inherently adversarial is difficult and, if achieved, the equilibrium can be unstable (that is, it can be suddenly lost after model convergence). 
This has also led to sensitivity to hyperparameters (making the tuning of hyperparameters a precarious endeavour) and to mode collapse, which occurs when the generator produces a limited and repeated number of outputs. 
To remedy these limitations, changes have been made to GAN architectures and loss functions. 
In particular, the deep convolutional GAN (DCGAN25), a popular GAN often used for medical-imaging tasks, aimed to combat instability by introducing key architecture-design decisions, including the replacement of fully connected layers with convolutional layers, and the introduction of batch normalization (to standardize the inputs to a layer when training deep neural networks) and ReLU (rectified linear unit) activation. 
The Laplacian pyramid of adversarial networks GAN (LAPGAN26) and the progressively growing GAN (ProGAN27) build on DCGAN to improve training stability and image quality. 
Both LAPGAN and ProGAN start with a small image, which promotes training stability, and progressively grow the image into a higher-resolution image. 
The conditional GAN (cGAN28) and the auxiliary classifier GAN (AC-GAN29) belong to a subtype of GANs that enable the model to be conditioned on external information to create synthetic data of a specific class or condition. 
This was found to improve the quality of the generated samples and increase the capability to handle the generation of multimodal data. 
The pix2pix GAN30, which is conditioned on images, allows for image-to-image translation (also across imaging modalities) and has been popular in healthcare applications. 
A recent major architectural change to GANs involve attention mechanisms. Attention was first introduced to facilitate language translation and has rapidly become a staple in deep-learning models, as it can efficiently capture longer-range global and spatial relations from input data. 
The incorporation of attention into GANs has led to the development of self-attention GANs (SAGANs)31,32 and BigGAN;33; the latter scales up SAGAN to achieve state-of-the-art performance. 
Another primary strategy to mitigate the limitations of GANs involves improving the loss function. 
Early GANs used the Jensen-Shannon divergence and the Kullback-Leibler divergence as loss functions to minimize the difference in distribution between the synthetic generated dataset and the real-data dataset. 
However, the Jensen-Shannon divergence was found to fail in scenarios where there is no overlap (or little overlap) between distributions, while the minimization of the Kullback-Leibler divergence can lead to mode collapse. 
To address these problems, a number of GANs have used alternative loss functions. 
The most popular are arguably the Wasserstein GAN (WGAN34) and the Wasserstein GAN gradient penalty (WGAN-GP35). 
The Wasserstein distance measures the effort to minimize the distance between dataset distributions and has been shown to have a smoother gradient. 
Additional popular strategies that have been implemented to improve GAN performance and that do not involve modifying the model architecture include spectral normalization and varying how frequently the discriminator is updated (with respect to the update frequency of the generator). 
The explosive progress of GANs has spawned many more offshoots of the original GAN, as documented by the diverse models that now populate the GAN Model Zoo.
Augmenting datasets. In the past decade, many deep-learning models for medical-image classification3,37, segmentation38,39 and detection40 have achieved physician-level performance.
However, the success of these models is ultimately beholden to large, diverse, balanced and well-labelled datasets. 
This is a bottleneck that extends across domains, yet it is particularly restrictive in healthcare applications where collecting comprehensive datasets comes with unique obstacles. 
In particular, large amounts of standardized clinical data are difficult to obtain, and this is exacerbated by the reality that clinical data often reflects the patient population of one or few institutions (with the data sometimes overrepresenting common diseases or healthy populations and making the sampling of rarer conditions more difficult). 
Datasets with high class imbalance or insufficient variability can often lead to poor model performance, generalization failures, unintentional modelling of confounders41 and propagation of biases42. 
To mitigate these problems, clinical datasets can be augmented by using standard data-manipulation techniques, such as the flipping, rotation, scaling and translation of images43. 
However, these methods can lead to limited increases in performance and generate highly correlated training data. 
GANs offer potent solutions to these problems. 
GANs can be used to augment training data to improve model performance. 
For example, a convolutional neural network (CNN) for the classification of liver lesions, trained on both synthetically and traditionally augmented data, boosted the performance of the model by 10% with respect to a CNN trained on only traditionally augmented datasets18. 
Moreover, when generating synthetic data across data classes, developing a generator for each class can result in higher model performance20,44, as was shown via the comparison of two variants of GANs (a DCGAN that generated labelled examples for each of three lesion classes separately and an AC-GAN that incorporated class conditioning to generate labelled examples)18. 
The aforementioned studies involved class-balanced datasets but did not address medical data with either simulated or real class imbalances. 
In an assessment of the capability of GANs to alleviate the shortcomings of unbalanced chest-X-ray datasets20, it was found that training a classifier on real unbalanced datasets that had been augmented with DCGANs outperformed models that were trained with the unbalanced and balanced versions of the original dataset. 
Although there was an increase in classification accuracy across all classes, the greatest increase in performance was seen in the most imbalanced classes (pneumothorax and oedema), which had just one-fourth the number of training cases as the next class. 
Protecting patient privacy. 
The protection of patient privacy is often a leading concern when developing clinical datasets45. 
Sharing patient data when generating multi-institution clinical datasets can pose a risk to patient privacy46. 
Even if privacy protocols are followed, patient characteristics can sometimes be inferred from the ML model and its outputs47,48. 
In this regard, GANs may provide a solution. D
ata created by GANs cannot be attributed to a single patient, as they synthesize data that reflect the patient population in aggregate. 
GANs have thus been used as a patient-anonymization tool to generate synthetic data for model training9,49. 
Although models trained on just synthetic data can perform poorly, models trained on synthetic data and fine-tuned with 10% real data resulted in similar performance to models trained on real datasets augmented with synthetic data19. 
Similarly, using synthetic data generated from GANs to train an image-segmentation model was sufficient to achieve 95% of the accuracy of the same model trained on real data49. 
Hence, using synthetic data during model development can mitigate potential patient-privacy violations. 
Image-to-image translation. 
One exciting use of GANs involves image-to-image translation. 
In healthcare, this capability has been used to translate between imaging modalities—between computed tomography (CT) and magnetic resonance (MR) images21,49–51, between CT and positron emission tomography (PET)52–54, between MR and PET55–57, and between T1 and T2 MR images58,59. 
Transfer between image modalities can reduce the need for additional costly and time-intensive image acquisitions, can be used in scenarios where imaging is not possible (as is the case for MR imaging in individuals with metal implants) and to expand the types of training data that can be created from image datasets. 
There are two predominant strategies for image-to-image translation: paired-image training (with pix2pix30) and unpaired training (with CycleGAN60). 
For example, pix2pix was used to generate synthetic CT images for accurate MR-based dose calculations for the pelvis61. 
Similarly, using paired magnetic resonance angiography and MR images, pix2pix was modified to generate a model for the translation of T1 and T2 MR images to retrospectively inspect vascular structures62. 
Obtaining paired images can be difficult in scenarios involving moving organs or multimodal medical images that are in three dimensions and do not have cross-modality paired data. 
In such cases, one can use CycleGAN60, which handles image-to-image translation on unpaired images. 
A difficulty with unpaired images is the lack of ground-truth labels for evaluating the accuracy of the predictions (yet real cardiac MR images have been used to compare the performance of segmentation models trained on synthetic cardiac MR images translated from CT images49). 
Another common problem is the need to avoid geometric distortions that destroy anatomical structures. 
Limitations with geometric distortions can be overcome by using two auxiliary mappings to constrain the geometric invariance of synthetic data21. 
Opportunities. 
In the context of clinical datasets, GANs have primarily been used to augment or balance the datasets, and to preserve patient privacy. 
Yet a burgeoning application of GANs is their use to systematically explore the entire terrain of clinical scenarios and disease presentations. 
Indeed, GANs can be used to generate synthetic data to combat model deterioration in the face of domain shifts63,64, for example, by creating synthetic data that simulate variable lighting or camera distortions, or that imitate data collected from devices from different vendors or from different imaging modalities. 
Additionally, GANs can be used to create data that simulate the full spectrum of clinical scenarios and disease presentations, from dangerous and rare clinical scenarios such as incorrect surgery techniques63, to modelling the spectrum of brain-tumour presentation19, to exploring the disease progression of neurodegenerative diseases65,66. 
However, GANs can suffer from training instability and low image diversity and quality. 
These limitations could hamper the deployment of GANs in clinical practice. 
For example, one hope for image-to-image translation in healthcare involves the creation of multimodality clinical images (from CT and MR, for example) for scenarios in which only one imaging modality is possible. 
However, GANs are currently limited in the size and quality of the images that they can produce. 
This raises the question of whether these images can realistically be used clinically when medical images are typically generated at high resolution. 
Moreover, there may be regulatory hurdles involved in approving ML healthcare models that have been trained on synthetic data. 
This is further complicated by the current inability to robustly evaluate and control the quality of GANs and of the synthetic data that they generate67. Still, in domains unrelated to healthcare, GANs have been used to make tangible improvements to deployed models68. 
These successes may lay a foundation for the real-world application of GANs in healthcare.
Federated learning
When using multi-institutional datasets, model training is typically performed centrally: data siloed in individual institutions are aggregated into a single server. 
However, data used in such ‘centralized training’ represent a fraction of the vast amount of clinical data that could be harnessed for model development. 
Yet, openly sharing and exchanging patient data is restricted by many legal, ethical and administrative constraints; in fact, in many jurisdictions, patient data must remain local. 
Federated learning is a paradigm for training ML models when decentralized data are used collaboratively under the orchestration of a central server69,70 (Fig. 2). 
In contrast to centralized training, where data from various locations are moved to a single server to train the model, federated learning allows for the data to remain in place. 
At the start of each round of training, the current copy of the model is sent to each location where the training data are stored. 
Each copy of the model is then trained and updated using the data at each location. 
The updated models are then sent from each location back to the central server, where they are aggregated into a global model. 
The subsequent round of training follows, the newly updated global model is distributed again, and the process is repeated until model convergence or training is stopped. 
At no point do the data leave a particular location or institution, and only individuals associated with an institution have direct access to its data. 
This mitigates concerns about privacy breaches, minimizes costs associated with data aggregation, and allows training datasets to quickly scale in size and diversity. 
The successful implementation of federated learning could transform how deep-learning models for healthcare are trained. 
Here we focus on two applications: cross-silo federated learning and cross-device federated learning (Table 2). 
Cross-silo federated learning. Cross-silo federated learning is an increasingly attractive solution to the shortcomings of centralized training71. 
It has been used to leverage EHRs to train models to predict hospitalization due to heart disease72, to promote the development of ‘digital twins’ or ‘Google for patients’73, and to develop a Coronavirus disease 2019 (COVID-19) chest-CT lesion segmenter74. 
Recent efforts have focused on empirically evaluating model-design parameters, and on logistical decisions to optimize model performance and overcome the unique implementation challenges of federated learning, such as bottlenecks in protecting privacy and in tackling the statistical heterogeneity of the data75,76. 
Compared with centralized training, one concern of federated learning is that models may encounter more severe domain shifts or overfitting. 
However, models trained through federated learning were found to achieve 99% of the performance of traditional centralized training even with imbalanced datasets or with relatively few samples per institution, thus showing that federated learning can be realistically implemented without sacrificing performance or generalization77,78. 
Although federated learning offers greater privacy protection because patient data are no longer being transmitted, there are risks of privacy breaches79. 
Communicating model updates during the training process can reveal sensitive information to third parties or to the central server. 
In certain instances, data leakage can occur, such as when ML models ‘memorize’ datasets80–82 and when access to model parameters and updates can be used to infer the original dataset83. 
Differential privacy84 can further reinforce privacy protection for federated learning70,85,86. 
Selective parameter sharing87 and the sparse vector technique88 are two strategies for achieving greater privacy, but at the expense of model performance (this is consistent with differential-privacy findings in domains outside of medicine and healthcare80,89). 
Another active area of research for federated learning in healthcare involves the handling of data that are neither independent nor identically distributed (non-IID data). 
Healthcare data are particularly susceptible to this problem, owing to a higher prevalence of certain diseases in certain institutions (which can cause label-distribution skew) or to institution-specific data-collection techniques (leading to ‘same label, different features’ or to ‘same features, different label’). 
Many federated learning strategies assume IID data, but non-IID data can pose a very real problem in federated learning; 
for example, it can cause the popular federated learning algorithm FedAvg70 to fail to converge90. 
The predominant strategies for addressing this issue have involved the reframing of the data to achieve a uniform distribution (consensus solutions) or the embracing of the heterogeneity of the data69,91,92 (pluralistic solutions). 
In healthcare, the focus has been on consensus solutions involving data sharing (a small subset of training data is shared among all institutions).
Cross-device federated learning to handle health data from individuals. 
‘Smart’ devices can produce troves of continuous, passive and individualized health data that can be leveraged to train ML models and deliver personalized health insights for each user1,16,39,95,96. 
As smart devices become increasingly widespread, and as computing and sensor technology become more advanced and cheaper to mass-produce, the amount of health data will grow exponentially. 
This will accentuate the challenges of aggregating large quantities of data into a single location for centralized training and exacerbate privacy concerns (such as any access to detailed individual health data by large corporations or governments).
Cross-device federated learning was developed to address the increasing amounts of data that are being generated ‘at the edge’ (that is, by decentralized smart devices), and has been deployed on millions of smart devices; 
for example, for voice recognition (by Apple, for the voice assistant Siri97) and to improve query suggestions (by Google, for the Android operating system98). 
The application of cross-device federated learning to train healthcare models for smart devices is an emerging area of research. 
For example, using a human-activity-recognition dataset, a global model (FedHealth) was pre-trained using 80% of the data before deploying it to be locally trained and then aggregated99. 
The aggregated model was then sent back to each user and fine-tuned on user-specific data to develop a personalized model for the user. 
Model personalization resolves issues arising from the highly different probability distributions that may arise across users and the global model. 
This training strategy outperformed non-federated learning by nearly 5.3%. Limitations and opportunities. 
In view of the initial promises and successes of federated learning, the next few years will be defined by progress towards the implementation of federated learning in healthcare. 
This will require a high degree of coordination across institutions at each step of the federated learning process. 
Before training, medical data will need to undergo data normalization and standardization. 
This can be challenging, owing to differences in how data are collected, stored, labelled and partitioned across institutions. 
Current data pre-processing pipelines could be adapted to create multi-institutional training datasets, yet in federated learning, the responsibility shifts from a central entity to each institution individually. 
Hence, methods to streamline and validate these processes across institutions will be essential for the successful implementation of federated learning. 
Another problem concerns the inability of the developer of the model to directly inspect data during model development. 
Data inspection is critical for troubleshooting and for identifying any mislabelled data as well as general trends. 
Tools (such as Federated Analytics, developed by Google100) that use GANs to create synthetic data that resemble the original training data101 and derive population-level summary statistics from the data, can be helpful. 
However, it is currently unclear whether tools that have been developed for cross-device settings can be applied to cross-silo healthcare settings while preserving institutional privacy. 
Furthermore, federated learning will require robust frameworks for the implementation of federated networks. 
Many such software is proprietary, and many of the open-source frameworks are primarily intended for use in research. 
The primary concerns of federated learning can be addressed by frameworks designed to reinforce patient privacy, facilitate model aggregation and tackle the challenges of non-IID data. 
One main hurdle is the need for each participating healthcare institution to acquire the necessary infrastructure. 
This implies ensuring that each institution has the same federated learning framework and version, that stable and encrypted network communication is available to send and receive model updates from the central server, and that the computing capabilities (institutional graphics processing units or access to cloud computing) are sufficient to train the model. 
Although most large healthcare institutions may have the necessary infrastructure in place, it has typically been optimized to store and handle data centrally. 
The adaptation of infrastructure to handle the requirements of federated learning requires coordinated effort and time. 
A number of ongoing federated learning initiatives in healthcare are underway. 
Specifically, the Federated Tumour Segmentation Initiative (a collaboration between Intel and the University of Pennsylvania) trains lesion-segmentation models collaboratively across 29 international healthcare institutions102. 
This initiative focuses on finding the optimal algorithm for model aggregation, as well as on ways to standardize training data from various institutions. 
In another initiative (a collaboration of NVIDIA and several institutions), federated learning was used to train mammography-classification models103. 
These efforts may establish blueprints for coordinated federated networks applied to healthcare.
Natural language processing
Harnessing natural language processing (NLP)—the automated understanding of text—has been a long-standing goal for ML in healthcare1,16,17. 
NLP has enabled the automated translation of doctor–patient interactions to notes5,104,105, the summarization of clinical notes106, the captioning of medical images107,108 and the prediction of disease progression6,7. 
However, the inability to efficiently train models using the large datasets needed to achieve adept natural-language understanding has limited progress. 
In this section, we provide an overview of two recent innovations that have transformed NLP: 
transformers and transfer learning for NLP. 
We also discuss their applications in healthcare. 
Transformers. 
When modelling sequential data, recurrent neural networks (RNNs) have been the predominant choice of neural network. 
In particular, long short-term memory networks109 and gated units110 were staple RNNs in modelling EHR data, as these networks can model the sequential nature of clinical data111,112 and clinical text5,104,105,113. 
However, RNNs harbour several limitations114. Namely, RNNs process data sequentially and not in parallel. 
This restricts the size of the input datasets and of the networks, which limits the complexity of the features and the range of relations that can be learned114. 
Hence, RNNs are difficult to train, deploy and scale, and are suboptimal for capturing long-range patterns and global patterns in data. 
However, learning global or long-range relationships are often needed when learning language representations. 
For example, sentences far removed from a word may be important for providing context for the word, and previous clinical events that have occurred can inform clinical decisions that are made years later. 
For a period, CNNs, which are adept at parallelization, were used to overcome some of the limitations of RNNs115, but were found to be inefficient when modelling longer global dependencies. 
In 2017, a research team at Google (the Google Brain team) released the transformer, a landmark model that has revolutionized NLP116. 
Compared with RNN and CNN models, transformers are more parallelizable and less computationally complex at each layer, and thus can handle larger training data and learn longer-range and global relations. 
The use of only attention layers for the encoders and decoders while forgoing the use of RNNs or CNNs was critical to the success of transformers. 
Attention was introduced and refined117,118 to handle bottlenecks in sequence-to-sequence RNNs110,119. 
Attention modules allow models to globally relate different positions of a sequence to compute a richer representation of the sequence116, and does so in parallel, allowing for increased computing efficiency and for the embedding of longer relations of the input sequence (Fig. 3). 
Transfer learning for NLP. 
Simultaneous and subsequent work following the release of the transformer resolved another main problem in NLP: 
the formalization of the process of transfer learning. 
Transfer learning has been used most extensively in computer vision, owing to the success of the ImageNet challenge, which made pre-trained CNNs widely available120. 
Transfer learning has enabled the broader application of deep learning in healthcare17, as researchers can fine-tune a pre-trained CNN adept at image classification on a smaller clinical dataset to accomplish a wide spectrum of healthcare tasks3,37,121,122. 
Until recently, robust transfer learning for NLP models was not possible, which limited the use of NLP models in domain-specific applications. 
A series of recent milestones have enabled transfer learning for NLP. 
The identification of the ideal pre-training language task for deep-learning NLP models (for example, masked-language modelling, predicting missing words from surrounding context, next-sentence prediction or predicting whether two sentences follow one another) was solved by universal language model fine-tuning (ULM-FiT123) and embeddings from language model (ELMo124). 
The generative pre-trained transformer (GPT125) from Open AI and the bidirectional encoder representations from transformers (BERT126) from Google Brain then applied the methods formalized by ULM-FiT and ELMo to transformer models, delivering pre-trained models that achieved unprecedented capabilities on a series of NLP tasks. 
Transformers for the understanding of clinical text. 
Following the success of transformers for NLP, their potential to handle domain-specific text, specifically clinical text, was quickly assessed. 
The performances of the transformer-based model BERT, the RNN-based model ELMo and traditional word-vector embeddings127,128 at clinical-concept extraction (the identification of the medical problems, tests and treatments) from EHR data were evaluated106. 
BERT outperformed traditional word vectors by a substantial margin and was more computationally efficient than ELMo (it achieved higher performance with fewer training iterations)129–132. 
Pre-training on a dataset of 2 million clinical notes (the dataset multiparameter intelligence monitoring in intensive care132; MIMIC-III) increased the performance of all NLP models. 
This suggests that contextual embeddings encode valuable semantic information not accounted for in traditional word representations106. 
However, the performance of MIMIC-III BERT began to decline after achieving its optimal model; this is perhaps indicative of the model losing information learned from the large open corpus and converging to a model similar to the one initialized from scratch106. 
Hence, there may be a fine balance between learning from a large open-domain corpus and a domain-specific clinical corpus. This may be a critical consideration when applying pre-trained models to healthcare tasks. 
To facilitate the further application of clinically pre-trained BERT129 to downstream clinical tasks, a BERT pre-trained on large clinical datasets was publicly released. 
Because transformers and deep NLP models are resource-intensive to train (training the BERT model can cost US$50,000–200,000133; 
and pre-training BERT on clinical datasets required 18d of continuous training, an endeavour that may be out of the reach of many institutions), openly releasing pre-trained clinical models can facilitate widespread advancements of NLP tasks in healthcare. 
Other large and publicly available clinically pre-trained models (Table 3) are ClinicalBERT130, BioBERT134 and SciBERT135. 
The release of clinically pre-trained models has spurred downstream clinical applications. 
ClinicalBERT, a BERT model pre-trained on MIMIC-III data using masked-language modelling and next-sentence prediction, was evaluated on the downstream task of predicting 30d readmission130. 
Compared with previous models136,137, ClinicalBERT can dynamically predict readmission risk during a patient’s stay and uses clinical text rather than structured data (such as laboratory values, or codes from the international classification of diseases). 
This shows the power of transformers to unlock clinical text, a comparatively underused data source in EHRs. 
Similarly, clinical text from EHRs has been harnessed using SciBERT for the automated extraction of symptoms from COVID-19-positive and COVID-19-negative patients to identify the most discerning clinical presentation138. 
ClinicalBERT has also been adapted to extract anginal symptoms from EHRs.
Others have used enhanced clinical-text understanding for the automatic labelling and summarization of clinical reports. 
BioBERT and ClinicalBERT have been harnessed to extract labels from radiology text reports, enabling an automatic clinical summarization tool and labeller140. 
Transformers have also been used to improve clinical questioning and answering141, in clinical voice assistants142,143, in chatbots for patient triage144,145, and in medical-image-to-text translation and medical-image captioning. 
Transformers for the modelling of clinical events. 
In view of their adeptness to model the sequential nature of clinical text, transformers have also been harnessed to model the sequential nature of clinical events147–151. 
A key challenge of modelling clinical events is properly capturing long-term dependencies—that is, previous clinical procedures that may preclude future downstream interventions. 
Transformers are particularly adept at exploring longer-range relationships and were recently used to develop BEHRT152, which leverages the parallels between sequences in natural language and clinical events in EHRs to portray diagnoses as words, visits as sentences and a patient’s medical history as a document152. 
When used to predict the likelihood of 301 conditions in future visits, BEHRT achieved an 8–13.2% improvement over the existing state-of-the-art EHR model152. 
BEHRT was also used to predict the incidence of heart failure from EHR data.
Data-limiting factors in the deployment of ML
The past decade of research in ML in healthcare has focused on model development, and the next decade will be defined by model deployment into clinical settings42,45,46,154,155. 
In this section, we discuss two data-centric obstacles in model deployment: 
how to efficiently deliver raw clinical data (Table 4) to models, and how to monitor and correct for natural data shifts that deteriorate model performance. 
Delivering data to models. 
A main obstacle to model deployment is associated with how to efficiently transform raw, unstructured and heterogeneous clinical data into structured data that can be inputted into ML models. 
During model development, pre-processed structured data are directly inputted into the model. 
However, during deployment, minimizing the delay between the acquisition of raw data and the delivery of structured inputs requires an adept data pipeline for collecting data from their source, and for ingesting, preparing and transforming the data (Fig. 4). 
An ideal system would need to be high-throughput, have low latency and be scalable to a large number of data sources. 
A lack of optimization can result in major sources of inefficiency and delayed predictions from the model. 
In what follows, we detail the challenges of building a pipeline for clinical data and give an overview of the key components of such a pipeline. 
The fundamental challenge of creating an adept data pipeline arises from the need to anticipate the heterogeneity of the data. 
ML models often require a set of specific clinical inputs (for example, blood pressure and heart rate), which are extracted from a suite of dynamically changing health data. 
However, it is difficult to extract the relevant data inputs. 
Clinical data vary in volume and velocity (the rate that data are generated), thus prompting the question of how frequently data should be collected. 
Furthermore, clinical data can vary in veracity (data quality), thus requiring different pre-processing steps.
Moreover, the majority of clinical data exist in an unstructured format that is further complicated by the availability of hundreds of EHR products, each with its own clinical terminology, technical specifications and capabilities156. 
Therefore, how to precisely extract data from a spectrum of unstructured EHR frameworks becomes critical. 
Data heterogeneity must be carefully accounted for when designing the data pipeline, as it can influence throughput, latency and other performance factors. 
The data pipeline starts with the process of data ingestion (by which raw clinical data are moved from the data source and into the pipeline), a primary bottleneck in the throughput of the data through the pipeline. 
In particular, handling peaks of data generation may require the design and implementation of scalable ways to support a variable number of connected objects157. 
Such data-elasticity issues can take advantage of software frameworks that scale up or down in real time to more effectively use computer resources in cloud data centres158. 
After the data enters the pipeline, the data-preparation stage involves the cleansing, denoising, standardization and shaping of the data into structured data that are ready for consumption by the ML system. 
In studies that developed data pipelines to handle healthcare data156,159,160, the data-preparation stage was found to regulate the latency of the data pipeline, as latency depended on the efficiency of the data queue, the streaming of the data and the database for storing the computation results. 
A final consideration is how data should move throughout the data pipeline; specifically, whether data should move in discrete batches or in continuous streams. 
Batch processing involves collecting and moving source data periodically, whereas stream processing involves sourcing, moving and processing data as soon as they are created. 
Batch processing has the advantages of being high-throughput, comprehensive and economical (and hence may be advantageous for scalability), whereas stream processing occurs in real time (and thus may be required for time-sensitive predictions). 
Many healthcare systems use a combination of batch processing and stream processing160. 
Established data pipelines are being harnessed to support real-time healthcare modelling. 
In particular, Columbia University Medical Center, in collaboration with IBM, is streaming physiological data from patients with brain injuries to predict adverse neurological complications up to 48h before existing methods can161. 
Similarly, Yale School of Medicine has used a data pipeline to support real-time data acquisition for predicting the number of beds available, handling care for inpatients and patients in the intensive care unit (such as managing ventilator capacity) and tracking the number of healthcare providers exposed to COVID-19 161. 
However, optimizing the components of the data pipeline, particularly for numerous concurrent ML healthcare systems, remains a challenging task.
Deployment in the face of data shifts. A main obstacle in deploying ML systems for healthcare has been maintaining model robustness when faced with data shifts162. 
Data shifts occur when differences or changes in healthcare practices or in patient behaviour cause the deployment data to differ substantially from the training data, resulting in the distribution of the deployment data diverging from the distribution of the training data. 
This can lead to a decline in model performance. 
Also, failure to correct for data shifts can lead to the perpetuation of algorithmic biases, missing critical diagnoses163 and unnecessary clinical interventions164. 
In healthcare, data shifts are common occurrences and exist primarily along the axes of institutional differences (such as local clinical practices, or different instruments and data-collection workflows), epidemiological shifts, temporal shifts (for example, changes in physician and patient behaviours over time) and differences in patient demographics (such as race, gender and age). 
A recent case study165 characterizing data shifts caused by institutional differences reported that pneumothorax classifiers trained on individual institutional datasets declined in performance when evaluated on data from external institutions. 
Similar phenomena have been observed in a number of studies41,163,166. 
Institutional differences are among the most patent causes of data shifts because they frequently harbour underlying differences in patient demographics, disease incidence and data-collection workflows. 
For example, in an analysis of chest-X-ray classifiers and their potential to generalize to other institutions, it was found that one institution collected chest X-rays using portable radiographs, whereas another used stationary radiographs41. 
This led to differences in disease prevalence (33% vs 2% for pneumonia) and patient demographics (average age of 63 vs 45), as portable radiographs were primarily used for inpatients who were too sick to be transported, whereas stationary radiographs were used primarily in outpatient settings. 
Similarly, another study found that different image-acquisition and image-processing techniques caused the deterioration of the performance of breast-mammography classifiers to random performance (areas under the receiver operating characteristic curve of 0.4–0.6) when evaluated on datasets from four external institutions and countries163. 
However, it is important to note that the models evaluated were trained on data collected during the 1990s and were externally tested on datasets created in 2014–2017. 
The decline in performance owing to temporal shifts is particularly relevant;  if deployed today, models that have been trained on older datasets would be making inferences on newly generated data. 
Studies that have characterized temporal shifts have provided insights into the conditions under which deployed ML models should be re-evaluated. 
An evaluation of models that used data collected over a period of 9 years found that model performance deteriorated substantially, drifting towards overprediction as early as one year after model development167. 
For the MIMIC-III dataset132 (commonly used for the development of models to predict clinical outcomes), an assessment of the effects of temporal shifts on model performance over time showed that, whereas all models experienced a moderate decline over time, the most significant drop in performance occurred owing to a shift in clinical practice, when EHRs transitioned systems164 (from CareVue to MetaVision). 
A modern-day analogy would be how ML systems for COVID-19 (ref. 168) that were trained on data169 acquired during the early phase of the pandemic and before the availability of COVID-19 vaccines would perform when deployed in the face of shifts in disease incidence and presentation. 
Data shifts and model deterioration can also occur when models are deployed on patients with gender, racial or socioeconomic backgrounds that are different from those of the patient population that the model was trained on. 
In fact, it has been shown that ML models can be biased against individuals of certain races170 or genders42, or particular religious171 or socioeconomic15 backgrounds. 
For example, a large-scale algorithm used in many health institutions to identify patients for complex health needs underpredicted the health needs of African American patients and failed to triage them for necessary care172. 
Using non-representative or non-inclusive training datasets can constitute an additional source of gender, racial or socioeconomic biases. 
Popular chest-X-ray datasets used to train classifiers have been shown to be heavily unbalanced15: 67.6% of the patients in these datasets are Caucasian and only 8.98% are under Medicare insurance. Unsurprisingly, the performance of models trained with these datasets deteriorates for non-Caucasian subgroups, and especially for Medicare patients15. 
Similarly, skin-lesion classifiers that were trained primarily on images of one skin tone decrease in performance when evaluated on images of different skin tones173; 
in this case, the drop in performance could be attributed to variations in disease presentation that are not captured when certain patient populations are not adequately represented in the training dataset174. 
These findings exemplify two underlying limitations of ML models: 
the models can propagate existing healthcare biases on a large scale, and insufficient diversity in the training datasets can lead to an inadequate generalization of model outputs to different patient populations. 
Training models on multi-institutional datasets can be most effective at combating model deterioration15, and directly combating existing biases in the training data can also mitigate their impact171. 
There are also solutions for addressing data shifts that involve proactively addressing them during model development175–178 or retroactively by surveilling for data shifts during model deployment179. 
A proactive attitude towards recognizing and addressing potential biases and data shifts will remain imperative.
Outlook
Substantial progress in the past decade has laid a foundation of knowledge for the application of ML to healthcare. 
In pursuing the deployment of ML models, it is clear that success is dictated by how data are collected, organized, protected, moved and audited. In this Review, we have highlighted methods that can address these challenges. 
The emphasis will eventually shift to how to build the tools, infrastructure and regulations needed to efficiently deploy innovations in ML in clinical settings. 
A central challenge will be the implementation and translation of these advances into healthcare in the face of their current limitations: 
for instance, GANs applied to medical images are currently limited by image resolution and image diversity, and can be challenging to train and scale; federated learning promises to alleviate problems associated with small single-institution datasets, yet it requires robust frameworks and infrastructure; 
and large language models trained on large public datasets can subsume racial and ethnic biases171. 
Another central consideration is how to handle the regulatory assessment of ML models for healthcare applications. 
Current regulation and approval processes are being adapted to meet the emerging needs; in particular, initiatives are attempting to address data shifts and patient representation in the training datasets165,180,181. 
However, GANs, federated learning and transformer models add complexities to the regulatory process. 
Few healthcare-specific benchmarking datasets exist to evaluate the performance of these ML systems during clinical deployment. 
Moreover, the assessment of the performance of GANs is hampered by the lack of efficient and robust metrics to evaluate, compare and control the quality of synthetic data. 
Notwithstanding the challenges, the fact that analogous ML technologies are being used daily by millions of individuals in other domains, most prominently in smartphones100, search engines182 and self-driving vehicles68, suggests that the challenges of deployment and regulation of ML for healthcare can also be addressed.
Technology acceptance and critical mass: Development of a consolidated model to explain the actual use of mobile health care communication tools
ABSTRACT
Objective: Secure mobile communication technologies are being implemented at an increasing rate across health care organizations, though providers’ use of these tools can remain limited by a perceived lack of other users to communicate with. 
Enabling acceptance and driving provider utilization of these tools throughout an organization requires attention to the interplay between perceived peer usage (i.e. perceived critical mass) and local user needs within the social context of the care team (e.g. inpatient nursing access to the mobile app). 
To explain these influences, we developed and tested a consolidated model that shows how mobile health care communication technology acceptance and utilization are influenced by the moderating effects of social context on perceptions about the technology. 
Methods: 
The theoretical model and questionnaire were derived from selected technology acceptance models and frameworks. 
Survey respondents (n = 1254) completed items measuring perceived critical mass, perceived usefulness, perceived ease of use, personal innovativeness in information technology, behavioral intent, and actual use of a recently implemented secure mobile communication tool. 
Actual use was additionally measured by logged usage data. 
Use group was defined as whether a hospital’s nurses had access to the tool (expanded use group) or not (limited use group). 
Results: 
The model accounted for 61% and 72% of the variance in intent to use the communication tool in the limited and expanded use groups, respectively, which in turn accounted for 53% and 33% of actual use. 
The total effects coefficient of perceived critical mass on behavioral intent was 0.57 in the limited use group (95% CI 0.51–0.63) and 0.70 in the expanded use group (95% CI 0.61–0.80). 
Conclusion: Our model fit the data well and explained the majority of variance in acceptance of the tool amongst participants. 
The overall influence of perceived critical mass on intent to use the tool was similarly large in both groups. 
However, the strength of multiple model pathways varied unexpectedly by use group, suggesting that combining sociotechnical moderators with traditional technology acceptance models may produce greater insights than traditional technology acceptance models alone. 
Practically, our results suggest that healthcare institutions can drive acceptance by promoting the recruitment of early adopters though liberal access policies and making these users and the technology highly visible to others.
1. Introduction and backgrou
Communication is essential to providing safe and effective medical care. When healthcare providers are not in the same place at the same time, they rely on various forms of technology to communicate. 
Perhaps due to the limitations of conventional pagers, many providers use standard text messaging for patient care-related communication. 
Over half of inpatient medical providers report sending or receiving at least one patient care-related text per day.[1,2] 
The proposed benefits of texting, as opposed to paging systems, include immediate response times, less workflow interruption, and a greater depth of information that can be sent (e.g. digital photographs of electrocardiograms).[3–5] 
Given the ubiquity of standard text messaging amongst healthcare providers, many organizations have opted to lower their risk of HIPAA breaches by adopting secure, third-party communication platforms. [6,7] 
However, fewer than thirty percent of hospital systems that adopt a secure platform achieve widespread utilization of the tool by a majority of clinicians.[1] 
Even though evaluation studies have shown that secure, smartphone-based messaging apps can improve providers’ efficacy and satisfaction, lack of access to and use of the tool by a patient’s entire care team may restrict widespread utilization.[8,9] 
In January 2019, our institution began a staged implementation of the HIPPA-compliant Vocera Collaboration Suite (Vocera Communications, Inc., San Jose, CA) across the health system, including text messaging and voice call functionality. 
Physicians and Advanced Practice Providers (APPs) were encouraged to voluntarily use the Vocera mobile app on their personal smartphones. 
Two of six main hospitals also provided registered nurses (RNs) with Vocera-enabled work phones, enabling communication between physicians, APPs, and RNs. 
We postulated that physicians and APPs in these two hospitals would find Vocera beneficial, while those who could not contact nurses in the app at the other four hospitals would see little benefit in using Vocera over the traditional paging system. 
To test our research hypothesis, knowing that mobile health care communication implementation is complicated,[10] we sought to untangle the influences of physician/APP perceptions on intent to use and actual use of the Vocera using methods that could be generalized to study other mobile communication implementations. 
To this end, we present the development and validation of a consolidated structural equation model that combines traditional technology acceptance assumptions with the moderating effects of sociotechnical context. 
This methodology allows us to highlight the pathways that are most influential in increasing end-user acceptance and utilization during a complicated, multi-stage health care communication technology implementation. 
Data consist of a survey instrument paired with logged Vocera use. 
First, we propose a research model derived from relevant technology acceptance models and sociotechnical frameworks from which hypotheses are derived. 
Next, we describe the methodology used to perform structural equation modeling (SEM) for testing the strength of relationships between perceptions across two moderating groups: 
1) the “expanded” use group – hospitals where nurses were provided Voceraenabled smartphones at work, 
and 2) the “limited” use group – hospitals where nurses were not provided Vocera-enabled smartphones at work. 
Results of this investigation are followed by a discussion of findings, including theoretical contributions to the field and practical suggestions for increasing adoption of secure mobile communication tools in healthcare.
2. Methods
2.1. Conceptual model and research hypotheses
2.1.1. Technology acceptance model
The antecedent phase of HIT implementation is adoption, wherein an organization decides to move forward with implementation of a new technology. 
Adaptation occurs subsequently, which includes developing, installing, and maintaining the technology. Finally, acceptance occurs as end-users commit to using and begin utilizing the technology for organizational work.[11] 
Understanding and predicting acceptance behavior can maximize the success of HIT implementation [12] and has therefore been the subject of a large body of technology acceptance literature. 
The Technology Acceptance Model (TAM) was developed by Davis [13] as a way to better explain and predict individuals’ acceptance of new information technologies. 
TAM suggests that two core beliefs (i.e. constructs) positively influence one’s self-reported intent to use a technology. 
The first, perceived usefulness (PU), is defined as “the degree to which a person believes that using a particular system would enhance his or her job performance.” The second, perceived ease of use (PEOU), is defined as “the degree to which a person believes that using a particular system would be free of effort.” TAM theorizes that PEOU influences intent to use the technology both directly and indirectly through its effect on PU. 
TAM studies in healthcare settings generally show that the direct influence of PU on intent to use is greater than the direct influence of PEOU on intent to use, highlighting PU as the key proximal mediator of intent to use.[14] 
TAM typically explains only about 40% of the variance in intention to use a new technology [15] – thus, as TAM was popularized, researchers tested the effects of adding mediating and moderating constructs to the original model, which led to sequentially more complex models including TAM2,[16] TAM3,[17] the Unified Theory of Acceptance and Use of Technology (UTAUT),[18] and UTAUT2.[19] 
Given the large number of pathways in later models, as well as a general lack of specificity when applied to specific domains like healthcare,[14] researchers will omit constructs not relevant to the IT being studied and add constructs that are more applicable.[20] 
Here, we preserve the original parsimony of TAM by retaining PEOU and PU as core constructs in our research model and replace the complexity of UTAUT with a moderation analysis by social context (use group) to provide a more holistic evaluation of the dynamic sociotechnical processes involved. 
Based on TAM, we hypothesize that both PU and PEOU will have a positive direct influence on intent to use Vocera, though the latter to a lesser degree, as has been described in prior studies.[21,22] 
Additionally, we do not expect these relationships to vary by use group. 
To remain consistent with the literature, we define the intent to use Vocera as the construct behavioral intent (BI), which is used as a measure of acceptance.
H1a: PU positively influences BI similarly in both groups.
H1b: PEOU positively influences BI similarly in both groups.
H1c: PEOU positively influences PU similarly in both groups.
H1d: The influence of PU on BI is greater than the influence of PEOU on BI in both groups.
2.1.2. Personal innovativeness in the domain of information technology
The Diffusion of Innovation Theory, developed by E.M. Rogers in 1962,[23] posits that the characteristics of certain user groups will affect how rapidly they adopt1 a new technology (i.e. an innovation). 
For example, early adopters embrace change and are comfortable with new ideas. 
Agarwal and Prasad [24] hypothesized that these unique adoption characteristics could be captured and incorporated into TAM as a construct called Personal Innovativeness in the Domain of Information Technology (PIIT). 
PIIT measures willingness to try out and experiment with new technologies, which are static personal traits. 
Agarwal and Prasad [24] recommended that PIIT be included in technology acceptance studies as a way of controlling for where each user sits on the Diffusion of Innovation continuum.
Lu et al. [25] found that PIIT had a direct positive impact on perceived usefulness and perceived ease of use of wireless mobile technology. 
We similarly hypothesize that more innovative individuals perceive Vocera as easier to use and more useful, and that innovativeness directly positively influences intent to use Vocera, though not differently between use groups.
H2a: PIIT positively influences PU similarly in both groups.
H2b: PIIT positively influences PEOU similarly in both groups.
H2c: PIIT positively influences BI similarly in both groups.
2.1.3. Perceived critical mass
Critical mass theory also arose from Rogers’ Diffusion of Innovation Theory wherein critical mass is defined as a point along the adoption curve where the innovation becomes self-sustaining.[23] 
On this curve, later adopters are influenced by earlier adopters. 
However, when assessing multidirectional communication innovations like phone and email, Markus[26] found that early adopters are also influenced by later adopters and nonadopters. 
For example, early users of email experienced low benefits and high costs relative to later adopters because of uncertainty as to whom else was using email and would or would not receive an early adopter’s email message.[27] 
Therefore, having a sense of how many others are using the technology is likely to affect one’s intention to the use the technology. 
In related work, Moore and Benbasat[28] sought to measure a technology’s perceived observability, defined as “the degree to which the results of an innovation are observable to others,” but found instead that observability comprised two independent constructs, result demonstrability (how obvious are the results of using the technology) and visibility (how often do users see the technology used around them). 
As opposed to physically obvious hardware like the portable work station that Moore and Benbasat[28] studied, mobile communication applications exist as software within the already ubiquitous hardware of smartphones, making physical visibility a poor construct to represent critical mass. 
Instead, perceived critical mass (PCM) measures individuals’ perception of the use of the technology within their peer group, which may grow from alternate exposures besides direct observation of usage, including organizational announcements and peers asking questions about the technology.[29] 
Lou et al.[29] found the total effects of PCM on BI to be 0.647, more influential than the direct effect of PU on BI of 0.590. 
Subsequently, Van Slyke et al.[30] used PCM to study instant messaging acceptance and, like Lou et al.,[31] confirmed that PCM directly and positively influences the constructs of relative advantage (akin to PU), PEOU, and BI. 
An affiliated concept, subjective norm, was developed in the 1970s by Fishbein and Ajzen[32] during their formulation of the Theory of Reasoned Action (TRA). 
In TRA, subjective norm describes the perceived social pressure that, along with attitudes toward a behavior, can predict that behavior.[33] 
As such, TRA provided a crucial theoretical development for the original development of TAM, and subjective norm was included as a construct in TAM2 and UTAUT variations under the name social influence.[21] 
In the setting of a voluntary mobile messaging technology, we suspect that subjective norm is less likely than PCM to explain actual use. 
Mobile apps are used on small, individual phone screens and can quickly be opened or closed if an important person is watching. 
A user’s actual use might increase slightly if one important person tries to reach the user through the app (measured by subjective norm); 
however, that user’s actual use is likely to increase a great deal more if a large number of peers are seen as potentially trying to reach out through the app (measured by PCM). 
Additionally, we would expect subjective norm to improve alongside PCM, as peers perceived as sending communications would expect others to receive them. 
For these reasons we do not expect subjective norm to contribute in a meaningfully theoretical or practical way in this research context and have not included it in our model. 
PCM, on the other hand, is a uniquely comprehensive construct in that it is intertwined with multiple dimensions of Sittig and Singh’s sociotechnical model for HIT[34], including people, workflow and communication, and internal organizational policies, procedures, and culture. 
Although measuring PCM will not give granular information as to which specific sub-dimensions are affecting it, observing how its influence on technology acceptance is moderated by sociotechnical context may highlight strategies to further improve acceptance on a local level. 
In concert with existing studies that have modeled PCM,[29–31] we hypothesize that PCM will positively influence PU, PEOU, and BI in both groups. 
Given ours is the first study assessing for a moderating effect of sociotechnical context (use group) on PCM, we must speculate based on our general understanding of hospital communication practices how the PCM pathways will differ between groups. 
Generally, we hypothesize that the total effects of PCM on BI will be similar in both groups. 
That is, when adding up direct and indirect influences, the perceived pervasiveness of Vocera positively influences BI similarly for all participants regardless of RN use of Vocera. 
Contrarily, we hypothesize that the indirect relationships of PCM on PU and PEUO will vary by use group. 
In the expanded use group, more providers may perceive that a critical mass of the people they work with (RNs) are using Vocera, which may increase the effect of PCM on PEOU (easier to find an active user to contact) and PU (increased utility in direct messaging with RNs). 
In contrast, limited group users who perceive Vocera as potentially useful and easy to use may never perceive that enough of their peers are reachable through Vocera, thereby decreasing the influence of PCM on PU and PEOU. 
Accordingly, we hypothesize that the direct influence of PCM on BI is greater in the limited use group as it must counterbalance this group’s lower PU and PEOU effects to maintain an overall equal total effects size of PCM on BI between groups, and vice versa.
H3a: PCM positively influences PU to a greater degree in the expanded use group.
H3b: PCM positively influences PEOU to a greater degree in the expanded use group.
H3c: PCM positively influences BI to a greater degree in the limited use group.
H3d: The combined effects of PCM on BI are similar in both groups.
2.1.4. Behavioral intent
Measures of utilization, the behavior of employing a technology in completing tasks, depend on the specific type of technology and the specific task at hand.[35] 
Measuring the total number of daily communication tasks for an individual provider (including pages sent and landline phone calls placed) would only be feasible with direct observation on a massive scale. 
Because of the inherent difficulty in measuring true utilization, TAM researchers most often make the assumption that BI, a marker of acceptance, reliably predicts selfreported use.[21,36] 
However, prior comparisons between self-reported use and logged use of mobile communications (texting and phone calls) show only a modest correlation, between 0.35 and 0.48.[37,38] 
Therefore, augmenting self-reported usage data with actual usage data can inform model veracity.[39] 
Included in the Vocera Collaboration Suite package is an analytics server that logs every action taken by every user. 
Thus, to best capture true Vocera utilization, we define a construct actual use (AU) that is measured by each user’s logged outgoing communication frequencies over the past four weeks combined with their self-reported use. 
Based on TAM, we hypothesize that intent to use Vocera will positively influence actual usage of Vocera, and that this effect will be similar between use groups.
H4: BI positively influences AU similarly in both groups.
2.1.5. Research model
We proposed a consolidated research model derived from these selected constructs to guide our study (Fig. 1). 
The model proposes that AU is directly influenced by BI, which is both directly and indirectly influenced by PEOU, PU, PIIT, and PCM. The use group of participants, either expanded or limited, is expected to moderate PCM pathways.
2.2. Vocera implementation
We recruited participants from six main hospitals within our large integrated academic health system in the Midwest. 
In Phase 1 of implementation, the medical system turned on access to Vocera for physicians (including resident physicians) and APPs in January 2019. 
Go-live was advertised though a series of emails that provided information on how to voluntarily download the Vocera app to a personal smartphone. 
Phase 2 of implementation expanded Vocera access to nursing staff and was enabled by providing RNs with Vocera-enabled smartphones at work. 
Two of the six hospitals had completed Phase 2 by July 2019, while the remaining four hospitals were slated to enter Phase 2 after the conclusion of this study. 
Thus, two hospitals comprised the limited use group, and four hospitals comprised the expanded use group.
2.3. Study population
Our study protocol was reviewed by our local Institutional Review Board and deemed not human research. 
We delivered an invitation to complete our survey by email to all physicians and APPs in our health system affiliated with any of the six main hospitals. 
Informed consent was provided in the email; after providing consent, participants could begin the survey. 
Participants were eligible to enter into a raffle for one of eight $25 Amazon gift cards upon completion of the survey. 
The survey was open from November 11th, 2019 to December 31st, 2019. 
A total of 1254 respondents completed the survey, for a completion rate of 20.5%.
2.4. Survey design and content
The survey was designed to collect data regarding 1) beliefs about Vocera, which were applied to the research model, and 2) individual demographic information, which were captured for descriptive purposes. 
The model contained 6 constructs measured by a total of 22 indicators (i.e. questions). 
Validated indicators for PU,[36] PEOU,[36] PIIT,[24] PCM,[30,31] and BI[17,40] were taken from the literature and, if necessary, adapted to replace the verbiage from the original study with verbiage specific to Vocera functionality. 
All indicator responses were scored on a 5-point Likert scale in the form of agreement, with the exception of AU indicators, which were scored by frequency of selfreported use (see Appendix A for full indicator list and scoring details). 
If participants had not downloaded the Vocera app, only demographic information was collected. 
Otherwise, participants were required to answer indicator items, including their self-reported frequency of Vocera texting and calling. 
Demographic items included personal characteristics and practice environment (e.g. department and training status). 
Likert response values for each research construct were averaged for each participant to test for differences between use groups using the Mann-Whitney U test for non-normal distributions. 
Only data from those who reported downloading Vocera were included in structural modeling.
2.5. Operationalization of logged usage data
Logged usage data was pulled from the Vocera server and joined individually to participant survey responses for Vocera downloaders. 
For each respondent, we captured all outgoing communications over the four weeks prior to survey completion. 
Four weeks was used as the lookback period to match the minimum self-reported use frequency listed on the survey as “less than once a month.” 
Communication differences between use groups were tested using the Mann-Whitney U test. 
Of the 726 participants who had downloaded Vocera, 127 (17.5%) had placed at least one call in the preceding 4 weeks (median 0.0, mean 4.9, maximum 287, Q1 0.0, Q3 0.0). 
Similarly, 211 (29.1%) had sent at least one text message in the preceding 28 days (median 0.0, mean 29.1, maximum 1198, Q1 0.0, Q3 2.0). 
Given these highly skewed counts, both variables were regularized into quartile-derived, ordered categories to achieve variances within one order of magnitude of the other items in the model. 
Total calls were grouped as follows: 0, 1–3, 4–14, and 15–287. Total text messages were grouped as follows: 0, 1–5, 6–55, and 56–1198. 
Data analysis was conducted in R 3.5.2 (R Core Team, 2020) and SEM was performed using the R package lavaan.[21]
3. Results
3.1. Characteristics of study participants 
Demographics and clinical practice characteristics of all participants are described in Table 1. Of the total sample (n = 1254), 726 had downloaded Vocera (57.9%). 
Age was similar between downloaders and non-downloaders. 
The majority of participants in both groups were from the departments of medicine and surgery. 
Less than 20% were APPs, and most were attending physicians. 
Among those who had downloaded Vocera, an inpatient practice was more common than a clinic practice (60.6% vs 37.6%, respectively). 
For downloaders, logged usage data was significantly higher in the expanded use group (n = 147) than the limited use group (n = 579). 
Expanded group users placed a median of 4 phone calls per month vs. 0 per month in the limited use group (Mann-Whitney U p-value < 0.001). 
Similarly, expanded group users sent a median of 42 text messages per month vs. 0 per month in the limited use group (p < 0.001). 
The average of responses to 5-point Likert scale questions, which were only captured for downloaders, also varied significantly by group for each construct (Fig. 2), with the exception of PIIT (median 3.75 in both groups, p = 0.508). 
Expanded group users were more likely to agree that Vocera is useful (3.33 vs. 2.83, p < 0.001) and easy to use (3.67 vs. 3.17, p = 0.005). 
They also were more likely to agree that most of their peers were using Vocera (4.00 vs. 1.33, p < 0.001), and that they intended to use Vocera in the future (3.33 vs. 2.00, p < 0.001).
3.2. The measurement model
A normality check was performed on the data before beginning model creation. The skewness for each indicator ranged from − 0.78 to 2.30 and the kurtosis for each indicator ranged from 1.97 to 6.96. 
Absolute skewness below 3.0 and absolute kurtosis below 8.0 indicate an acceptably normal univariate distribution.[41] 
Internal reliability was demonstrated by Cronbach’s alpha coefficients that ranged from 0.86 to 0.98 for each construct, higher than the generally acceptable level of 0.70.[42] 
Next, a confirmatory factor analysis (CFA) model was made to check the convergent validity of constructs, i.e. how well variables within constructs are measuring the construct they are theorized to measure. 
Construct validity is generally met when factor loadings are greater than 0.5, meaning that the model should explain the majority of the variance in each variable.[41] 
Amongst our data, factor loadings ranged from 0.67 to 0.99, thus demonstrating construct validity. 
Finally, discriminant validity, i.e. how well variables are measuring one construct and not another, is generally assumed when correlations between constructs are less than 0.90.[41] The largest construct correlation in our CFA model was 0.79, thus satisfying the condition of discriminant validity. 
Measurement model parameters are listed in the Appendix B.
3.3. The structural equation model
We estimated the structural equation model using diagonally weighted least squares (DWLS) and a scaled-shifted test statistic (SS) to provide robust standard errors and model fit measurements given the completeness of our data.[43] 
To test the hypothesis that path effects in the model may be moderated by use group, we first constructed a grouped configural invariance model where loadings and intercepts were free to vary between groups. 
We compared this model to a grouped strong invariance model where loadings and thresholds were constrained between use groups. 
The strong invariance model significantly explained a greater amount of our data than the configural invariance model, thus allowing us to reject the null hypothesis that the relationships between constructs were the same in both use groups (ANOVA ΔХ2 = 185.7, Δdf = 92, p < 0.001). 
The moderated model’s overall fit was assessed in comparison to fit measure thresholds reported in the literature. 
Commonly accepted values for a well-fitting model are as follows: 
ratio of Х2 / df < 2- 5, root mean square error of approximation (RMSEA) less than 0.05–0.08, Tucker Lewis Index (TLI) greater than 0.95, and comparative fit index (CFI) greater than 0.95.[44,45] 
Our model demonstrated a Х2 / df of 3.06, an RMSEA of 0.079, a TLI of 0.96, and a CFI of 0.96 (robust test statistics). 
Overall, these fit measures indicate that the moderated model explains the variation in the data to a satisfactory degree.
3.4. Path effects and influence
We considered standardized path coefficients to be significant if a ttest on their robust standard error estimations produced a p-value of < 0.05 at the predetermined significance level of α = 0.05. 
In our model, all coefficients were significantly positive in both groups, with the exception of PIIT on PU and PEOU on BI. 
We performed path by path analysis between groups by sequentially constraining each path to determine whether the relationship between individual constructs varied (i.e. were moderated) by use group. (Fig. 3). 
Coefficient values with 95% confidence intervals are provided in Table 2. 
Lastly, we calculated the total influence of the direct and indirect PCM pathways on BI. 
This measure indicates by how much increasing PCM is expected to increase BI overall. 
The total influence of PCM on BI was significantly positive in both groups, with a standardized coefficient of 0.70 (95% CI 0.61–0.80) in the expanded use group and 0.57 (95% CI 0.51–0.63) in the limited use group. 
All significant relationships were included in the final model diagram shown in Fig. 4. 
Hypothesis assessments are shown in Table 2. 
Within the limited use group, the amount of variance explained by incoming construct influences (R2 values) were as follows: PEOU = 10%, PU = 53%, BI = 61%, and AU = 53%. 
Within the expanded use group, the R2 values were as follows: 
PEOU = 17%, PU = 63%, BI = 72%, and AU = 33%.
4. Discussion
4.1. Theoretical implications
Our consolidated research model confirms the core TAM beliefs that PEOU influences PU which in turn influences BI (H1a, H1c). 
Our hypothesis (H1d) that the influence of PEOU on PU would be greater than the influence of PEOU on BI was supported, though the influence of PEOU on BI was not significant in the expanded use group (H1b). 
Conversely, the influence of PEOU on PU was higher in the expanded use group. PEOU’s moderated influence may be explained by differing levels of Vocera exposure between the groups.
As seen in the logged usage data, limited group participants sent few communications using Vocera and thus may be considered inexperienced users. 
Design theory shows that usability (i.e. ease of use) is more important to inexperienced users than to seasoned users who have learned the fundamental operating characteristics of a new technology.[46] 
For seasoned users, ease of use makes Vocera more useful but does not directly influence acceptance. 
We found no evidence of an influence of PIIT on PU in either group (H2a), although PIIT had a small positive influence on PEOU (H2b) and BI (H2c) that was similar in both groups. 
Thus, Vocera early adopters (akin to having high PIIT) may be slightly more likely to find Vocera easy to use and to intend to use it, but other constructs are significantly more important in driving adoption. 
As expected, PCM positively influenced PEOU, PU, and BI in both groups, and the total influence of PCM on BI was statistically similar between groups. 
Contrary to our hypotheses, the influence of PCM on PU was greater in the limited use group (H3a) while the influence of PCM on PEOU did not differ between groups (H3b), and the influence of PCM on BI was greater in the expanded use group (H3c). 
This shift in path contribution from PU mediation to the direct effect of PCM on BI in the expanded use group could be explained by the bandwagon effect,[47] wherein a user who does not hold positive beliefs about Vocera’s ease of use or usefulness may use it anyway to "keep up" with everyone else. 
Uniquely, our model showed a strong significant influence of BI on AU in both groups (H4).
Whereas TAM considers BI an indicator of behavioral acceptance, we defined our AU construct (measured by a combination of recent logged usage data and self-reported use) to be inherently representative of utilization in the present. 
In our data, BI explained 20% less variance in AU in the expanded use group as compared to the limited use group. 
This may reflect asynchrony between evolving beliefs and actual use over time in the expanded use group, driven by more exposure to and experimentation with Vocera. 
Future research should aim to clearly define the relationship between intent to use a mobile communication tool and actual utilization of that tool over time. 
The variances in BI explained by our model (61% in the limited use group and 72% in the expanded use group) exceed the maximum of what has been achieved in prior technology acceptance studies (40–70%), [20,21] thus supporting the use of our model to explain communication technology acceptance in the healthcare environment. 
As healthcare organizations and technologies become more complex, the study of technology acceptance in the overall context of implementation must take into account the social and organizational dimensions of the environment in additional to the pure technological (hardware/software) dimensions[48]. 
In this research, we incorporated the sociotechnical context inherent within each use group as a moderating factor applied it across generalizable TAM-based constructs. 
Clearly identifying and defining a such a sociotechnical moderator necessitates a focused review of one’s local implementation environment but, as shown here, may be worth the effort: 
the inaccuracies of our PCM moderation hypotheses uncovered areas for additional research clarification that would have remained unknown in the absence of a consolidated model.
4.2. Practical implications 
Our findings suggest that increasing provider adoption of mobile health communication tools may be achieved most effectively by intensifying perceived critical mass. 
To increase PCM, PU and PEOU must be sufficient to initialize a group of early adopters2 of the technology. 
In this study, early adopters likely flourished in the expanded use group where the perceived usefulness of direct, twoway communication between inpatient providers and RNs led certain providers (with high PIIT and thus higher PEOU) to download Vocera. 
As these early adopters’ activities become visible to their peers, PCM would have increased, demonstrating to later adopters that the technology was beneficial, culminating in the conspicuously higher adoption levels observed in the expanded use group. 
To achieve this domino effect[30] of positive impacts, hospital leaders should develop strategies to 1) increase the number of early adopters and 2) make these early adopters more visible to the majority. 
The former could be achieved through expanded technology access for the entire care team, including the provision of devices for certain providers to use during work, which would increase the number of early adopters from multiple care specialties (e.g. RNs, respiratory therapists, physical therapists, etc.). 
Then, visibility campaigns could be used to highlight the positive experiences of these early adopters. 
For example, leaders might choose to share stories showcasing early adopters’ enthusiasm for and use of the technology through organizational meetings, email, and social media. 
These constant reminders would ensure a continuous increase in PCM. 
Provided the technology meets basic usability requirements, resultant adoption would be expected to increase substantially.
4.3. Limitations 
Our study focused on physicians and APPs as the primary end-users of Vocera and did not include RNs in the survey. 
Although we might expect RNs to share similar beliefs about Vocera to physicians and APPs, it is possible that RNs expect different functionality and communication practices from mobile communication tools. 
The strong association found in this study between RN access to Vocera and physician/APPs’ actual use of Vocera suggests that RNs should be included in future studies. 
Other members of the care team who regularly communicate with physicians/APPs and RNs should also be included and considered as key stakeholders in mobile communication technology adaptation and evaluation. 
As suggested by sociotechnical frameworks,[34,49] the hospitals in this study might house unique cultural norms or workflow differences that could contribute to variations in perceptions and use that were not accounted for in our model. 
Although we accounted for one moderating sociotechnical variable, use group, a more comprehensive set of other moderating variables could be considered in future research. 
It is also possible that variations in participants’ work schedules and clinical care duties affected the number of logged usage events, and additional work is needed to control for these potential effects when describing utilization as a function of time. Finally, as this study examined perceptions over a single month during a multi-year implementation, any evolving temporal changes in construct coefficients outside of this month were notcaptured.
5. Conclusion
The implementation of secure communication tools within healthcare systems is increasing; however, the acceptance of these tools is largely determined by how they are perceived by users within their unique sociotechnical contexts. 
Here, we have developed a consolidated model that combines the generalizability of technology acceptance model assumptions with the local specificity of a sociotechnical moderating variable. 
The model reveals that acceptance and ultimate utilization of a mobile communication tool rely critically on the perception that most of one’s peers also use the tool. 
Sociotechnical moderation analysis uncovered unexpected changes in the downstream influences of perceived critical mass that suggest new areas for research.
A real-time integrated framework to support clinical decision making for covid-19 patients
Abstract
Background: 
The COVID-19 pandemic affected healthcare systems worldwide. 
Predictive models developed by Artificial Intelligence (AI) and based on timely, centralized and standardized real world patient data could improve management of COVID-19 to achieve better clinical outcomes. 
The objectives of this manuscript are to describe the structure and technologies used to construct a COVID-19 Data Mart architecture and to present how a large hospital has tackled the challenge of supporting daily management of COVID-19 pandemic emergency, by creating a strong retrospective knowledge base, a real time environment and integrated information dashboard for daily practice and early identification of critical condition at patient level. 
This framework is also used as an informative, continuously enriched data lake, which is a base for several on-going predictive studies. 
Methods: 
The information technology framework for clinical practice and research was described. 
It was developed using SAS Institute software analytics tool and SAS® Vyia® environment and Open-Source environment R ® and Python ® for fast prototyping and modeling. The included variables and the source extraction procedures were presented. 
Results: 
The Data Mart covers a retrospective cohort of 5528 patients with SARS-CoV-2 infection. 
People who died were older, had more comorbidities, reported more frequently dyspnea at onset, had higher d-dimer, C-reactive protein and urea nitrogen.
The dashboard was developed to support the management of COVID-19 patients at three levels: hospital, single ward and individual care level. 
Interpretation: 
The COVID-19 Data Mart based on integration of a large collection of clinical data and an AI-based integrated framework has been developed, based on a set of automated procedures for data mining and retrieval, transformation and integration, and has been embedded in the clinical practice to help managing daily care. 
Benefits from the availability of a Data Mart include the opportunity to build predictive models with a machine learning approach to identify undescribed clinical phenotypes and to foster hospital networks. 
A real-time updated dashboard built from the Data Mart may represent a valid tool for a better knowledge of epidemiological and clinical features of COVID-19, especially when multiple waves are observed, as well as for epidemic and pandemic events of the same nature (e. g. with critical clinical conditions leading to severe pulmonary inflammation). 
Therefore, we believe the approach presented in this paper may find several applications in comparable situations even at region or state levels. 
Finally, models predicting the course of future waves or new pandemics could largely benefit from network of DataMarts.
1. Introduction
An unexpected rapid spread of SARS-CoV-2, the agent of the coronavirus disease 2019 (COVID-19), had been observed in China since January 2020, which resulted in a pandemic [1].
On January 5th 2022, more than 200 million people were found positive to SARS-CoV-2 globally and more than 5 million died [2]. 
Since the beginning, a relevant percentage of people with COVID-19 had clinical deterioration requiring hospitalization or intensive care admission and national health care systems were profoundly challenged in a very short time [3]. 
Hospitals have a critical role in the response to the pandemic [4]; 
however, many countries early saturated their intensive care bed capacities. During the different phases of the pandemic, Italy has been among the most impacted countries at global level. 
Facing the challenge of the emergency, availability of a timely, centralized, standardized, and reliable patient dataset is of high priority. 
The development of diagnostic and prognostic predictive models through the application of advanced statistical modeling and machine learning techniques (Artificial Intelligence – AI) have the potential to improve patient outcomes [5,6]. 
To name three main purposes of an AI framework from real-world data (RWD), COVID-19 archives may, at the same time, be used as a data classifier, as an approach to stratify patients into risk categories and as resource to train predictive tools. 
A real-time acquisition, centralization, and constant update of a COVID-19 Data Mart with information collected in healthcare systems of patients affected by COVID-19, and the availability of user-oriented data visualization tools, is a valuable source of information to support clinical practice and research on the pandemic. 
The primary objective of this manuscript is to describe the structure and technologies used to construct the COVID-19 Data Mart architecture. 
The secondary objective is to present a pragmatic response to urgent needs due to the COVID-19 pandemic, particularly, creating a strong retrospective knowledge base, a realtime environment and integrated information dashboard for daily practice and early identification of critical condition at patient level.
2. Methods
A competence center named Generator, based on Data Integration, Analytics and AI was built at the Fondazione Policlinico Universitario A. Gemelli IRCCS, Rome, Italy. 
The main purpose of the Generator program is to centralize and integrate previously decentralized, and heterogeneous (structured and unstructured) healthcare data, stored daily in the hospital’s Data Warehouse (DWH) or archives of individual departments, using highquality ontology-based systems and effective information technology (IT) procedures, while respecting data ownership and patient privacy. 
Generator has the goal to build standardized and structured archives (called Data Marts) for a specific area or disease to conduct research projects, quality assessments and to develop a rapid-learning framework through the daily feeding of data sources and the periodic re-train and validation of predictive models on new data [7,8].
2.1. A COVID-19 real time analysis framework
An integrated framework for clinical practice and research for COVID-19 disease was implemented at the Fondazione Policlinico Universitario A. 
Gemelli IRCCS, Rome, Italy, a 1450-bed hospital where, since March 1st, two Centers were dedicated to the care of patients with COVID-19: 
the Gemelli Hospital (FPG) and the Columbus Hospital (CIC) – in the highest peak moments of the spread of the disease, additional temporary care units were made available. 
The COVID-19 IT framework is available to all care units to serve three main goals: 
• A real-time Data Mart covering patients hospitalized at Policlinico Gemelli, refreshed on daily basis with new inpatients, discharged and deceased patients, update of clinical data for the ones still in charge. 
Thus, providing a patient-centered longitudinal view on the disease and treatment progression. 
• A library of Data Visualization dashboards, profiled for the different wards treating Covid-19 patients, showing the evolution of the cared cohort for the ward and for each patient on daily basis, and allowing to drill down the most critical cases with their key indicators of the disease status (including hospitalization history). 
• A set of AI-based modeling tools that currently support ongoing research studies from several clinical teams. 
The framework was designed through the following steps 
a) the cross-disciplinary group of clinicians from all domains (Gemelli against Covid Group) identified the list of variables of interest sharing knowledge and evidences from the daily practice in treating COVID-19; 
the focus has been to capture the widest set of data to be able to follow all relevant clinical aspects in the context of a rapidly evolving scenario of the disease; 
b) IT specialists and data scientists have developed the interoperability procedures to extract from hospital clinical workflows the data variables as agreed with the clinician team (in structured and unstructured format) for all hospitalized and incoming patients; 
c) standard procedures have been developed for data treatment and the daily updated of the integrated Data Mart; these include natural language processing (NLP) algorithms to map medical reports into categorical variables, validation procedures for data quality and consistency, semantic control steps; 
d) Iterative design and development of profiled visualization dashboard for data analytics and daily exploitation from the clinical teams; 
e) lastly, the platform provides a series of tools for the development of predictive models directly used by clinicians in clinical practice through user-friendly interfaces. 
The framework was developed using SAS Institute software analytics tool and SAS® Vyia® environment and Open-Source environment R ® and Python ® for fast prototyping and modeling.
2.2. Identification of variables of interest and data archives
As a result of this process of clinical variable selection and semantic control, the following variable groups have been identified to feed the Data Mart and support clinical activities: 
demographic, reverse-transcriptase polymerase chain reaction (RT-PCR) nasopharyngeal test for SARS-CoV-2, SARS-CoV-2 serology, respiratory isolated pathogens other than SARS-CoV-2, laboratory parameters at admission and during the hospitalization, comorbidities and treatments before the hospital admission, symptoms, arterial blood gasses parameters, respiratory support, therapies for COVID-19, anticoagulant therapies, other drugs, radiology findings, intensive care measures, complications during the hospitalization, length of hospitalization and outcomes (needs of oxygen therapy, mechanical ventilation and death) 
The detailed description of the data available for each category is available in Table 1 in the online Appendix.
2.3. Data and source extraction procedures
The selected variables have been extracted from the corresponding data sources through the implementation of a standard extract, transform and load (ETL) procedure. 
This procedure has made it possible to integrate data from different applications, including data cleaning and standardization to the target structure – a relational database (the COVID-19 Data Mart) with a general structure able to support the daily practice and research activities. 
Where necessary, the procedures include a transformation step to transform unstructured information into useful structured data. 
Therefore, this ETL procedures consisted of several components as briefly described below and summarized in Fig. 1. 
1. Daily update of the cohort of patients currently hospitalized, including only those patients who have carried out at least one positive RT-PCR nasopharyngeal test for SARS-CoV-2 and who have passed through one of the COVID dedicated wards in the hospital; 
patients discharged (full recovered or transferred to a pre-discharge structure) or deceased become part of the retrospective cohorts for statistical analysis and research studies. 
2. Daily extraction, validation, and Data Mart update for structured clinical variables (e. g. laboratory data) for each hospitalized patient (defined as ETL 1 + ETL 3 in Fig. 1); 
baseline data for patients just hospitalized were included in this step. In the case of a structured source, an identification code has been associated with each field. 
The codes used were referred to national and international standard such as: 
the International Classification of Disease (ICD) version 9 ICD9, Diagnosis Related Group Classification. 
Where none of these standards were available, specific coding for hospital legacy applications were used. 
3. Daily extraction, transformation, validation, and Data Mart update for unstructured clinical variables (e. g. text extracted from medical reports and converted into structured clinical data) for each patient (defined as ETL 2 + ETL 3 in Fig. 1). 
To obtain structural information from unstructured texts (such as clinical diary, radiology reports etc.) NLP algorithms have been applied, based on text mining procedures such as:
sentences/words tokenization; rule-based approach supported by annotations defined by the clinical SMEs, and using semantic / syntactic corrections where necessary. 
To ensure the development of a framework that respects privacy by design, specific procedures for pseudonymization have been included. 
The resulting COVID-19 Data Mart is a relational database, where the primary key is a patient identifier in pseudonymized form, that provides a longitudinal, comprehensive view of the disease status for each hospitalized patient allowing to drill down on symptoms, vital signs, oxygenation status, comorbidities. 
This is built on the base of chained ETL procedures on an incremental basis and includes robust and reliable error management through the creation of a Log file. 
Once identified the sensitive data associated with each patient (such as hospital code and hospitalization code), the cryptographic hash function MD5 has been used for each identification code. 
The conversion table is saved into a dedicated and safe area for separation of duty and privacy reasons. 
A simplified view of the relational database generated is shown as an example in Fig. 2. 
Statistical differences between died and survived patients was evaluated by Pearson’s chi-square for categorical variables and Mann-Whitney test for the numerical ones.
2.4. Ethical aspects
The Fondazione Policlinico Universitario A. Gemelli IRCCS Institutional Review Board approved the study protocol (IRB 3447).
3. Results
3.1. A COVID-19 data mart cohort description
As of January 5th 2022, the Data Mart covers a retrospective cohort of 5528 patients with SARS-CoV-2 infection. 
We excluded from the analysis 210 who were currently being hospitalized and under care. 
Table 1 shows a summary view of the main characteristics of the retrospective cohort, on a subset of patients’ and clinical data selected from the larger set available in the Data Mart shown in Appendix. 
The data confirm several findings from previous research, e. g. people who died were older, had more comorbidities, were more frequently dyspnoic at onset, had higher d-dimer, Creactive protein and urea nitrogen. 
Leveraging the wealth of information available from the data mart updated on daily basis, we are currently analyzing in detail a variety of correlation index which are pre-requisite to build accurate predictors for critical outcome.

3.2. Data visualization utilities and process view
With the COVID-19 Data Mart online, the team has developed an extensive library of visualization dashboards, using SAS® Vyia® functionalities, to enable information at bedside for the clinical teams engaged in the daily care of infected patients. 
These dashboards cover cumulative views for hospital management level, ward management and patient care level. 
A conceptual view on how this is exploited is shown in Fig. 3.
3.3. Dashboard views: hospital management level
Figs. 4A and 4B show two examples of what is available online for the hospital care management in terms of day-to-day view of the inpatients, discharged, intensive care unit and other wards’ loads, as well as evolution of outcomes at overall hospital level. 
Some of the dashboard views show median age of hospitalized patients and comparison among COVID-19 waves, the evolution of oxygenation parameters such as PO2/FiO2 ratio (P/F) for the arterial blood gasses at admission and during the hospitalization course, the average length of hospitalization or the rate and timing of admission to Intensive Care.
This can help understanding several factors influencing the progression of the disease and comparative view of the different waves, which is relevant given that COVID-19 has shown mutable features in different stages. 
Figs. 5A to 5C show another set of cumulative dashboards, showing the overall cohort (retrospective, discharged patients and currently patients) with focus on evolution of clinical parameters. 
Dashboard 5A is focused on providing one integrated snapshot of the evolution of main clinical features along the timeline of the disease spread (wave 1, wave 2). It includes average age of inpatients overtime, impact of comorbidities, symptoms etc. 
This dashboard also focuses how the number of days for symptoms onset evolves on average. 
Dashboard 5B-5C show how this integrated framework can be used to investigate to which extent different clinical parameters at baseline can translate into early predictors and provide real-time comparison for the most recently hospitalized patients (examples shown cover d-dimer and IL-6 but the same are available for all critical laboratory data). 
These dashboards are useful to explore in real-time the possible correlations among included variables and clinical outcomes (Fig. 6).
3.4. Dashboard views: ward management level
A real-time situation for each ward (number of patients, severity scoring, new admissions, number of patients who died or who have been transferred to Intensive Care Unit the day before) are also available to support the daily management. 
Dashboards 6A-6C show the views that are made available on daily basis at ward level to support their overall management and prioritization: 
The view in 6A aims at giving in one-page a summary of the situation of the ward with respect to the COVID-19 trend i.e., newly hospitalized patients, total patients in charge, number of patients transferred to intensive care unit (ICU). 
The dashboard allows to also filter the discharged patients by outcomes (recovery; transfer to a pre-discharge facility; death). 
Views 6B-6C are designed to support clinicians at bedside in their daily activity: 6B gives an overall summary of hospitalized patients. 
Besides providing compare for key average parameters (such as P/F) to have a one-shot summary of how critical the ward situation is, more relevant is the list of hospitalized patients with a high-level scoring of their severity condition, in terms of age, days from first symptom onset, length of stay, current P/F and the P/F trend.
3.5. Dashboard views: patient management level
In dashboard 6C a detail for a specific patient is provided. 
This includes the history and the trend of both P/F and fever along the hospitalization period; result of RT-PCR nasopharyngeal test; 
features of most recent chest X-ray or chest computed tomography (CT).
4. Discussion
The burden of the COVID-19 pandemic on the daily life of people and the impact on healthcare systems all over the world are still impressive. 
The high number of people with COVID-19 admitted to emergency rooms, general wards and ICUs critically stressed hospitals [9]. 
Preparedness for the pandemic has been largely suboptimal [10]. 
In particular, the onset of several multiple waves of pandemic, with continuously mutable condition, is an additional challenge that requires flexible and comprehensive tools for data analysis and understanding. 
In fact, clinical and epidemiological data may be significantly different among patients from the different waves and therefore healthcare needs may vary even in very short time. 
Among strategies to respond to a pandemic such as that caused by SARS-CoV-2, we experienced the need to evolve from manual data sharing towards building a health data infrastructure (the so-called “health data superhighway”) [11] that facilitates automatic, interoperable data exchange and use. 
The potential insights provided from a very comprehensive Data Mart integrating clinical data and RWD for COVID-19 patients along with the whole natural history of the disease combined with AI methods is significant. 
We presented basic assumptions and the description of a Data Mart architecture developed at the Fondazione Policlinico Universitario A. 
Gemelli IRCCS based on the Generator infrastructure just after the onset of the SARS-CoV-2 pandemic. 
An extensive amount of data was quickly available for data monitoring, data analysis and clustering. 
Variables collected and integrated with the Data Mart are those commonly collected in daily practice [12–18] and already available in electronic medical records (EMRs);
the Data Mart can be augmented with multi-dimensional or external data. 
Details on technical aspects and a list of variables included in the COVID-19 Data Mart were shared to ensure reproducibility. 
The dashboard provides several functions. 
At hospital level, it covers cumulative views, provide a visual trend of evolution of critical parameters in the different waves, gives a snapshot of the evolution of main clinical features overtime, allows the knowledge in real-time of total number of inpatients in charge, discharged patients, those admitted and transferred in ICU and other wards’ loads, as well as evolution of outcomes at hospital level. 
Moreover, the dashboard allows to investigate early predictors of unfavorable outcomes and provide real-time comparison of clinical characteristics and laboratory parameters for the most recently hospitalized patients. 
At the ward level, the dashboard gives information listing hospitalized patients with a high-level scoring of their severity condition, older, with longer hospitalization stay, the current P/F and its trend. 
Finally, at the patient level, the dashboard provides a capture for a specific patient including days from symptom onset, evolution of P/F and fever along the hospitalization period, results of polymerase chain reaction–positive nasopharyngeal tests and most recent chest X-ray or CT. 
Many benefits are stemming from the availability of a Data Mart focused on COVID-19. 
First of all, this wide knowledge base can be exploited to build diagnostic and prognostic predictive models. 
Such advanced predictive models may be a great support to the most impacted wards in early alerting of critical evolution and most severe outcomes. 
The identification of the individual risk for each patient can also ease a more personalized approach. 
From a clinical point of view, such a tool may enhance the early implementation of supporting and therapeutic measures, to differentiate levels of needed healthcare resources (low, medium or high intensity) or may ease the identification of predictors of chronic lung damage in the follow-up. 
Moreover, different combinations of clinical variables and features may cluster into previously undescribed phenotypes and define different risks for poor outcomes. 
From a public health point of view, the prediction model may support optimal resource use. 
For instance, predictive models identify clinical criteria and laboratory values to safely allocate a person to common wards or to be discharged at home or to be de-isolated when probability of a COVID-19 diagnosis is poor [19]. 
This may result in saving hospital resources (beds, nurse and physician staff, personal protective equipment, disinfectant materials). 
In addition, we are currently working to connect this approach with tools for early prediction of critical evolution within the practice of base medicine and territory health management. 
In this regard, a hospital-based Data Mart may also be integrated with patient self-reported or personal data (through addressed App or IoT) to improve the diagnosis or to identify specific pattern of onset, progression or recovery for COVID-19. 
Of course, machine learning techniques to personalize treatment plans are not peculiar to COVID-19: 
machine learning has previously been used to improve diagnostic algorithms [20,21], predicting outcomes for patients with different diseases [22] or conditions [23]. 
Building a Data Mart for patients with COVID-19 may be reproduced for other clinical and epidemiological scenarios. 
Our framework has possible limitations. 
First, currently only routinely available clinical data of electronic health records are used. 
A generalization of the rules used to apply text mining techniques, based on the recognition of reports drawn up in national language would be necessary. 
An external and international ontology validation is mandatory and finally this workflow has only been implemented and used by a single hospital. 
In conclusion, a large Data Mart, including numerous structured and unstructured variables, gives the opportunity to realize a realworld, readily available, interactive dashboard and to build sophisticated and advanced predictive models [24]. 
Networks and pancohorts promoting collaboration across health centers, disciplines, and institutions represent crucial instruments to respond to pandemics or global health events. 
Therefore, complex integration of a large volume of clinical, radiological and laboratory data in an advanced architecture could be useful to quickly and reliably test new predictive models or therapeutic agents active against SARSCoV-2 or innovative regimens.
5. Conclusion
The experience that can be produced by the application and exploitation of a COVID DataMart can be paradigmatic for a wider application such as that of an entire region or state. 
Lastly, models predicting the course of future waves or new pandemics could largely benefit from dataMart networks like the one presented here.
Virtual screening of peptides with high affinity for SARS-CoV-2 main protease
ABSTRACT
The current pandemic of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused more than 2,000,000 deaths worldwide. 
Currently, vaccine development and drug repurposing have been the main strategies to find a COVID-19 treatment. 
However, the development of new drugs could be the solution if the main strategies fail. 
Here, a virtual screening of pentapeptides was applied in order to identify peptides with high affinity to SARS-CoV-2 main protease (Mpro). 
Over 70,000 peptides were screened employing a genetic algorithm that uses a docking score as the fitness function. 
The algorithm was coupled with a RESTful API to persist data and avoid redundancy. 
The docking exhaustiveness was adapted to the number of peptides in each virtual screening step, where the higher the number of peptides, the lower the docking exhaustiveness. 
Two potential peptides were selected (HHYWH and HYWWT), which have higher affinity to Mpro than to human proteases. 
Albeit preliminary, the data presented here provide some basis for the rational design of peptide-based drugs to treat COVID-19.
1. Introduction
Since December 2019, the worldwide massive health crisis provoked by the newly identified severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused over 2,000,000 deaths worldwide, according to World Health Organization.2 
As a global imperative, vaccine development has been accelerated, with more than 100 vaccine candidates [1]. 
Although there are three FDA-approved vaccines,3 from Moderna, Pfizer-BioNTech and Janssen, SARS-CoV-2 is still a serious issue to combat and investigate at the molecular level [4].
Some aspects of coronavirus disease 2019 (COVID-19) remain unknown, mainly due to the emergence of novel strains [5]. 
The SARS-CoV-2 RNA genome consists of about 30,000 bp. 
The largest open read frame, ORF1a/b, encodes polyproteins 1a and 1 ab (pp1a and pp1ab) [6–9]. 
These proteins are further processed by the main protease (Mpro) and a papain-like protease to produce different functional proteins [6,10]. 
This cleavage releases important viral enzymes, including RNA-dependent RNA polymerase, helicase and methyltransferase [6]. 
Therefore, Mpro is key to the viral cycle and, for this reason, this enzyme has been proposed as a therapeutic target for anti-coronavirus drug development [11,12,36,37]. 
Drug repurposing has been extensively explored as an attempt to identify possible Mpro inhibitors, where docking experiments play a critical role in virtual screening projects [13–15]. Drug repurposing is an elegant approach, because these drugs are already approved for human use [16]. 
However, the only officially approved drug for COVID-19 treatment is remdesivir for very specific cases [17]. 
Therefore, considering a pessimistic scenario, anti-SARS-CoV-2 drugs need to be developed from scratch. 
Peptide therapeutics have gained attention in the last decade [18–20], with some recent possible applications for COVID-19 [21,22]. 
These peptides offer a wide combinatorial space to explore (i.e. for a decapeptide there are 2010 possible combinations, taking into account only the 20 proteinogenic amino acid residues), and this enormous combinatorial space allows the development of inhibitors for different enzymes [20,23–25]. 
Besides, there is no convergence between different techniques for yielding such peptides, allowing different solutions for the same problem [20]. 
The main in vitro technique has been the high throughput screening of chemical, genetic and/or recombinant libraries, which could explore about 108 -1013 different peptides [19]. 
For the in silico counterpart, virtual screening is the alternative mean to identify possible peptide therapeutics, using docking as the main engine [26–28]. 
Therefore, Mpro inhibitors based on peptides could be an alternative for COVID-19 treatment. 
In fact, computer science and technology information applications have contributed in different ways to dealing with the pandemic [29]. 
Drug repurposing has been the main application of virtual screening; 
however, this technology could also be applied for exploring the combinatorial peptide space. 
Therefore, here, a virtual screening strategy using docking and genetic algorithms, speeded up by information technology applications, was developed to identify peptides with high affinity to Mpro. 
Two peptides with high affinity to Mpro were identified, and their possible applications to develop new drugs to treat COVID-19 are discussed.
2. Results
2.1. Minimum exhaustiveness dockings for a huge number of peptides
The virtual screening system was constructed using a client-server architecture, which allowed the task distribution in different computers and/or different cores of multicore processors and, due to the persistence layer on server side, more than 70,000 peptide sequences were explored (Fig. 1). 
The structure of SARS-CoV-2 Mpro was used as the target for developing peptides with high affinity by means of the genetic algorithm. 
The genetic algorithm simulates the evolution of a set of sequences, the population, by a number of generations. 
Thus, the population of peptide sequences was evolved using the docking scores against the Mpro active site, increasing the score and, therefore, increasing the affinity of these peptides to the enzyme. 
Fig. 2 shows the overall assessment of our virtual screening system. 
Due to the prevalence of aromatic residues, the same simulation excluding those residues was performed; however, none of them reached the affinity of the complete set (Fig. 2A). 
In fact, the rarefaction curves (Fig. 2B) indicated that there are more sequences to be discovered as more independent simulations are performed for both amino acid sets; 
however, the aliphatic-only set is more diverse than the full set. 
This effect should occur due to the preference for aromatic amino acids (including histidine), as demonstrated by Fig. 2A. 
Therefore, there was a small overlap between the sets, indicating that the aromatic amino acids rapidly populate the full set simulations (Fig. 2C). 
Although the algorithm generated evolved populations, final sequences from each population were not selected, instead, the database was used to select those sequences with high affinity to Mpro. 
Sequences with scores higher than 8955 (− 9.1 kcal mol− 1 ) were subjected to cluster analysis, in order to select the best sequence from each cluster for further analysis. 
Three clusters were selected (Fig. 3). 
Overall, sequences presented at least one Trp and one His residue. 
In addition, the best sequence from the simulation without aromatic side chains was also selected. 
Thus, the number of compounds was reduced from more than seventy thousand to only four peptides.
2.2. Medium exhaustiveness dockings for a reduced number of peptides
With fewer compounds to analyze, more exhaustiveness docking could be applied. 
Due to the fact that the initial dockings were performed with minimum exhaustiveness, the results were coarse and then they were refined. 
Besides, some residue combinations might not be tested by the genetic algorithm (in particular, proline residues were not included). 
Thus, amino acid scanning was performed. 
The scanning indicated that some point modifications improved the docking score (Fig. 4). 
Although point modifications in different positions increased the docking score, combining two or more of them resulted in reduced scores. 
Both combination models, simply-the-best and Joker, did not achieve better scores than the original sequences (data not shown). 
Therefore, from amino acid scanning, the sequence with the highest affinity or a value close to it (polar amino acids were preferred due to water solubility) was selected for further analysis, resulting in the sequences WWHWR, HHYWH, HYWWT and ARAHR.
2.3. High exhaustiveness dockings for a limited number of peptides
Given that the designed molecules are peptides and the target is a protease, we need to verify whether the peptides could bind to human proteases with more affinity than the virus protease. 
Thus, designed peptides were subjected to maximum exhaustiveness dockings with 100- fold repetitions. 
The peptide WWHWR showed no statistical differences in its scores against Mpro and human chymotrypsin, and thus this peptide would not be suitable for drug development. 
The remaining peptides showed higher affinity to the viral protease than to human proteases (Fig. 5). 
Binding poses from dockings on Mpro indicated that the peptides assume a similar conformation to the 11a inhibitor. 
The indole ring from the 11a inhibitor is overlapped by Trp4 on HHYWH and HYWWT (Fig. 6A), indicating that both sequences present the “HYW” motif; however, the 4th residue is key to the correct positioning in Mpro active site. 
The Mpro residues Thr25, Thr26, His41, Cys44, Glu166, Pro168 and Arg188 showed interactions with both peptides (Fig. 6B and C). 
The 11a inhibitor is smaller than the pentapeptides, where there is no overlap between the first two residues and the 11a inhibitor (Fig. 6A).
3. Discussion
The COVID-19 pandemic has provided the opportunity to apply the basic science developed in the last few decades and also to develop some frugal innovations [30]. 
Numerous efforts to reposition drugs against SARS-CoV-2 have been made. 
Indeed, virtual screening has been the leading strategy for selecting drug candidates for repurposing against COVID-19 [13,14]. 
Drug repurposing is a fast and elegant approach, because the drug is already approved for human use. 
However, the current scenario indicates that this could be a dead-end and new drugs should be developed. 
Indeed, a vaccine may be the ultimate resource to prevent SARSCoV-2 infection, but the COVID-19 mechanisms are not well enough understood [31,32]. 
The emergence of other SARS-CoV-2 strains [5] makes it necessary to carry out continuous screening to find new possible solutions. 
Besides, protective immunity may only last for short time periods [33]. In the viral cycle, viral proteases play a critical role in processing the viral polyprotein, and they were therefore used as targets for antiviral therapy, with a classical example of HIV treatment based on protease inhibitors [34]. 
The development of peptide inhibitors targeting key enzymes in a pathogen’s metabolism has gained attention during the last decade. 
This situation is clearly depicted when it comes to β-lactamase enzymes, which confers resistance to bacteria, with several peptide inhibitors being developed by using different strategies [20]. 
Despite the enormous combinatorial space of pentapeptides (195 , excluding proline residues), the strategy here applied (Fig. 1) was able to explore the combinatorial space with an optimal performance (Fig. 2). 
Although combinatorial space for pentapeptides was 195 combinations, a target number was not defined in the algorithm; instead, this number was used as an open goal, and when this goal was reached, the goal could be doubled. 
Indeed, Fig. 2B indicated that more simulations are needed to reach a plateau in the discovery of pentapeptides. 
In the first virtual screening step, more than 70,000 pentapeptide sequences were screened (Fig. 2C), which would require high computational power. 
In this context, the minimum exhaustiveness dockings and the information technology application speeded up the virtual screening process. 
In fact, information technology played a crucial role in this work, because the system architecture using a server application allowed fast screening using the genetic algorithm, given that an HTTP request is faster than a docking experiment. 
Also, despite Moore’s law reaching its limit [35], the client-server architecture of the whole application allowed the raspberry pi to simulate about 10% of all genetic algorithm simulations, even with less processing power. 
Nevertheless, the minimum exhaustiveness dockings had a cost: the noise in data could result in a non-optimal choice of molecules. 
To overcome this bias, the next steps were designed to exclude the noise, using the cluster analysis (Fig. 3), amino acid scanning with maximum exhaustiveness (Fig. 4) and several replicates. 
At this point, the use of docking replicates is important because peptides are flexible ligands, and docking simulations could therefore explore this property. 
The resulting peptides showed affinities in the range between − 10 and − 9 kcal mol− 1 (Fig. 5), which is in a similar range as that of lichen metabolites [36]; 
and a series of compounds from ChEMBL [37] against Mpro. 
Although some compounds from other works presented affinities below − 10 kcal mol− 1 [36,37], a direct comparison should be made with care, due to the differences in the number of docking replicates. 
Nevertheless, in comparison with the 11a inhibitor, under the same conditions, this approach resulted in peptides with higher affinities to Mpro (Fig. 5). 
Furthermore, the peptides should have some degree of selectivity towards Mpro, with less probability of binding to human proteases (Fig. 5). 
Therefore, the prototype of hydrophobicity cluster was discarded, due to its predicted affinity to human chymotrypsin. 
However, upon binding, these peptides could inhibit the enzyme or they could be cleaved by the enzyme. 
In fact, in both situations, they can be useful. 
In the case of inhibition, the peptide could be linked to a cellpenetrating peptide (CPP) [22] and inhibit the virus’s replication; 
otherwise, a toxin could be designed to kill the infected cells, by a combination of a four-domain peptide, including a toxin, the peptide, a toxin inactivating sequence and a CPP. 
Once cleaved on the pentapeptide, the toxin would be released and the cell would die. 
Due to the selectivity towards viral protease, only the infected cells would be affected. 
Another possibility is the use of these peptides as scaffolds for peptidomimetic [38] or nucleopeptide [39] development, which would take advantage of their high affinities to Mpro and inhibit viral replication.
4. Conclusions
Peptides have been considered “the drugs of the future” and, although they are preliminary, the data presented here provide some basis for the rational design of peptide-based drugs to treat COVID-19. 
The actual activity of such peptides remains unknown. 
However, independently of whether they act by inhibition or just by binding with high affinity to SARS-CoV-2 Mpro, these peptides may represent only part of the puzzle solution. 
As discussed, they could be used for engineering multi-domain peptides with different mechanisms depending on the core activity (inhibition or just binding) or even for application as part of a drug cocktail, similar to HIV treatment. 
Despite the development of vaccines against COVID-19 [40], these peptides could be useful as a treatment for those who are hospitalized with severe COVID-19 infection.
5. Material and methods
5.1. Virtual screening system architecture 
A raspberry pi 3 running the LAMP stack (Linux, Apache, MySQL and PHP) was used as the server layer. 
This was used for persisting data about peptides generated by genetic algorithm. 
A RESTful API was developed, allowing the communication between the server and the clients, where the execution of the genetic algorithm was performed. 
The GET and POST HTTP protocols were used to retrieve or post the data, respectively. 
This layer allowed maximum parallelization and data integration between different computers acting as clients. 
On the client layer, the local cache file is the primary resource of fitness data; 
when the value was not found in cache, the RESTful API was triggered, where for determining the fitness function, the algorithm sent a GET request to verify the existence of this registry; 
if the registry existed, the JSON was processed to get the fitness value; 
otherwise, the fitness had been calculated by the client and then sent using POST request for data persistence. 
After retrieving the data either by docking or RESTful API, the value was added to the local cache file. 
Eight Intel i7 cores plus three raspberry pi 3 cores were used as clients, running independent simulations.
5.2. Genetic algorithm
The genetic algorithm simulates the evolution of a population of sequences during a number of iterations, where given iteration In generates the population Pn from the population Pn− 1, evaluating the sequences according to the value of a fitness function, also known as chance of survivor and mating (Fig. 1B). 
Here the algorithm was implemented in PERL programming language, according to Porto et al. [50]. 
In the first iteration (I1) of the implementation of our custom genetic algorithm, P0 was composed by 19 pentapeptides, each one for a respective amino acid residue (e.g. AAAAA, RRRRR or WWWWW); 
proline residues were not included due to a bug in automatic structure generation. 
All sequences from P0 had the same fitness value, thus providing a random pairing for crossing over (Fig. 1). 
From iteration I2 to In, the sequence pairing for mating was performed according to the corresponding fitness values by means of a roulette wheel pairing model. 
For each iteration, 50 sequence pairs were selected from population Pn and each pair was submitted to a crossing over process, generating a couple of children for population Pn+1. No mutation was applied. 
Next, sequences from Pn+1 were evaluated by fitness function. The 10 worst sequences were removed from the population Pn+1 and then another iteration step was initiated (Fig. 1B). 
The cycle was repeated until the number of iterations was exhausted. One hundred independent simulations were performed, each one with 50 iterations using the same conditions. 
A second set of simulations was performed under the same conditions, excluding His, Phe, Tyr and Trp residues, because these residues seem to increase the affinities between peptide and enzyme.
5.3. Fitness function
The fitness value was obtained either from the persisted data (cache or RESTful API) or by calculating the fitness value as a function of a docking score, by means of Eq. (1):
where the DS states the docking score, which was given by AutoDock Vina (see below). 
The modulus of docking score was used in order to increase the fitness value, while the exponential function was applied because the docking score has only one decimal place, which could not be reflected in great fitness differences in the roulette wheel selection from the genetic algorithm. 
For the automatic docking subroutine, an extended peptide structure had been generated by a PyMOL script before performing the minimum exhaustiveness docking using AutoDock Tools [41] and AutoDock Vina [42].
5.4. Docking
AutoDock Vina [41] and AutoDock Tools [42] were used to perform molecular docking, which was used to verify the affinities between the generated peptides and SARS-CoV-2 Mpro (PDB ID: 6LZE) [43] or human proteases (PDB IDs: 4H4F and 1TRN) [44,45]. 
The PDB structures were converted to PDBQT files using AutoDock Tools. 
Dockings were performed using AutoDock Vina. For all structures, the box size was set as 90 Å3 ; 
then the grid box was positioned in X, Y, Z coordinates according to the respective enzyme: 
24.33, 22.34 and − 4.27 for 1TRN; 28.86, 44.46 and 16.10 for 4H4F and − 10.94, 12.69 and 68.91 for 6LZE. 
Depending on the virtual screening step, minimum or maximum exhaustiveness were used.
5.4.1. Minimum exhaustiveness dockings
Minimum exhaustiveness dockings were performed by setting the AutoDock Vina exhaustiveness and the number of CPUs to use options as one. 
This allowed maximum parallelization of independent docking experiments. 
No replicas were performed for these dockings. 
5.4.2. Maximum exhaustiveness dockings
Maximum exhaustiveness dockings were performed using the AutoDock Vina default options. 
Dockings were performed with three or one hundred independent runs with different seeds, according to the virtual screening step. 
5.5. Clustering
The sequences with fitness values higher than 8955 (corresponding to docking scores below − 9.1), were selected for cluster analysis according to physicochemical properties. 
Three physicochemical properties were calculated: average hydrophobicity using the Wimley & White scale [46]; 
average flexibility using the Bhaskaran-Ponnuswamy scale [47]; and peptide instability using the Guruprasad scale based on dipeptide compositions [48]. 
Three clusters were defined by PAMK algorithm, using the R package for statistical computing. 
The best sequence from each cluster was selected for the next steps. 
The best sequence from the simulations without the aromatic ring containing residues was also selected for the next steps.
5.6. Amino acid scanning
The four sequences selected from virtual screening were subjected to amino acid scanning, where each position was replaced by each proteinogenic amino acid residue, generating a variant with a single amino acid alteration. 
MODELLER 9.23 [51] was used to build a model from each primary sequence, by means of environ and model classes. 
These variants were subjected to maximum exhaustiveness dockings in triplicate.
5.7. Combinations
Two models for combining data from amino acid scanning were assumed. In the first model (simply-the-best), each position was filled up by the amino acid residue with the highest affinity; 
while in the second model (Joker), a prosite grammar was constructed using the amino acid residues with higher affinity than the native one, and then the Joker algorithm [49] was used to select the residue according to the hydrophobicity of each position in the prosite grammar. 
MODELLER 9.23 [51] was used to build a model from each primary sequence, by means of environ and model classes. 
Combinations were forwarded to maximum exhaustiveness docking experiments in triplicate. 
The Joker combinations were used as a control because their hydrophobic scale favors the selection of aliphatic instead of aromatic hydrophobic residues.
5.8. Statistical analysis
The one-sided Mann-Whitney-Wilcoxon test was used to verify the differences between the affinities among the peptides and 11a inhibitor towards Mpro; 
and among the Mpro and human proteases for each peptide. 
The test was performed using the R package for statistical computing.